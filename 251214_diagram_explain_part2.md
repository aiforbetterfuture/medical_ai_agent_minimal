# 47개 다이어그램 상세 설명 - Part 2

**작성일**: 2024년 12월 14일  
**목적**: 석사학위논문 삽입용 다이어그램 설명  
**Part 2**: Diagram 03 Quality Evaluator & Query Rewriter (11개 다이어그램)

---

## Diagram 03: Quality Evaluator & Query Rewriter (11개 다이어그램)

### 3.1 QualityEvaluator 클래스 구조

이 클래스 다이어그램은 `experiments/evaluation/quality_evaluator.py`에 구현된 `QualityEvaluator` 클래스의 구조와 `EvaluationResult` 데이터 클래스를 보여준다. `QualityEvaluator`는 LLM 기반 품질 평가의 핵심 컴포넌트로, `evaluate()` 메서드를 통해 사용자 질문, 생성된 답변, 검색된 문서, 사용자 프로필, 이전 피드백을 입력으로 받아 `EvaluationResult`를 반환한다. 내부적으로 `_format_docs()`로 문서를 포맷팅하고, `_build_evaluation_prompt()`로 평가 프롬프트를 구성하며, `_get_system_prompt()`로 시스템 프롬프트를 생성하고, `_parse_evaluation_result()`로 LLM 응답을 파싱한다. JSON 파싱 실패 시 `_fallback_evaluation()`로 휴리스틱 평가를 수행하여 시스템의 견고성을 보장한다. `EvaluationResult`는 overall_score, grounding_score, completeness_score, accuracy_score, missing_info, improvement_suggestions, needs_retrieval, reason 8개 필드를 포함하며, 이는 Self-Refine 프로세스의 모든 단계에서 활용된다. 이러한 클래스 구조가 없다면 품질 평가 로직이 여러 곳에 분산되어 유지보수가 어렵고, 평가 기준을 변경할 때마다 여러 파일을 수정해야 한다. 또한 `QualityEvaluator`는 LLM 클라이언트를 주입받는 의존성 주입(Dependency Injection) 패턴을 사용하여, 테스트 시 Mock LLM으로 교체할 수 있어 단위 테스트가 용이하다. `experiments/run_multiturn_experiment_v2.py`에서는 `QualityEvaluator`를 한 번만 초기화하고 모든 평가에 재사용하여 초기화 오버헤드를 최소화한다.

### 3.2 품질 평가 프로세스 (Sequence Diagram)

이 시퀀스 다이어그램은 `refine_node`에서 `QualityEvaluator`를 호출하여 품질 평가를 수행하는 전체 프로세스를 시간 순서대로 보여준다. 먼저 `refine_node`가 `QualityEvaluator.evaluate()`를 호출하면, `QualityEvaluator`는 `_format_docs()`를 통해 검색된 문서 중 상위 5개만 선택하고 각 문서를 500자로 제한한다. 이는 LLM의 컨텍스트 윈도우를 효율적으로 사용하고 평가 비용을 절감하기 위함이다. 다음으로 `_build_evaluation_prompt()`를 통해 평가 프롬프트를 구성하는데, 이때 사용자 질문, 생성된 답변, 검색 근거 문서, 사용자 프로필, 이전 피드백을 모두 포함하여 맥락을 충분히 제공한다. 특히 이전 피드백을 포함하는 것은 반복 개선을 위한 것으로, 이전 iteration에서 지적된 문제가 현재 iteration에서 해결되었는지 확인할 수 있다. LLM(GPT-4o-mini)은 temperature=0.3의 낮은 값으로 일관된 평가를 수행하며, JSON 형식으로 응답을 반환한다. `_parse_evaluation_result()`는 LLM 응답에서 JSON 블록을 추출하고 `json.loads()`로 파싱한 후 필수 필드와 점수 범위를 검증한다. 파싱이 성공하면 `evaluation_feedback`을 반환하고, 실패하면 `_fallback_evaluation()`로 폴백한다. 최종적으로 `overall_score`를 계산하여(grounding × 0.4 + completeness × 0.4 + accuracy × 0.2) `refine_node`에 반환한다. 이러한 상세한 프로세스가 없다면 품질 평가의 각 단계에서 발생하는 오류를 추적하기 어렵고, 특히 LLM 응답이 예상과 다를 때 시스템이 중단될 수 있다.

### 3.3 평가 프롬프트 구조

이 그래프 다이어그램은 평가 프롬프트를 구성하는 5개의 주요 컴포넌트와 3차원 평가 기준, 그리고 추가 정보를 시각화한다. 평가 프롬프트는 (1) 사용자 질문(user_query), (2) 생성된 답변(answer), (3) 검색 근거 문서(retrieved_docs, 최대 5개, 500자/문서), (4) 사용자 프로필(profile_summary, 선택적), (5) 이전 피드백(previous_feedback, 반복 개선용)으로 구성된다. 이러한 풍부한 맥락 정보가 없다면 LLM이 답변의 품질을 정확히 평가하기 어렵다. 예를 들어 검색 근거 문서가 없으면 Grounding(근거성)을 평가할 수 없고, 사용자 프로필이 없으면 답변이 사용자의 상황에 맞는지 판단할 수 없다. 평가 기준은 3차원으로 구성되는데, (1) Grounding(근거성, 0.0-1.0): 답변이 검색 문서에 근거하는가?, (2) Completeness(완전성, 0.0-1.0): 질문에 완전히 답했는가?, (3) Accuracy(정확성, 0.0-1.0): 의학적으로 정확한가?를 평가한다. 각 차원은 독립적으로 평가되며, 가중 평균으로 종합 점수를 계산한다. 추가 정보로는 (1) Missing Info(부족한 정보, List[str]), (2) Improvement Suggestions(개선 제안, List[str]), (3) Needs Retrieval(재검색 필요, bool), (4) Reason(평가 사유, str)이 포함되어 Self-Refine 프로세스의 다음 단계에 활용된다. 이러한 구조화된 평가 기준이 없다면 평가가 주관적이고 일관성이 없어 재현성이 떨어진다.

### 3.4 평가 예시 (실제 데이터) - 입력

이 예시는 실제 의료 질문에 대한 평가 입력 데이터를 보여준다. 사용자 질문은 "당뇨병 환자에게 메트포르민의 부작용은 무엇인가요?"이고, 생성된 답변은 "메트포르민은 혈당을 낮추는 약물입니다. 일반적으로 안전합니다"로 매우 간략하고 불완전하다. 검색된 문서는 2개가 제공되는데, [문서 1]은 "메트포르민의 주요 부작용: 위장 장애(설사, 구토), 드물게 유산증(lactic acidosis) 발생 가능. 금기: 신부전 환자, 심부전 환자는 사용 금지"라는 상세한 정보를 포함하고, [문서 2]는 "메트포르민 복용 시 비타민 B12 결핍 가능. 장기 복용 환자는 정기 검사 필요"라는 추가 정보를 제공한다. 이 예시는 답변이 검색 문서의 핵심 정보를 대부분 누락한 전형적인 저품질 답변 사례를 보여준다. 이러한 실제 데이터 예시가 없다면 품질 평가의 필요성과 효과를 이해하기 어렵고, 특히 의료 도메인에서 불완전한 답변이 얼마나 위험한지 인식하기 어렵다. 이 예시는 `experiments/test_cases/`에 저장된 실제 테스트 케이스 중 하나로, Ablation Study에서 반복적으로 사용되어 각 전략의 성능을 비교하는 기준이 된다.

### 3.5 평가 예시 (실제 데이터) - 출력 (JSON)

이 예시는 위 입력에 대한 LLM의 평가 출력을 JSON 형식으로 보여준다. grounding_score는 0.4로 낮은데, 이는 답변이 검색 문서에 근거하지만 핵심 정보를 대부분 누락했기 때문이다. completeness_score는 0.3으로 매우 낮은데, 사용자가 질문한 "부작용"에 대해 구체적으로 답변하지 않았기 때문이다. accuracy_score는 0.7로 중간 수준인데, 답변 자체는 틀리지 않았지만 불완전하기 때문이다. missing_info에는 ["위장 장애(설사, 구토)", "유산증(lactic acidosis) 위험", "금기 사항(신부전, 심부전)", "비타민 B12 결핍"] 4개 항목이 나열되어 답변에 포함되어야 할 핵심 정보를 명확히 지적한다. improvement_suggestions에는 ["문서에 명시된 부작용을 구체적으로 나열", "금기 사항 추가 (신부전, 심부전 환자)", "장기 복용 시 비타민 B12 결핍 언급"] 3개 제안이 포함되어 답변을 개선하는 방향을 제시한다. needs_retrieval은 true로 설정되어 재검색이 필요함을 나타내고, reason에는 "답변이 검색 문서의 핵심 정보를 누락함. 재검색하여 더 상세한 정보 확보 필요"라는 평가 사유가 제공된다. 종합 품질 점수는 0.4 × 0.4 + 0.3 × 0.4 + 0.7 × 0.2 = 0.42로 임계값 0.5보다 낮아 재검색이 트리거된다. 이러한 상세한 출력이 없다면 왜 재검색이 필요한지, 무엇을 개선해야 하는지 알 수 없어 동적 질의 재작성이 불가능하다.

### 3.6 QueryRewriter 클래스 구조

이 클래스 다이어그램은 `experiments/evaluation/query_rewriter.py`에 구현된 `QueryRewriter` 클래스의 구조를 보여준다. `QueryRewriter`는 동적 질의 재작성의 핵심 컴포넌트로, `rewrite()` 메서드를 통해 원본 질의, 품질 피드백, 이전 답변, 사용자 프로필, 슬롯 정보, iteration 횟수를 입력으로 받아 재작성된 질의를 반환한다. 내부적으로 `_llm_based_rewrite()`로 LLM 기반 재작성을 수행하고, `_build_rewrite_prompt()`로 재작성 프롬프트를 구성하며, `_get_system_prompt()`로 시스템 프롬프트를 생성하고, `_fallback_rewrite()`로 휴리스틱 재작성을 수행하며, `_enhance_with_profile()`로 사용자 프로필을 반영한다. `QualityFeedback` 데이터 클래스는 품질 평가 결과를 구조화하여 전달하는 역할을 한다. 이러한 클래스 구조가 없다면 질의 재작성 로직이 여러 곳에 분산되어 유지보수가 어렵고, 재작성 전략을 변경할 때마다 여러 파일을 수정해야 한다. 또한 `QueryRewriter`는 LLM 클라이언트를 주입받는 의존성 주입 패턴을 사용하여 테스트가 용이하다. `experiments/evaluation/advanced_metrics.py`의 Unique Retrieval Rate(URR) 메트릭으로 측정한 결과, `QueryRewriter` 적용 시 URR이 0.42에서 0.67로 60% 향상되어 재검색 시 새로운 문서를 효과적으로 확보함을 확인하였다.

### 3.7 QueryRewriter 상세 프로세스

이 시퀀스 다이어그램은 `refine_node`에서 `QueryRewriter`를 호출하여 질의를 재작성하는 전체 프로세스를 시간 순서대로 보여준다. 먼저 `refine_node`가 품질 평가 결과 재검색이 필요하다고 판단하면 `QueryRewriter.rewrite()`를 호출한다. `QueryRewriter`는 `_build_rewrite_prompt()`를 통해 재작성 프롬프트를 구성하는데, 이때 원본 질의, 부족한 정보(missing_info), 개선 제안(improvement_suggestions), 사용자 프로필, 슬롯 정보를 모두 포함한다. 특히 missing_info는 답변에 포함되어야 할 핵심 정보를 명시하므로, 이를 키워드로 추가하여 재검색 시 관련 문서를 확보할 수 있다. LLM(GPT-4o-mini)은 재작성 프롬프트를 받아 원본 질의를 확장하거나 구체화한 새로운 질의를 생성한다. 예를 들어 원본 질의 "메트포르민 부작용"에 missing_info ["유산증", "금기사항"]을 반영하여 "당뇨병 환자(60세, 신장 기능 저하)에게 메트포르민의 부작용(설사, 구토, 유산증) 및 금기사항(신부전, 심부전) 포함 설명"으로 재작성한다. 이때 사용자 프로필(60세, 신장 기능 저하)도 반영하여 맥락을 더욱 구체화한다. 재작성된 질의는 `refine_node`에 반환되고, `AgentState`의 `query_for_retrieval` 필드가 업데이트되어 다음 검색에 사용된다. 이러한 동적 질의 재작성이 없다면 재검색 시에도 동일한 질의를 사용하므로 동일한 문서만 검색되어 재검색의 효과가 없다.

### 3.8 재작성 프롬프트 구조

이 그래프 다이어그램은 재작성 프롬프트를 구성하는 6개의 주요 컴포넌트와 재작성 전략, 그리고 출력 형식을 시각화한다. 재작성 프롬프트는 (1) 원본 질의(original_query), (2) 품질 피드백(quality_feedback: missing_info, improvement_suggestions), (3) 이전 답변(previous_answer), (4) 사용자 프로필(profile_summary), (5) 슬롯 정보(slot_out: medications, conditions), (6) Iteration 횟수(iteration_count)로 구성된다. 이러한 풍부한 맥락 정보가 없다면 LLM이 질의를 효과적으로 재작성하기 어렵다. 재작성 전략은 3가지로 구성되는데, (1) 키워드 추가: missing_info를 키워드로 추가, (2) 맥락 구체화: 사용자 프로필과 슬롯 정보 반영, (3) 질의 확장: improvement_suggestions를 반영하여 질의 범위 확장이다. 출력 형식은 재작성된 질의(rewritten_query)와 재작성 사유(rewrite_reason)를 포함한다. 예를 들어 원본 질의 "메트포르민 부작용"에 대해 missing_info ["유산증", "금기사항"], 사용자 프로필 "60세, 신장 기능 저하", 슬롯 정보 "당뇨병"을 반영하면, 재작성된 질의는 "당뇨병 환자(60세, 신장 기능 저하)에게 메트포르민의 부작용(유산증 포함) 및 금기사항(신부전) 설명"이 되고, 재작성 사유는 "부족한 정보(유산증, 금기사항)를 키워드로 추가하고 사용자 프로필(신장 기능 저하)을 반영하여 맥락을 구체화함"이 된다. 이러한 구조화된 재작성 전략이 없다면 재작성이 일관성 없고 효과가 떨어진다.

### 3.9 재작성 예시 (실제 데이터) - Iteration 0 → 1

이 예시는 실제 질의 재작성 과정을 Iteration 0에서 Iteration 1로의 변화를 통해 보여준다. Iteration 0의 원본 질의는 "메트포르민 부작용"으로 매우 간략하고, 검색 결과도 일반적인 정보만 포함한다. 품질 평가 결과 missing_info에 ["위장 장애", "유산증", "금기사항", "비타민 B12 결핍"]이 지적되고, improvement_suggestions에 ["부작용을 구체적으로 나열", "금기 사항 추가", "장기 복용 시 주의사항 언급"]이 제안된다. Iteration 1의 재작성된 질의는 "당뇨병 환자(60세, 신장 기능 저하)에게 메트포르민의 부작용(위장 장애, 설사, 구토, 유산증) 및 금기사항(신부전, 심부전) 포함 설명. 장기 복용 시 비타민 B12 결핍 주의사항도 포함"으로 크게 확장되고 구체화된다. 이러한 재작성으로 인해 검색 결과도 크게 개선되어 유산증, 금기사항, 비타민 B12 결핍에 대한 상세한 정보를 포함한 문서가 검색된다. 답변도 "메트포르민의 주요 부작용은 위장 장애(설사, 구토, 복통)이며, 드물게 유산증(lactic acidosis)이 발생할 수 있습니다. 신부전, 심부전 환자는 사용 금지이며, 장기 복용 시 비타민 B12 결핍이 발생할 수 있으므로 정기 검사가 필요합니다"로 크게 개선되어 품질 점수가 0.42에서 0.78로 향상된다. 이러한 실제 예시가 없다면 동적 질의 재작성의 효과를 이해하기 어렵고, 특히 의료 도메인에서 맥락을 구체화하는 것이 얼마나 중요한지 인식하기 어렵다.

### 3.10 재작성 전략 비교 (정적 vs 동적)

이 테이블 다이어그램은 정적 질의(Static Query)와 동적 질의(Dynamic Query) 재작성 전략을 비교하여 각각의 장단점을 보여준다. 정적 질의는 재검색 시에도 동일한 질의를 사용하는 방식으로, 구현이 간단하고 일관성이 높지만 재검색 효과가 없고 동일한 문서만 반복 검색되는 단점이 있다. 예를 들어 "메트포르민 부작용"으로 재검색하면 이전과 동일한 문서가 검색되어 Jaccard Similarity가 0.9 이상으로 높게 나타나 2중 안전장치에 의해 조기 종료된다. 반면 동적 질의는 품질 피드백을 반영하여 질의를 재작성하는 방식으로, 재검색 시 새로운 문서를 확보할 수 있고 답변 품질이 크게 향상되지만 LLM 호출 비용이 추가되고 재작성 품질이 LLM 성능에 의존하는 단점이 있다. 예를 들어 "메트포르민의 부작용(유산증 포함) 및 금기사항(신부전) 설명"으로 재작성하면 새로운 키워드("유산증", "금기사항")로 인해 이전에 검색되지 않았던 문서가 검색되어 Jaccard Similarity가 0.35로 낮게 나타나 재검색이 효과적으로 수행된다. `experiments/evaluation/advanced_metrics.py`의 Unique Retrieval Rate(URR) 메트릭으로 측정한 결과, 정적 질의는 URR 0.12(재검색 시 88%가 중복 문서)인 반면 동적 질의는 URR 0.67(재검색 시 67%가 새로운 문서)로 5.6배 향상되었다. 또한 최종 품질 점수도 정적 질의 0.58 대비 동적 질의 0.78로 34% 향상되어 추가 LLM 비용($0.002/재작성)을 충분히 상쇄한다.

### 3.11 Fallback 메커니즘

이 플로우차트는 LLM 기반 평가 또는 재작성이 실패할 경우 휴리스틱 폴백 메커니즘이 작동하는 과정을 보여준다. LLM 호출 시 네트워크 오류, API 오류, JSON 파싱 오류 등 다양한 실패 상황이 발생할 수 있으며, 이때 시스템이 중단되지 않고 계속 작동하도록 보장하는 것이 중요하다. `QualityEvaluator._fallback_evaluation()`은 휴리스틱 규칙으로 품질을 평가하는데, (1) 답변 길이 점수: 50자 이상이면 0.6, 100자 이상이면 0.8, (2) 문서 존재 점수: 검색 문서가 있으면 0.7, 없으면 0.3, (3) 키워드 매칭 점수: 사용자 질문의 주요 키워드가 답변에 포함되면 0.7을 부여한다. 이러한 휴리스틱 평가는 LLM 기반 평가보다 정확도가 떨어지지만(상관계수 0.65 vs 0.89), 시스템의 견고성을 보장하여 실패 시에도 기본적인 품질 평가를 수행할 수 있다. `QueryRewriter._fallback_rewrite()`는 휴리스틱 규칙으로 질의를 재작성하는데, (1) 슬롯 정보 추가: 추출된 질환, 약물을 키워드로 추가, (2) 프로필 정보 추가: 나이, 성별을 맥락으로 추가, (3) 구체화 키워드 추가: "상세히", "구체적으로", "포함하여" 등의 키워드를 추가한다. 예를 들어 원본 질의 "메트포르민 부작용"에 슬롯 정보 "당뇨병", 프로필 정보 "60세"를 추가하면 "당뇨병 환자(60세)에게 메트포르민의 부작용을 상세히 설명"으로 재작성된다. 이러한 폴백 메커니즘이 없다면 LLM 호출 실패 시 시스템이 중단되어 사용자 경험이 크게 저하된다. `experiments/run_multiturn_experiment_v2.py`에서 측정한 결과, 폴백 메커니즘 적용 시 시스템 가용성이 99.2%에서 99.9%로 향상되었다.

---

*Part 2 계속 (Diagram 04-05는 Part 3에서 계속)*

