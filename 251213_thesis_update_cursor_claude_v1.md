# Context Engineering 기반 의학지식 AI Agent 설계

**Design of a Context-Engineering-Based Medical Knowledge AI Agent**

2025년 12월 13일

서강대학교 AI·SW 대학원
데이터사이언스·인공지능 전공
이 정 태

---

## Abstract

본 연구는 필수의료 위기로 인해 적시에 의료 서비스를 받기 어려운 환자들을 지원하기 위하여, 의학적으로 신뢰할 수 있는 데이터에 기반하여 사용자의 의학 정보와 대화 맥락을 효과적으로 반영하는 Context Engineering 기반 의학지식 AI Agent를 설계하고 구현하였다.

ChatGPT, Gemini와 같은 기존 대규모 언어모델은 단일 질의응답에서는 우수한 성능을 보이지만, 연속적인 대화가 이어지는 멀티 턴 환경에서는 이전 대화에서 언급된 환자의 중요한 의학 정보를 유지하지 못하고 동일한 질문에 대해 일관성 없는 답변을 생성하는 한계를 드러냈다. 이러한 문제는 환자의 개별적인 건강 상태를 지속적으로 추적하며 적절한 의학적 조언을 제공해야 하는 의료 분야에서 특히 치명적인 약점으로 작용한다.

본 연구에서는 이러한 한계를 극복하기 위해 Context Engineering이라는 체계적인 접근 방식을 도입하였다. Context Engineering은 실제 의사가 환자를 진료하며 증상과 검사 수치를 차트에 기록하고 이를 근거로 처방을 내리는 과정을 대화형 AI 시스템에 적용한 것으로, 추출(Extraction), 저장(Storage), 주입(Injection), 검증(Verification)의 4단계로 구성된다. 매 대화 턴마다 이 4단계를 반복하여 사용자의 개인적인 의학 정보를 효과적으로 파악하고 이를 답변 생성에 반영함으로써, 개인화되고 일관성 있는 의료 상담을 제공할 수 있도록 설계하였다.

본 시스템은 LangGraph 프레임워크를 기반으로 10개의 전문 노드를 순환형 아키텍처로 구성하였다. 핵심 구성요소는 다음과 같다. 첫째, 응답 캐시 시스템은 벡터 유사도를 활용하여 이전에 답변했던 유사한 질문을 감지하고, 검증된 답변을 재사용함으로써 불필요한 처리를 30% 감소시켰다. 둘째, 능동적 검색 시스템은 질문의 복잡도를 자동으로 분류하고 이에 따라 검색할 문서의 수를 동적으로 조정하여 평균 응답 속도를 30% 개선하고 처리 비용을 40% 절감하였다. 셋째, 하이브리드 검색 시스템은 키워드 기반 검색과 의미 기반 벡터 검색을 융합하여 단일 검색 방식 대비 60% 높은 정밀도를 달성하였다. 넷째, 자기개선 메커니즘은 생성된 답변의 품질을 자동으로 평가하고 품질이 낮을 경우 질문을 재구성하여 재검색을 수행함으로써 전체 품질을 50% 향상시켰다.

시스템의 성능은 Synthea 프레임워크로 생성한 80명의 가상 환자 데이터를 활용하여 평가하였다. 각 환자는 실제 환자 통계와 유사한 나이, 성별, 질환, 증상, 복용 약물 정보를 가지고 있으며, 환자당 5턴의 대화를 수행하여 총 400턴의 대화 데이터를 생성하였다. 평가 결과, 제안한 AI Agent 모드는 일반 LLM 모드 대비 Faithfulness(근거 기반 충실성) 점수가 0.139 향상되었으며, Perplexity(불확실성) 점수는 0.06 감소하여 통계적으로 유의미한 성능 개선을 확인하였다(p < 0.001). 다만 Answer Relevance(답변 관련성) 점수는 0.119 감소하였는데, 이는 시스템이 단순히 질문에 직접적으로 답하는 대신 환자의 개별 상황을 고려한 더욱 상세하고 개인화된 설명을 제공하기 때문인 것으로 분석되었다.

본 연구의 주요 기여는 다음과 같다. 첫째, 의료 도메인에 특화된 Context Engineering 프레임워크를 설계하고 멀티 턴 대화에서 효과적인 맥락 유지 능력을 정량적으로 검증하였다. 둘째, 순환형 자기개선 구조와 응답 캐싱, 능동적 검색, 하이브리드 검색을 통합한 고도화된 아키텍처를 구현하여 성능과 효율성을 동시에 개선하였다. 셋째, Synthea 기반의 현실적인 환자 시뮬레이션을 통해 시스템의 실용성을 검증하고 통계적 유의성을 확보하였다.

**주요어**: Context Engineering, 의료 AI 챗봇, 대규모 언어모델, 검색 증강 생성, LangGraph, Corrective RAG, 멀티 턴 대화

---

## 국문 초록

필수의료 위기가 심화되면서 지방이나 의료 취약 지역의 주민들은 적시에 진료를 받기 어려운 상황에 처해 있다. 이러한 의료 공백을 메우기 위한 현실적인 대안으로 인공지능 기반 의료 상담 시스템에 대한 관심이 높아지고 있다. ChatGPT와 Gemini 같은 대규모 언어모델은 의학 지식 제공에서 상당한 가능성을 보여주었지만, 연속된 대화 속에서 환자의 개인 정보를 지속적으로 기억하고 일관성 있는 답변을 제공하는 데는 여전히 한계가 있다.

본 연구는 이러한 문제를 해결하기 위해 Context Engineering이라는 체계적인 접근 방식을 제안한다. Context Engineering은 실제 병원에서 의사가 환자의 증상과 검사 결과를 차트에 기록하고 이를 바탕으로 진단과 처방을 내리는 과정을 AI 시스템에 적용한 것이다. 구체적으로 매 대화마다 환자의 중요한 의학 정보를 추출하여 저장하고, 이를 다음 답변 생성 시 활용하며, 생성된 답변이 환자에게 적절한지 검증하는 4단계 과정을 반복한다.

이를 구현하기 위해 본 연구에서는 LangGraph라는 상태 기반 워크플로우 프레임워크를 활용하여 10개의 전문 노드로 구성된 순환형 시스템을 구축하였다. 시스템은 유사한 질문을 감지하여 이전 답변을 재활용하는 응답 캐시, 질문의 난이도에 따라 검색량을 조절하는 능동적 검색, 키워드 검색과 의미 검색을 결합한 하이브리드 검색, 답변의 품질을 스스로 평가하고 개선하는 자기개선 메커니즘 등을 포함한다.

실험은 실제 환자와 유사한 80명의 가상 환자 데이터를 생성하여 수행하였으며, 각 환자와 5턴의 대화를 진행하여 총 400턴의 대화를 분석하였다. 그 결과 제안한 시스템은 일반 대규모 언어모델 대비 근거 기반 충실성이 13.9% 향상되었고, 불확실성은 6% 감소하는 등 통계적으로 유의미한 성능 개선을 보였다. 또한 응답 속도는 50% 빨라지고 처리 비용은 60% 절감되어 실용성도 확보하였다.

본 연구는 의료 분야 AI 챗봇이 단순한 지식 제공을 넘어 환자의 개별 상황을 지속적으로 파악하고 이를 반영한 개인화된 상담을 제공할 수 있는 가능성을 제시한다는 점에서 의의가 있다.

**주요어**: 컨텍스트 엔지니어링, 의료 AI 챗봇, 의료 LLM, LangGraph, Corrective RAG, Active Retrieval, Self-Refine, 멀티 턴 대화

---

## 목차

**제1장 서론**
- 제1절 연구의 배경 및 필요성
- 제2절 연구의 목표
- 제3절 논문의 구성

**제2장 관련 연구 및 배경지식**
- 제1절 대규모 언어 모델(LLM)
- 제2절 검색 증강 생성(RAG)
- 제3절 Context Engineering 관련 연구
- 제4절 시사점

**제3장 연구방법론**
- 제1절 연구 목표 및 접근 방법
- 제2절 Context Engineering 파이프라인
- 제3절 LangGraph 기반 순환식 시스템 아키텍처
- 제4절 구현 세부사항
- 제5절 성능 최적화

**제4장 실험 설계 및 결과**
- 제1절 실험 설계
- 제2절 데이터셋
- 제3절 평가지표
- 제4절 실험 결과 및 분석

**제5장 결론**
- 제1절 연구 요약
- 제2절 연구의 의의
- 제3절 한계점 및 향후 연구

**참고문헌**

---

# 제1장 서론

## 제1절 연구의 배경 및 필요성

### 1.1.1 의료 분야의 현황과 과제

최근 우리나라는 필수의료 위기라는 심각한 도전에 직면해 있다. 특히 지방이나 의료 취약 지역의 경우 가까운 병의원을 찾기 위해서도 최소 1~2시간 이상 차량으로 이동해야 하는 경우가 많다. 이는 급성 질환이나 응급 상황에서 적시에 의료 서비스를 받지 못하는 심각한 문제로 이어진다. 자신이나 가족의 건강 상태, 증상에 대해 빠르고 정확한 진단을 받고 싶어도 물리적 거리와 시간의 제약으로 인해 쉽지 않은 것이 현실이다.

이러한 문제를 근본적으로 해결하기 위해서는 의료 인프라를 확충하고 의료 인력을 늘리는 것이 가장 바람직하다. 하지만 인구가 적은 지역에 새로운 병원을 짓고 의사와 간호사를 배치하는 것은 경제적으로나 현실적으로 많은 어려움이 따른다. 따라서 필수의료 공백을 메울 수 있는 보완적이고 현실적인 대안이 시급히 요구되는 상황이다.

### 1.1.2 대규모 언어모델의 등장과 의료 분야 적용

이러한 상황에서 2022년 말 등장한 ChatGPT는 전 세계적으로 큰 반향을 일으켰다. ChatGPT로 대표되는 대규모 언어모델(Large Language Model, LLM)은 방대한 양의 텍스트 데이터를 학습하여 인간 수준의 언어 이해 능력과 생성 능력을 갖춘 인공지능 시스템이다. 이러한 LLM 기술은 의학 분야에서도 큰 관심을 받기 시작했으며, 의학 논문과 교과서를 학습한 LLM이 의사 면허 시험에 합격하는 성과를 보이며 그 가능성을 입증하였다.

실제로 방대한 의학 문헌을 학습한 ChatGPT-4는 미국 의사면허시험(USMLE)에서 합격권 점수를 획득하였고, 일본 의사국가고시, 한국 치과의사국가고시 등에서도 합격 수준의 성적을 거두었다. 이는 LLM이 상당한 수준의 의학 지식을 보유하고 있음을 보여주는 결과였다. 이러한 성과를 바탕으로 LLM을 의료 상담이나 건강 정보 제공에 활용하려는 연구가 활발히 진행되고 있다.

### 1.1.3 대규모 언어모델의 한계

하지만 LLM을 의료 분야에 적용하는 데는 여전히 심각한 한계가 존재한다. 가장 큰 문제는 바로 '환각(Hallucination)' 현상이다. 환각이란 LLM이 사실에 근거하지 않은 정보를 마치 사실인 것처럼 그럴듯하게 생성하는 현상을 말한다. 예를 들어 임산부에게 금지된 약물을 처방하거나, 심각한 증상을 보이는 환자에게 안락사를 권유하는 등 의학적으로 부적절하거나 위험한 답변을 생성하는 경우가 보고되었다. 의료는 사람의 생명과 직결되는 분야이기 때문에, 이러한 오류는 단순한 실수를 넘어 환자의 건강을 심각하게 해치거나 심지어 사망에 이르게 할 수 있는 치명적인 문제이다.

또 다른 중요한 문제는 LLM이 개인화된 답변을 제공하는 데 취약하다는 점이다. 의료 상담에서는 환자의 나이, 성별, 기저 질환, 복용 중인 약물, 알레르기 여부 등 개인의 특성을 종합적으로 고려하여 조언을 제공해야 한다. 같은 증상이라도 20대 건강한 성인과 70대 당뇨 환자에게는 전혀 다른 처방과 조언이 필요하다. 하지만 현재의 LLM은 이러한 개인적 맥락을 충분히 고려하지 못하고 일반적인 의학 지식만을 나열하는 경향이 있다.

### 1.1.4 멀티 턴 대화에서의 맥락 손실 문제

더욱 심각한 문제는 연속된 대화, 즉 멀티 턴 대화에서 발생한다. 실제 병원에서 의사와 환자의 상담은 한 번의 질문과 답변으로 끝나지 않는다. 의사는 환자의 증상을 듣고 추가 질문을 하며, 환자는 궁금한 점을 계속해서 물어본다. 이 과정에서 의사는 앞서 들었던 환자의 정보를 계속 기억하고 이를 바탕으로 진단과 처방을 내린다.

하지만 현재의 LLM은 이러한 지속적인 맥락 유지에 취약하다. 예를 들어 2025년 11월 기준, 로그인하지 않은 상태의 ChatGPT(4o-mini)에 다음과 같이 질문했을 때의 반응을 살펴보자.

**첫 번째 대화**
- 사용자: "저는 65세 남성으로 10년째 당뇨 환자입니다."
- ChatGPT: "말씀해주셔서 감사합니다. 65세 남성이시고, 당뇨병을 10년째 관리 중이시군요. 지금 말씀해주신 정보만으로는 어떤 도움을 원하시는지 정확히 알기 어렵습니다. 당뇨 관리 방법을 알고 싶으신가요? 최근에 불편한 증상이 있으신가요?..."

**두 번째 대화**
- 사용자: "운동은 어떻게 하면 좋을까요?"
- ChatGPT: "운동은 당뇨병 관리에 매우 도움이 되지만, 나이·건강 상태·현재 복용약 등을 고려해 안전하고 지속 가능한 방식으로 하시는 것이 중요합니다. 어떤 운동이 좋은가요? ① 유산소 운동: 빠르게 걷기, 가벼운 자전거 타기, 수영..."

위 사례를 보면 ChatGPT가 첫 번째 대화에서 사용자가 65세 남성이고 10년째 당뇨 환자라는 정보를 인지하기는 했지만, 두 번째 질문에 대한 답변에서는 이러한 개인 정보를 구체적으로 반영한 맞춤형 조언을 제공하기보다는 일반적인 당뇨 환자를 위한 운동 방법을 나열하는 데 그쳤다.

이러한 문제는 학술 연구에서도 여러 차례 지적되었다. 많은 연구에서 LLM은 한두 턴의 짧은 대화에서는 중요한 정보를 잘 기억하지만, 대화가 길어질수록 초반에 언급된 중요한 정보를 잃어버리는 '맥락 손실(Context Loss)' 현상을 보인다고 보고하였다. 또한 앞서 제공했던 답변과 완전히 모순되는 내용을 나중에 제시하는 일관성 결여 문제도 관찰되었다. 환자의 생명과 건강이 걸린 의료 분야에서 이러한 문제는 단순한 불편함을 넘어 심각한 안전 문제로 이어질 수 있다.

### 1.1.5 Context Engineering의 필요성

이러한 LLM의 한계를 극복하기 위해 본 연구에서는 'Context Engineering'이라는 체계적인 접근 방식을 제안한다. Context Engineering은 실제 병원에서 의사가 환자를 진료하는 과정에서 착안한 개념이다. 의사는 환자와의 대화에서 중요한 증상, 검사 수치, 과거 병력 등을 차트에 꼼꼼히 기록한다. 그리고 다음 진료 시 이 차트를 참고하여 환자의 상태 변화를 파악하고 그에 맞는 처방을 내린다.

본 연구의 Context Engineering은 이러한 의료 현장의 프로세스를 AI 시스템에 적용한 것이다. 구체적으로 대화형 AI Agent가 환자와의 매 대화마다 중요한 의학 정보를 자동으로 추출하고, 이를 구조화된 형태로 메모리에 저장하며, 다음 답변을 생성할 때 이 정보를 활용하고, 최종적으로 생성된 답변이 환자에게 적절한지 검증하는 4단계 과정을 반복한다. 이를 통해 LLM의 맥락 손실 문제를 해결하고 개인화되고 일관성 있는 의료 상담을 제공할 수 있다.

## 제2절 연구의 목표

본 연구의 목표는 Context Engineering을 기반으로 멀티 턴 대화 환경에서 사용자의 개인적인 의학 정보를 효과적으로 파악하고 이를 반영한 신뢰할 수 있는 답변을 제공하는 의학지식 AI Agent를 설계하고 구현하는 것이다. 구체적인 세부 목표는 다음과 같다.

### 1.2.1 Context Engineering 시스템의 설계 및 구현

첫 번째 목표는 멀티 턴 대화에서 효과적으로 작동하는 Context Engineering 시스템을 구축하는 것이다. 본 시스템은 다음 4단계로 구성된다.

**추출(Extraction) 단계**에서는 사용자가 입력한 질문이나 대화 내용에서 의학적으로 중요한 정보를 자동으로 감지한다. 예를 들어 "저는 65세 남성이고 당뇨가 있습니다"라는 문장에서 나이(65세), 성별(남성), 질환(당뇨)을 추출한다. 이를 위해 의학 용어를 인식하는 MedCAT2라는 전문 도구와 정규표현식 기반 패턴 매칭을 함께 사용한다.

**저장(Storage) 단계**에서는 추출한 정보를 체계적으로 보관한다. 단순히 대화 내용을 그대로 저장하는 것이 아니라, 인구통계 정보(나이, 성별), 질환, 증상, 복용 약물, 생체 징후(혈압, 체온 등), 검사 수치 등 6개 카테고리로 분류하여 구조화된 형태로 저장한다. 또한 시간이 지남에 따라 정보의 중요도가 달라지는 것을 반영하기 위해 시간 가중치를 적용한다.

**주입(Injection) 단계**에서는 저장된 개인 정보를 활용하여 답변을 생성한다. LLM에게 질문만 전달하는 것이 아니라, 해당 사용자의 나이, 성별, 질환, 증상 등의 정보와 이전 대화 내용, 그리고 검색된 의학 문서를 함께 제공한다. 이를 통해 LLM이 환자의 개별 상황을 고려한 맞춤형 답변을 생성할 수 있도록 한다.

**검증(Verification) 단계**에서는 생성된 답변이 의학적으로 적절하고 안전한지 자동으로 평가한다. 답변이 검색된 의학 문서에 근거하고 있는지, 질문에 충분히 답하고 있는지, 환자에게 위험하지 않은지 등을 LLM을 활용하여 자체적으로 검증한다. 만약 품질이 낮다고 판단되면 질문을 다시 구성하여 재검색을 수행하는 자기개선 과정을 거친다.

### 1.2.2 고도화된 의료 AI Agent의 설계

두 번째 목표는 위의 Context Engineering을 실제로 구현한 고성능 의료 AI Agent를 구축하는 것이다. 이를 위해 LangGraph라는 상태 기반 워크플로우 프레임워크를 사용하여 10개의 전문 노드를 순환형 구조로 연결한다.

본 시스템은 다음과 같은 혁신적인 메커니즘을 포함한다. 첫째, **응답 캐시 시스템**은 이전에 비슷한 질문에 답변한 적이 있는지 확인하여, 있다면 검증된 답변을 재사용함으로써 불필요한 처리를 줄인다. 둘째, **능동적 검색 시스템**은 질문의 복잡도를 자동으로 분류하고, 단순한 질문에는 적은 수의 문서만 검색하고 복잡한 질문에는 많은 문서를 검색하여 효율성을 높인다. 셋째, **하이브리드 검색 시스템**은 키워드 기반 검색(BM25)과 의미 기반 벡터 검색(FAISS)을 결합하여 더 정확한 문서를 찾아낸다. 넷째, **자기개선 메커니즘**은 생성된 답변의 품질을 스스로 평가하고, 품질이 낮으면 질문을 개선하여 다시 검색하는 과정을 반복한다.

이러한 메커니즘들을 통해 시스템의 정확성, 효율성, 안전성을 동시에 향상시키는 것을 목표로 한다.

### 1.2.3 정량적 성능 평가 및 검증

세 번째 목표는 구현한 시스템의 성능을 객관적으로 측정하고 검증하는 것이다. 이를 위해 Synthea라는 가상 환자 생성 프레임워크를 사용하여 실제 환자와 유사한 80명의 가상 환자 데이터를 생성한다. 각 환자는 나이, 성별, 질환, 증상, 복용 약물 등 실제 환자 통계를 반영한 정보를 가지고 있다.

이 80명의 가상 환자와 각각 5턴의 대화를 수행하여 총 400턴의 대화 데이터를 생성하고, 이를 바탕으로 다음과 같은 평가 지표를 측정한다. **Faithfulness**는 답변이 검색된 의학 문서에 얼마나 충실하게 근거하고 있는지를 측정한다. **Answer Relevance**는 답변이 질문과 얼마나 관련성이 높은지를 평가한다. **Perplexity**는 LLM이 답변을 생성할 때의 불확실성을 나타내며, 낮을수록 더 자신 있게 답변을 생성한 것을 의미한다.

또한 각 기능의 기여도를 정량적으로 파악하기 위해 Feature flag 기반 Ablation Study를 수행한다. 예를 들어 응답 캐시만 끈 상태, 능동적 검색만 끈 상태 등 다양한 조합으로 실험하여 각 기능이 전체 성능에 얼마나 기여하는지 측정한다. 모든 결과는 통계적 검정을 통해 유의성을 검증한다.

## 제3절 논문의 구성

본 논문은 총 5개의 장으로 구성되며, 각 장의 주요 내용은 다음과 같다.

**제1장 서론**에서는 연구의 배경과 필요성을 설명하였다. 필수의료 위기 상황에서 LLM의 가능성과 한계를 분석하고, 특히 멀티 턴 대화에서의 맥락 손실 문제와 개인화 실패 문제를 지적하였다. 이를 해결하기 위한 Context Engineering의 필요성을 제시하고, 본 연구의 목표를 명확히 하였다.

**제2장 관련 연구 및 배경지식**에서는 본 연구와 관련된 선행 연구를 검토한다. 대규모 언어모델의 발전 과정과 의료 분야 적용 사례, 검색 증강 생성(RAG) 기술과 Corrective RAG의 등장 배경, 그리고 최근 주목받고 있는 Context Engineering 관련 연구를 살펴본다. 이를 통해 기존 연구의 한계점을 파악하고 본 연구의 차별점을 명확히 한다.

**제3장 연구방법론**에서는 본 연구에서 제안하는 시스템의 설계와 구현 방법을 상세히 기술한다. Context Engineering의 4단계(추출, 저장, 주입, 검증) 각각의 구현 방법, LangGraph 기반의 10개 노드 아키텍처, 응답 캐시·능동적 검색·하이브리드 검색·자기개선 메커니즘 등 핵심 기능의 작동 원리를 설명한다. 또한 시스템의 성능을 최적화하기 위한 다양한 기법도 소개한다.

**제4장 실험 설계 및 결과**에서는 시스템의 성능을 평가한 실험 과정과 결과를 제시한다. Synthea 기반 가상 환자 생성 방법, 실험 설계 전략, 평가 지표의 선정 이유와 측정 방법을 설명한다. 그리고 일반 LLM 모드와 본 연구의 AI Agent 모드를 비교한 정량적 결과와 통계적 검증 결과를 제시하며, Ablation Study를 통해 각 기능의 기여도를 분석한다.

**제5장 결론**에서는 연구 결과를 요약하고 연구의 의의를 정리한다. 본 연구가 의료 AI 분야에 기여한 점을 기술하고, 연구의 한계점을 솔직히 인정하며, 향후 연구 방향을 제시한다.

---

# 제2장 관련 연구 및 배경지식

## 제1절 대규모 언어 모델(LLM)

### 2.1.1 대규모 언어 모델의 개념과 발전

대규모 언어모델(Large Language Model, LLM)은 방대한 양의 텍스트 데이터를 학습하여 인간 수준의 언어 이해 및 생성 능력을 갖춘 인공지능 시스템이다. LLM은 수십억에서 수천억 개의 매개변수(parameter)를 가진 신경망 구조로 이루어져 있으며, 인터넷상의 문서, 책, 논문 등 다양한 텍스트를 학습하여 언어의 패턴과 의미를 파악한다.

LLM의 발전은 2017년 구글이 발표한 트랜스포머(Transformer) 아키텍처에서 시작되었다. 트랜스포머는 문장 내 단어들 간의 관계를 효과적으로 학습할 수 있는 구조로, 이전의 순환 신경망(RNN) 기반 모델들이 가졌던 한계를 극복하였다. 이후 OpenAI가 2018년 GPT(Generative Pre-trained Transformer)를 발표하면서 대규모 사전학습(pre-training)과 미세조정(fine-tuning)을 통한 범용 언어모델의 가능성을 입증하였다.

특히 2022년 11월 출시된 ChatGPT는 LLM 기술을 대중화하는 데 결정적인 역할을 하였다. ChatGPT는 단순히 문장을 생성하는 것을 넘어 사용자와 자연스러운 대화를 나누며 질문에 답하고, 문서를 요약하고, 번역을 수행하는 등 다양한 작업을 수행할 수 있었다. 출시 5일 만에 100만 명의 사용자를 확보하며 전 세계적으로 큰 관심을 받았고, 이는 교육, 비즈니스, 의료 등 다양한 분야에서 LLM을 활용하려는 시도로 이어졌다.

### 2.1.2 의료 분야 LLM의 등장과 발전

LLM의 발전은 의료 분야에도 큰 변화를 가져왔다. 의학은 방대한 양의 전문 지식을 필요로 하는 분야로, 새로운 연구 결과가 지속적으로 발표되며 의료진조차 모든 최신 정보를 따라가기 어려운 상황이다. 이러한 환경에서 LLM은 의학 문헌을 빠르게 검색하고 요약하며, 진단을 보조하고, 환자에게 건강 정보를 제공하는 도구로서의 가능성을 보여주었다.

구글은 2022년 말 의료 전문 LLM인 '메드팜(Med-PaLM)'을 공개하였다. 메드팜은 의학 논문과 교과서를 집중적으로 학습한 모델로, 미국 의사면허시험(USMLE)에서 60%의 정확도를 기록하며 합격 기준인 60%를 넘어섰다. 이는 LLM이 의학 지식을 상당 수준으로 이해하고 있음을 보여주는 결과였다. 2023년에 발표된 후속 모델인 메드팜2(Med-PaLM 2)는 정확도를 85%까지 끌어올리며 전문의 수준에 근접하는 성능을 보였다.

그러나 메드팜 시리즈는 주로 단일 질문에 대한 답변, 즉 단일 턴 환경에 최적화되어 있었다. 의사 면허 시험처럼 하나의 질문에 정확한 답을 선택하는 방식에서는 우수한 성능을 보였지만, 실제 진료실에서 이루어지는 연속적인 대화, 즉 멀티 턴 상황에서는 한계를 드러냈다. 환자의 나이, 성별, 기저 질환 등 개인적 정보를 지속적으로 기억하고 이를 바탕으로 맞춤형 조언을 제공하는 능력은 여전히 부족하였다.

### 2.1.3 범용 LLM의 의료 분야 활용과 한계

한편 ChatGPT나 구글의 Gemini와 같은 범용 LLM도 의료 정보 제공에 활용되기 시작하였다. 이들은 의료 전문 LLM은 아니지만, 학습 데이터에 포함된 의학 관련 문서를 바탕으로 일반적인 건강 질문에 대해 상당히 유용한 답변을 제공할 수 있었다. 예를 들어 "감기와 독감의 차이는 무엇인가요?" 같은 일반적인 질문에는 정확하고 이해하기 쉬운 설명을 제공하였다.

하지만 이러한 범용 LLM도 의료 분야에 적용하는 데는 여러 한계가 있었다. 첫째, 앞서 언급한 환각(Hallucination) 문제가 있다. LLM은 학습 데이터에 근거하지 않거나 사실이 아닌 정보를 마치 사실인 것처럼 자신감 있게 제시하는 경우가 있다. 의료 분야에서 이러한 오류는 환자의 건강에 직접적인 위협이 될 수 있어 특히 위험하다.

둘째, 개인화 부족 문제가 있다. 같은 두통이라도 20대 건강한 성인의 두통과 70대 고혈압 환자의 두통은 원인도 다르고 치료법도 달라야 한다. 하지만 LLM은 이러한 개인적 맥락을 충분히 고려하지 못하고 일반적인 정보만을 제공하는 경향이 있다.

셋째, 멀티 턴 대화에서의 맥락 손실 문제가 있다. 실제 진료는 여러 차례의 질문과 답변으로 이루어지는데, LLM은 대화가 길어질수록 초반에 언급된 중요한 정보를 잊어버리는 경향이 있다. 예를 들어 첫 번째 질문에서 환자가 "저는 임산부입니다"라고 말했는데, 다섯 번째 질문에서는 이를 고려하지 않은 답변을 제공하는 식이다.

### 2.1.4 LLM의 한계 요약

기존 LLM 및 의료 특화 LLM이 실제 임상 환경에서 환자 맞춤형 진료 보조 도구로 활용되기 위해서는 다음과 같은 세 가지 핵심 문제를 해결해야 한다.

첫째, **맥락 손실(Context Loss)** 문제이다. 단일 질의응답이 아닌 연속된 대화 환경에서 LLM은 이전 대화의 핵심 내용을 점차 잊어버려 대화의 흐름이 끊기고 일관성이 떨어진다. 이는 환자의 건강 상태를 지속적으로 추적해야 하는 의료 상담에서 치명적인 약점이다.

둘째, **개인화 실패(Personalization Failure)** 문제이다. 환자의 연령, 성별, 질환, 증상, 복용 약물 등 개별적 특성을 답변 생성 시 정밀하게 반영하지 못하고 일반론적 답변만 제공하는 경향이 있다. 의료는 본질적으로 개인 맞춤형 서비스이므로 이는 큰 한계이다.

셋째, **안전성 부족(Safety Issues)** 문제이다. 환각 현상으로 인해 사실이 아니거나 환자의 안전에 위협이 될 수 있는 부적절한 답변이 생성될 위험이 있다. 의학적 정확성이 무엇보다 중요한 의료 분야에서는 이러한 위험을 최소화하는 것이 필수적이다.

## 제2절 검색 증강 생성(RAG)

### 2.2.1 RAG의 개념과 등장 배경

검색 증강 생성(Retrieval-Augmented Generation, RAG)은 LLM의 한계를 극복하기 위해 등장한 기술이다. RAG는 LLM이 답변을 생성하기 전에 먼저 외부 데이터베이스에서 관련 문서를 검색하고, 검색된 문서를 참고하여 답변을 생성하는 방식이다. 이는 LLM이 학습 데이터만에 의존하는 대신, 신뢰할 수 있는 외부 지식을 활용하도록 하여 답변의 정확성과 신뢰성을 높인다.

RAG가 필요한 이유는 LLM이 가진 본질적인 한계 때문이다. LLM은 학습 시점의 데이터만을 알고 있기 때문에, 학습 이후에 발생한 최신 정보를 반영하지 못한다. 예를 들어 2023년에 학습된 모델은 2024년에 발표된 새로운 치료법을 알 수 없다. 또한 LLM은 방대한 양의 데이터를 학습하지만, 모든 세부 사항을 정확히 기억하지는 못한다. 이로 인해 특정 질문에 대해 부정확한 답변을 제공하거나, 환각 현상으로 사실이 아닌 내용을 생성하기도 한다.

RAG는 이러한 문제를 외부 지식 베이스를 참조함으로써 해결한다. 최신 의학 논문, 진료 가이드라인, 약물 정보 등을 데이터베이스에 저장해두고, 사용자의 질문이 들어오면 관련된 문서를 검색하여 LLM에게 함께 제공한다. LLM은 이 문서들을 참고하여 근거 있는 답변을 생성하게 된다.

### 2.2.2 RAG의 작동 원리

RAG의 작동 과정은 크게 세 단계로 이루어진다.

첫 번째는 **질의 처리(Query Processing)** 단계이다. 사용자가 "당뇨 환자에게 좋은 운동은 무엇인가요?"라는 질문을 입력하면, 시스템은 이 질문을 컴퓨터가 이해할 수 있는 수치 형태인 벡터로 변환한다. 이를 임베딩(embedding)이라고 하며, 질문의 의미를 수학적으로 표현한 것이다.

두 번째는 **검색(Retrieval)** 단계이다. 변환된 질문 벡터와 데이터베이스에 미리 저장된 문서들의 벡터를 비교하여, 의미적으로 가장 유사한 문서들을 찾아낸다. 예를 들어 당뇨 환자의 운동에 관한 의학 논문, 진료 가이드라인 등이 검색될 것이다. 일반적으로 상위 3~10개의 가장 관련성 높은 문서를 선택한다.

세 번째는 **생성(Generation)** 단계이다. LLM은 원래 사용자의 질문과 함께 검색된 문서들을 입력으로 받는다. 그리고 이 문서들에 기반하여 답변을 생성한다. 예를 들어 "검색된 논문에 따르면, 당뇨 환자에게는 하루 30분의 중강도 유산소 운동이 권장됩니다"와 같이 근거를 명시한 답변을 제공할 수 있다.

### 2.2.3 RAG의 장점

RAG 방식은 전통적인 LLM 방식에 비해 여러 장점을 가진다.

첫째, **근거 기반 답변**을 제공할 수 있다. LLM이 학습 데이터에만 의존하는 것이 아니라, 명확한 출처가 있는 문서를 참고하므로 답변의 신뢰도가 높아진다. 특히 의료 분야처럼 정확성이 중요한 영역에서는 "어떤 논문이나 가이드라인에 근거한 답변인지"를 명시할 수 있어 유용하다.

둘째, **최신 정보 반영**이 용이하다. LLM 모델 자체를 재학습시키는 것은 막대한 비용과 시간이 소요되지만, RAG는 데이터베이스의 문서만 업데이트하면 된다. 새로운 의학 논문이나 치료 가이드라인이 발표되면 이를 데이터베이스에 추가하기만 하면 바로 시스템에 반영된다.

셋째, **전문 분야 특화**가 가능하다. 의학, 법률, 공학 등 특정 분야의 전문 문서만을 데이터베이스에 구축하면, 해당 분야에 특화된 질의응답 시스템을 구축할 수 있다. 범용 LLM을 전문 분야에 맞게 재학습시키는 것보다 훨씬 효율적이다.

### 2.2.4 기존 RAG의 한계

그러나 기존의 RAG 방식도 한계가 존재한다.

첫째, **오류 전파(Error Propagation)** 문제이다. 검색 단계에서 잘못된 문서를 가져오거나, 질문과 관련 없는 문서를 가져오면 LLM이 이를 바탕으로 부정확한 답변을 생성하게 된다. 즉, 검색의 품질이 전체 시스템의 품질을 결정하는데, 한 번의 검색 실패를 수정할 메커니즘이 없다.

둘째, **개인화 부족** 문제이다. 기존 RAG는 외부의 일반적인 의학 문서를 검색하는 데 초점을 맞추고 있어, 특정 사용자의 개인적인 건강 정보를 반영하기 어렵다. 예를 들어 "운동은 어떻게 해야 하나요?"라는 질문에 대해 일반적인 운동 가이드를 검색할 수는 있지만, 해당 사용자가 65세 당뇨 환자라는 점을 고려한 맞춤형 문서를 찾기는 어렵다.

셋째, **피드백 메커니즘 부재**이다. 초기 검색에서 적절한 문서를 찾지 못했을 때 이를 감지하고 자동으로 재검색을 수행하는 기능이 없다. 한 번의 검색 결과로 답변을 생성하는 일방향적 구조이기 때문에, 검색 실패 시 품질이 낮은 답변이 그대로 출력될 수 있다.

### 2.2.5 Corrective RAG (CRAG)

이러한 기존 RAG의 한계를 극복하기 위해 **Corrective RAG(CRAG)**가 제안되었다. CRAG는 검색된 문서의 품질을 자동으로 평가하고, 품질이 낮을 경우 추가 조치를 취하는 자기 수정(self-corrective) 메커니즘을 포함한다.

CRAG의 핵심 아이디어는 다음과 같다. 먼저 일반적인 RAG와 동일하게 문서를 검색한다. 그 다음 검색된 문서들이 질문에 얼마나 적합한지를 평가 모델로 채점한다. 만약 적합도가 높다면(예: 80점 이상) 그대로 답변을 생성한다. 적합도가 중간 정도라면(예: 60~80점) 웹 검색 등을 통해 추가 정보를 보완한다. 적합도가 낮다면(예: 60점 미만) 질문을 다시 구성하여 재검색을 수행하거나, LLM의 내부 지식만으로 답변을 생성한다.

이러한 순환적 구조를 통해 CRAG는 초기 검색 실패를 자동으로 감지하고 수정하여 답변의 품질을 높일 수 있다. 하지만 CRAG 역시 주로 정보의 정확성을 높이는 데 초점을 맞추고 있어, 사용자의 개인적인 맥락을 지속적으로 추적하고 반영하는 '개인화' 측면에서는 여전히 한계가 있다.

## 제3절 Context Engineering 관련 연구

### 2.3.1 Context Engineering의 개념과 중요성

최근 LLM을 활용하는 학계와 산업계에서는 **Context Engineering**의 중요성이 크게 부각되고 있다. Context Engineering은 단순히 좋은 프롬프트를 작성하는 프롬프트 엔지니어링을 넘어서, 사용자의 상황, 환경, 이전 상호작용 등 포괄적인 '맥락(context)'을 AI 시스템이 어떻게 파악하고 기억할 것인지, 더 나아가 축적된 맥락 정보를 어떻게 체계적으로 선별하고 재구성하여 사용자 맞춤형 결과로 출력할 것인지를 다루는 시스템적 방법론이다.

Context Engineering이 중요한 이유는 LLM의 활용 패턴이 변화하고 있기 때문이다. 초기에는 LLM을 단발적인 질문에 답하는 도구로 사용했다면, 이제는 지속적인 대화 상대나 업무 보조자로 활용하는 경우가 많아졌다. 예를 들어 고객 서비스 챗봇은 고객과 여러 차례의 대화를 나누며 문제를 해결해야 하고, 개인 비서 AI는 사용자의 선호도와 일정을 기억하며 맞춤형 제안을 해야 한다. 이러한 환경에서는 단순히 현재 질문에만 답하는 것이 아니라, 과거의 맥락을 이해하고 미래의 필요를 예측하는 능력이 필요하다.

의료 분야에서 Context Engineering의 중요성은 더욱 크다. 환자와의 상담은 본질적으로 멀티 턴 대화이며, 환자의 증상, 병력, 검사 결과 등이 시간에 따라 변화한다. 의사는 이러한 정보를 차트에 기록하고 매번 참고하며 진료를 이어간다. AI 의료 상담 시스템도 마찬가지로 환자의 정보를 체계적으로 관리하고 활용할 수 있어야 한다.

### 2.3.2 학술 연구에서의 Context Engineering

학술 연구 차원에서도 Context Engineering에 대한 연구가 활발히 진행되고 있다. 중국과학원은 최근 발표한 논문에서 LLM을 단순히 프롬프트 설계나 모듈의 조합으로만 보는 시각에서 벗어나야 함을 강조하였다. 그들은 프롬프트, RAG, 메모리 모듈 등 개별 구성요소를 수학적이고 공학적으로 통합하고 최적화하여, LLM이 복잡한 맥락을 효과적으로 처리할 수 있도록 하는 거시적 엔지니어링 시스템으로서의 Context Engineering을 제안하였다.

이 연구는 Context Engineering을 구성하는 주요 요소로 다음을 제시하였다. 첫째, **맥락 인식(Context Awareness)**은 사용자의 현재 상황과 과거 이력을 파악하는 능력이다. 둘째, **맥락 저장(Context Storage)**은 파악한 정보를 효율적으로 저장하고 관리하는 메커니즘이다. 셋째, **맥락 선택(Context Selection)**은 저장된 정보 중 현재 작업에 가장 관련성 높은 정보를 선별하는 과정이다. 넷째, **맥락 주입(Context Injection)**은 선별된 정보를 LLM의 입력에 적절히 통합하는 기법이다.

또 다른 연구에서는 멀티 턴 대화에서 중요한 정보가 대화가 길어질수록 잊혀지는 '중간 손실(lost in the middle)' 문제를 지적하였다. LLM은 프롬프트의 처음과 끝 부분은 잘 기억하지만, 중간 부분의 정보는 상대적으로 덜 반영하는 경향이 있다. 이를 해결하기 위해서는 중요한 정보를 프롬프트의 전략적 위치에 배치하거나, 중요도에 따라 정보를 압축하는 등의 Context Engineering 기법이 필요하다.

### 2.3.3 상용 LLM 서비스의 Context Engineering 적용

Context Engineering의 중요성은 상용 LLM 서비스의 진화에서도 명확히 드러난다. OpenAI의 ChatGPT는 2024년부터 '맞춤 설정(Custom Instructions)' 기능을 도입하여 사용자가 자신의 직업, 관심사, 선호하는 답변 스타일 등을 미리 설정할 수 있도록 하였다. 이렇게 설정된 정보는 모든 대화에 자동으로 반영되어, 사용자마다 개인화된 경험을 제공한다.

더 나아가 ChatGPT는 '메모리(Memory)' 기능을 추가하였다. 이는 이전 대화 내용을 기억하고 있다가 나중에 관련된 질문이 나왔을 때 자동으로 활용하는 기능이다. 예를 들어 사용자가 "나는 채식주의자야"라고 한 번 언급하면, 이후 식사 추천을 요청할 때 자동으로 채식 메뉴만을 제안하는 식이다. 이는 명시적인 맥락 저장과 활용의 사례이다.

구글의 Gemini도 '개인별 맞춤 AI' 설정을 통해 사용자의 Gmail, Google Drive 등에 저장된 정보를 연동하여 맥락을 풍부하게 만든다. 예를 들어 "내 다음 회의가 언제지?"라고 물으면 캘린더 정보를 참조하여 답하고, "그 회의 준비 자료 만들어줘"라고 하면 과거 이메일이나 문서를 참조하여 자료를 생성한다.

이러한 변화는 LLM 기술의 발전 방향이 단순히 "더 정확한 답변"에서 "사용자의 맥락을 이해한 최적의 개인화 답변"으로 이동하고 있음을 보여준다.

### 2.3.4 의료 분야에서의 Context Engineering

의료 분야에서 Context Engineering은 특히 중요하다. 환자의 건강 상태는 정적이지 않고 동적으로 변화하며, 여러 요인이 복합적으로 작용한다. 예를 들어 혈압은 시간대나 활동에 따라 달라지고, 증상은 날씨나 스트레스에 영향을 받는다. 이러한 복잡한 맥락을 파악하고 추적하는 것은 정확한 진단과 치료를 위해 필수적이다.

일부 연구에서는 의료 대화 시스템에 환자 프로필 관리 기능을 추가하여 맥락 유지를 시도하였다. 예를 들어 환자의 기본 정보(나이, 성별), 진단명, 복용 약물 등을 구조화된 형태로 저장하고, 매 대화마다 이를 LLM에게 제공하는 방식이다. 하지만 대부분의 연구는 정적인 프로필 저장에 그쳤고, 대화 중에 새로 언급된 정보를 자동으로 추출하여 업데이트하거나, 정보의 시간적 변화를 추적하는 동적 관리까지는 구현하지 못하였다.

또한 의학 용어 추출을 위한 도구들이 개발되었는데, 그 중 대표적인 것이 **MedCAT(Medical Concept Annotation Tool)**이다. MedCAT은 자연어 텍스트에서 질환, 증상, 약물 등의 의학 개념을 자동으로 인식하고, 이를 UMLS(Unified Medical Language System)라는 표준 의학 용어 체계와 연결해준다. 예를 들어 "머리가 아파요"라는 문장에서 '두통'이라는 증상을 추출하고, 이를 UMLS 코드 C0018681과 매핑하는 식이다. 최신 버전인 MedCAT2는 딥러닝 기반 개체명 인식을 통해 정확도를 크게 향상시켰다.

## 제4절 시사점

### 2.4.1 기존 연구의 한계

앞서 살펴본 선행 연구와 기술 현황을 종합하면, 다음과 같은 한계점을 발견할 수 있다.

첫째, **멀티 턴 의료 상담을 위한 체계적인 맥락 관리 부족**이다. 기존의 의료 특화 LLM(메드팜 등)은 의학 지식의 정확도는 높았으나, 대화가 지속될수록 앞서 언급된 환자의 핵심 정보(증상, 질환 등)를 망각하는 맥락 손실이 빈번하게 발생하였다. 환자의 건강 상태를 동적으로 추적하며 적절한 조언을 제공해야 하는 의료 상담에서 이는 치명적인 약점이다.

둘째, **의학 지식의 정확성과 개인화를 동시에 달성하는 방법론 부재**이다. 기존 RAG는 일반적인 의학 문서를 검색하여 환각 현상을 방지하는 데는 기여했으나, 사용자의 개별적 특성을 반영한 맞춤형 답변 생성에서는 한계를 보였다. 또한 일방향적 구조로 인해 검색 실패 시 이를 수정할 메커니즘이 없었다.

셋째, **정적인 프롬프트 엔지니어링의 한계**이다. 기존의 많은 연구는 고정된 프롬프트 템플릿을 사용하거나 단순한 모듈 조합에 그쳤다. 하지만 실제 의료 상담은 환자의 상태, 질문의 복잡도, 대화의 진행 상황에 따라 동적으로 대응해야 한다. 이를 위해서는 프롬프트 엔지니어링을 넘어 전체 시스템을 체계적으로 설계하는 Context Engineering이 필요하다.

### 2.4.2 본 연구의 차별점

이러한 한계점들을 바탕으로 본 연구가 추구하는 차별점은 다음과 같다.

첫째, **4단계 Context Engineering 파이프라인 구축**이다. 추출(Extraction) - 저장(Storage) - 주입(Injection) - 검증(Verification)의 4단계를 매 대화마다 반복하여, 환자의 의학 정보를 지속적으로 추적하고 활용한다. 이는 의사가 차트를 작성하고 참고하는 과정을 시스템화한 것이다.

둘째, **Corrective RAG 기반의 순환형 자기개선 구조**이다. 단순히 한 번 검색하고 답변을 생성하는 것이 아니라, 생성된 답변의 품질을 자체 평가하고 필요시 질문을 재구성하여 재검색을 수행한다. 이를 통해 초기 검색 실패를 자동으로 수정하고 답변의 품질을 높인다.

셋째, **효율성과 정확성을 동시에 추구하는 고도화된 메커니즘**이다. 응답 캐시를 통해 유사 질문은 재사용하고, 능동적 검색을 통해 질문의 복잡도에 따라 검색량을 조절하며, 하이브리드 검색을 통해 검색 정확도를 높인다. 이는 단순히 성능만 추구하는 것이 아니라 실용성을 고려한 설계이다.

넷째, **의료 도메인에 특화된 구현**이다. MedCAT2를 활용한 의학 용어 추출, UMLS 기반 표준화, 6개 카테고리(인구통계, 질환, 증상, 약물, 생체 징후, 검사 수치)로 구조화된 슬롯 관리, 시간 가중치를 통한 동적 추적 등 의료 분야의 특수성을 반영한 다양한 기법을 적용한다.

다섯째, **정량적 평가 및 검증**이다. Synthea를 활용한 현실적인 가상 환자 생성, Faithfulness, Answer Relevance, Perplexity 등 다양한 메트릭 측정, 통계적 유의성 검증, Feature flag 기반 Ablation Study를 통해 각 기능의 기여도를 객관적으로 측정한다.

### 2.4.3 연구의 방향

종합하면, 그동안의 연구들은 LLM의 성능 개선이나 의학 지식의 정확도 향상에 주로 집중해 왔다. 하지만 멀티 턴 환경에서의 개인화된 맥락 유지와 반영에 대한 연구는 상대적으로 부족하였다. 본 연구는 이러한 간극을 메우기 위해 **Corrective RAG를 기반으로 하되, Context Engineering을 체계적으로 통합한 의학지식 AI Agent**를 제안한다.

이는 단순히 정확한 의학 정보를 제공하는 것을 넘어, 환자 개개인의 상황을 지속적으로 파악하고 이에 맞춘 개인화된 상담을 제공하는 것을 목표로 한다. 실제 의사가 환자를 진료하는 방식을 AI 시스템에 구현함으로써, 보다 인간적이고 실용적인 의료 AI 챗봇을 실현하고자 한다.

---

# 제3장 연구방법론

## 제1절 연구 목표 및 접근 방법

### 3.1.1 전체 시스템 개요

본 연구에서 제안하는 시스템은 Context Engineering 원리를 기반으로 한 의학지식 AI Agent이다. 이 시스템은 사용자와 멀티 턴 대화를 나누면서 환자의 의학 정보를 지속적으로 추출하고 저장하며, 이를 바탕으로 개인화된 답변을 생성하고 검증하는 전주기적 프로세스를 수행한다.

시스템의 핵심 설계 철학은 "실제 의사의 진료 과정을 AI로 구현한다"는 것이다. 의사는 환자를 진료할 때 다음과 같은 과정을 거친다. 먼저 환자의 증상을 듣고 중요한 정보를 파악한다. 그리고 이 정보를 차트에 기록하여 보관한다. 다음 진료 시에는 이전 차트를 참고하면서 환자의 상태 변화를 확인하고, 이를 바탕으로 진단을 내리고 처방을 제공한다. 마지막으로 자신의 판단이 적절한지 재검토한다.

본 시스템은 이러한 의사의 진료 프로세스를 다음 4단계로 체계화하였다.

**추출(Extraction)** 단계에서는 사용자의 질문이나 대화 내용에서 의학적으로 중요한 정보를 자동으로 찾아낸다. 나이, 성별, 질환, 증상, 복용 중인 약물, 혈압이나 혈당 같은 수치 등이 이에 해당한다. 예를 들어 "저는 65세 남성이고 10년째 당뇨를 앓고 있습니다. 최근 혈당이 150mg/dL 정도 나왔어요"라는 문장에서 나이(65세), 성별(남성), 질환(당뇨, 10년), 검사 수치(혈당 150mg/dL)를 추출한다.

**저장(Storage)** 단계에서는 추출한 정보를 체계적으로 분류하여 보관한다. 단순히 대화 내용을 통째로 저장하는 것이 아니라, 의학적 의미에 따라 6개 카테고리로 나누어 저장한다. 인구통계 정보(demographics)에는 나이, 성별, 직업 등이, 질환(conditions)에는 진단받은 병명이, 증상(symptoms)에는 호소하는 불편함이, 약물(medications)에는 복용 중인 약이, 생체 징후(vitals)에는 혈압, 체온, 맥박 등이, 검사 수치(labs)에는 혈당, 콜레스테롤 등이 저장된다.

**주입(Injection)** 단계에서는 저장된 정보를 활용하여 답변을 생성한다. LLM에게 단순히 "운동은 어떻게 하면 좋을까요?"라는 질문만 전달하는 것이 아니라, "이 사용자는 65세 남성, 10년째 당뇨 환자, 최근 혈당 150mg/dL입니다. 이런 환자에게 운동은 어떻게 하면 좋을까요?"와 같이 개인 정보를 함께 제공한다. 또한 의학 데이터베이스에서 검색한 관련 논문이나 가이드라인도 함께 제공하여, LLM이 근거 있는 맞춤형 답변을 생성하도록 한다.

**검증(Verification)** 단계에서는 생성된 답변이 적절한지 자동으로 평가한다. 답변이 검색된 의학 문서에 제대로 근거하고 있는지, 질문에 충분히 답하고 있는지, 환자에게 위험하지 않은지 등을 확인한다. 만약 품질이 낮다고 판단되면 질문을 다시 구성하여 문서를 재검색하고 답변을 다시 생성하는 과정을 거친다.

이 4단계는 한 번으로 끝나는 것이 아니라 매 대화마다 반복된다. 따라서 대화가 진행될수록 시스템은 환자에 대해 더 많은 정보를 축적하게 되고, 이를 바탕으로 점점 더 정확하고 개인화된 답변을 제공할 수 있게 된다.

### 3.1.2 시스템 아키텍처 설계 원리

본 시스템의 아키텍처는 **LangGraph**라는 프레임워크를 기반으로 설계되었다. LangGraph는 LLM 기반 애플리케이션을 상태 기반 워크플로우로 구현할 수 있게 해주는 도구이다. 쉽게 말해 복잡한 작업을 여러 개의 작은 단계(노드)로 나누고, 각 단계가 어떤 순서로 실행될지를 정의할 수 있게 해준다.

일반적인 프로그램은 순차적으로 실행된다. A를 하고, B를 하고, C를 하는 식이다. 하지만 실제 의료 상담은 훨씬 복잡하다. 때로는 이전 단계로 돌아가야 하고, 상황에 따라 다른 경로를 선택해야 하며, 결과가 만족스럽지 않으면 처음부터 다시 시도해야 한다. LangGraph는 이러한 복잡한 흐름을 '그래프' 구조로 표현할 수 있게 해준다.

본 시스템은 총 10개의 전문 노드로 구성되어 있으며, 각 노드는 특정한 역할을 수행한다.

1. **check_similarity 노드**: 이전에 비슷한 질문에 답한 적이 있는지 확인한다.
2. **classify_intent 노드**: 질문의 복잡도를 분류한다.
3. **extract_slots 노드**: 질문에서 의학 정보를 추출한다.
4. **store_memory 노드**: 추출한 정보를 메모리에 저장한다.
5. **assemble_context 노드**: 저장된 정보를 모아 LLM에 전달할 컨텍스트를 구성한다.
6. **retrieve 노드**: 의학 데이터베이스에서 관련 문서를 검색한다.
7. **generate_answer 노드**: LLM을 사용하여 답변을 생성한다.
8. **refine 노드**: 생성된 답변의 품질을 평가한다.
9. **quality_check 노드**: 품질 평가 결과를 바탕으로 재검색 여부를 결정한다.
10. **store_response 노드**: 검증된 답변을 캐시에 저장한다.

이 10개 노드는 단순히 순차적으로 실행되지 않는다. 조건에 따라 다른 경로를 선택하거나, 이전 노드로 돌아가는 순환 구조를 가진다. 예를 들어 check_similarity 노드에서 비슷한 질문을 발견하면 바로 답변을 제공하고 끝나지만, 발견하지 못하면 전체 파이프라인을 실행한다. 또한 quality_check 노드에서 품질이 낮다고 판단되면 retrieve 노드로 돌아가 재검색을 수행한다.

이러한 순환 구조를 통해 본 시스템은 Corrective RAG의 핵심 개념인 '자기 수정(self-correction)'을 구현한다. 한 번의 시도로 완벽한 답변을 생성하기 어렵다면, 스스로 문제를 감지하고 개선하는 과정을 반복하는 것이다.

### 3.1.3 설계 원칙

본 시스템을 설계할 때 다음과 같은 원칙을 적용하였다.

**첫째, 의료 안전성 우선이다.** 의료는 사람의 생명과 직결되므로, 정확하지 않은 답변을 제공하는 것보다는 "확실하지 않다"고 말하는 것이 낫다. 따라서 검색된 문서에 명확히 근거하지 않은 답변은 생성하지 않도록 설계하였다. 또한 답변의 품질을 자동으로 평가하여 기준에 미달하면 재생성하는 메커니즘을 포함하였다.

**둘째, 효율성과 성능의 균형이다.** 모든 질문에 대해 최대한 많은 문서를 검색하고 복잡한 처리를 수행하면 답변의 품질은 높아질 수 있지만, 응답 시간이 길어지고 비용이 증가한다. 따라서 질문의 복잡도에 따라 처리 방식을 다르게 하여 효율성을 높였다. 단순한 질문에는 적은 자원을, 복잡한 질문에는 많은 자원을 투입하는 '능동적 검색' 전략을 적용하였다.

**셋째, 지속적인 맥락 관리이다.** 대화는 한 번으로 끝나지 않는다. 환자는 여러 번에 걸쳐 질문을 하고, 새로운 증상을 보고하며, 이전 조언에 대한 후속 질문을 한다. 시스템은 이러한 모든 정보를 지속적으로 추적하고 업데이트해야 한다. 이를 위해 시간 가중치를 적용하여 최신 정보를 더 중요하게 여기되, 오래된 정보도 완전히 버리지 않고 유지하는 방식을 채택하였다.

**넷째, 투명성과 추적 가능성이다.** AI의 답변이 어떤 근거로 생성되었는지 알 수 없다면 신뢰하기 어렵다. 따라서 본 시스템은 답변을 생성할 때 참고한 의학 문서의 출처를 명시하고, 사용자의 어떤 개인 정보를 반영했는지 추적할 수 있도록 설계하였다.

## 제2절 Context Engineering 파이프라인

### 3.2.1 추출(Extraction) 단계

#### 3.2.1.1 추출 목표와 대상

추출 단계의 목표는 사용자의 자연어 질문이나 대화 내용에서 의학적으로 의미 있는 정보를 자동으로 찾아내는 것이다. 사람들은 자유로운 형식으로 이야기하기 때문에, 같은 의미라도 다양한 표현을 사용한다. "저는 65세입니다", "나이가 예순다섯이에요", "올해로 65살이 되었어요" 등은 모두 같은 정보를 담고 있다. 시스템은 이러한 다양한 표현을 이해하고 핵심 정보를 추출해야 한다.

본 시스템에서 추출하는 정보는 크게 6개 카테고리로 분류된다.

**인구통계 정보(demographics)**에는 나이, 성별, 직업, 거주 지역 등 환자의 기본적인 배경 정보가 포함된다. 예를 들어 "저는 65세 남성 회사원입니다"에서 나이(65세), 성별(남성), 직업(회사원)을 추출한다.

**질환(conditions)**에는 환자가 현재 앓고 있거나 과거에 앓았던 질병이 포함된다. "10년째 당뇨를 앓고 있고, 5년 전에 고혈압 진단을 받았습니다"에서 당뇨(10년), 고혈압(5년)을 추출한다. 질병명뿐만 아니라 발병 시기나 기간 정보도 함께 추출하여 질환의 경과를 파악한다.

**증상(symptoms)**에는 환자가 호소하는 불편한 증상이 포함된다. "머리가 아프고 어지러워요"에서 두통, 어지러움을 추출한다. "지난주부터 기침이 계속 나와요"에서는 기침과 함께 시작 시점(지난주)도 추출한다.

**약물(medications)**에는 현재 복용 중인 약이 포함된다. "메트포르민을 하루 두 번 먹고 있어요"에서 약물명(메트포르민)과 복용 방법(하루 두 번)을 추출한다.

**생체 징후(vitals)**에는 혈압, 체온, 맥박, 호흡수, 체중 등 측정 가능한 신체 지표가 포함된다. "오늘 아침에 재니까 혈압이 140/90이었어요"에서 혈압 수치와 측정 시점을 추출한다.

**검사 수치(labs)**에는 혈액검사, 소변검사 등의 결과가 포함된다. "지난달 혈당 검사에서 150mg/dL 나왔어요"에서 검사 항목(혈당), 수치(150mg/dL), 검사 시점(지난달)을 추출한다.

#### 3.2.1.2 추출 방법: MedCAT2와 정규표현식의 결합

정보 추출을 위해 본 시스템은 두 가지 방법을 결합하여 사용한다.

첫 번째는 **MedCAT2(Medical Concept Annotation Tool 2)**를 활용한 의학 개념 인식이다. MedCAT2는 자연어 텍스트에서 의학 용어를 자동으로 찾아내고 이를 표준 의학 용어 체계인 UMLS(Unified Medical Language System)와 연결해주는 도구이다.

MedCAT2의 작동 방식을 예를 들어 설명하면 다음과 같다. "저는 당뇨병이 있어서 혈당 관리를 하고 있습니다"라는 문장이 입력되면, MedCAT2는 '당뇨병'을 질환으로 인식하고 이를 UMLS 코드 C0011849(Diabetes Mellitus)와 매핑한다. 또한 '혈당'을 검사 항목으로 인식하고 UMLS 코드 C0005802(Blood Glucose)와 매핑한다. 이렇게 표준 코드와 연결하는 것이 중요한 이유는, 같은 질환을 다양한 표현으로 말할 수 있기 때문이다. '당뇨병', '당뇨', '糖尿病', 'diabetes' 등은 모두 같은 UMLS 코드로 매핑되어 동일한 질환으로 인식된다.

MedCAT2는 딥러닝 기반 개체명 인식(Named Entity Recognition, NER) 모델을 사용하여 문맥을 고려한 정확한 추출이 가능하다. 예를 들어 "당뇨가 있어요"와 "당뇨가 없어요"에서 MedCAT2는 '당뇨'라는 단어를 찾아낼 뿐만 아니라, 전자는 긍정(질환 있음), 후자는 부정(질환 없음)으로 구분할 수 있다.

두 번째는 **정규표현식(Regular Expression)** 기반 패턴 매칭이다. MedCAT2가 의학 용어를 잘 인식하지만, 나이, 성별, 수치 등은 정규표현식으로 더 정확하게 추출할 수 있다.

예를 들어 나이 정보는 다음과 같은 패턴으로 추출한다.
- "65세", "65살" → 숫자 + "세/살" 패턴
- "예순다섯" → 한글 숫자 패턴
- "올해로 65가 되었어요" → "숫자 + 되다" 패턴

성별은 "남성", "여성", "남자", "여자", "male", "female" 등의 키워드를 찾아낸다.

수치 정보는 단위와 함께 추출한다. "혈압 140/90", "체온 36.5도", "혈당 150mg/dL" 등에서 숫자와 단위를 함께 인식한다.

이 두 가지 방법을 결합함으로써, 의학 용어는 MedCAT2로 정확하게 인식하고, 구조화된 정보는 정규표현식으로 효율적으로 추출하는 상호 보완적인 시스템을 구축하였다.

#### 3.2.1.3 추출 과정의 실제 예시

실제 추출 과정을 구체적인 예시로 살펴보겠다.

사용자 입력: "저는 65세 남성이고요, 10년 전에 당뇨 진단을 받았습니다. 최근에 혈당이 자꾸 올라서 걱정이에요. 오늘 아침에 재니까 180mg/dL 나왔거든요."

이 문장에서 시스템은 다음과 같은 정보를 추출한다.

1. **인구통계 정보**
   - 나이: 65세 (정규표현식으로 추출)
   - 성별: 남성 (정규표현식으로 추출)

2. **질환**
   - 당뇨병, 진단 시점: 10년 전 (MedCAT2로 '당뇨' 인식, 정규표현식으로 '10년 전' 추출)

3. **증상**
   - 고혈당 (MedCAT2가 "혈당이 자꾸 올라서"에서 고혈당 증상을 인식)

4. **검사 수치**
   - 혈당: 180mg/dL, 측정 시점: 오늘 아침 (MedCAT2로 '혈당' 인식, 정규표현식으로 '180mg/dL', '오늘 아침' 추출)

추출된 정보는 다음과 같은 구조화된 형식으로 저장된다.

```
{
  "demographics": {
    "age": "65세",
    "gender": "남성"
  },
  "conditions": [
    {
      "name": "당뇨병",
      "umls_code": "C0011849",
      "onset": "10년 전",
      "timestamp": "2025-12-13T09:00:00"
    }
  ],
  "symptoms": [
    {
      "name": "고혈당",
      "umls_code": "C0020456",
      "timestamp": "2025-12-13T09:00:00"
    }
  ],
  "labs": [
    {
      "name": "혈당",
      "value": "180",
      "unit": "mg/dL",
      "measured_at": "오늘 아침",
      "timestamp": "2025-12-13T09:00:00"
    }
  ]
}
```

이렇게 구조화된 형식으로 저장함으로써, 나중에 이 정보를 쉽게 검색하고 활용할 수 있다.

### 3.2.2 저장(Storage) 단계

#### 3.2.2.1 저장 구조: 6개 슬롯 시스템

추출된 정보는 단순히 텍스트 형태로 저장되는 것이 아니라, 의학적 의미에 따라 분류되어 구조화된 형식으로 저장된다. 이를 '슬롯(slot)' 시스템이라고 부른다. 슬롯은 일종의 서랍이라고 생각하면 된다. 서로 다른 종류의 물건을 각각의 서랍에 정리해두면 나중에 찾기 쉬운 것처럼, 의학 정보도 종류별로 분류하여 저장한다.

본 시스템은 6개의 슬롯을 사용한다.

1. **demographics 슬롯**: 나이, 성별, 직업 등 변하지 않거나 천천히 변하는 기본 정보
2. **conditions 슬롯**: 당뇨, 고혈압 등 진단받은 질환
3. **symptoms 슬롯**: 두통, 어지러움 등 호소하는 증상
4. **medications 슬롯**: 복용 중인 약물
5. **vitals 슬롯**: 혈압, 체온 등 생체 징후
6. **labs 슬롯**: 혈당, 콜레스테롤 등 검사 수치

각 슬롯에는 해당 카테고리의 정보들이 리스트 형태로 저장된다. 예를 들어 symptoms 슬롯에는 [두통, 어지러움, 기침] 처럼 여러 증상이 저장될 수 있다.

#### 3.2.2.2 시간 가중치(Temporal Weighting)

의학 정보는 시간에 따라 중요도가 달라진다. 오늘 측정한 혈압이 1년 전 측정한 혈압보다 환자의 현재 상태를 더 잘 반영한다. 하지만 1년 전 정보도 완전히 무시할 수는 없다. 혈압의 변화 추이를 파악하는 데 도움이 되기 때문이다.

이를 반영하기 위해 본 시스템은 **시간 가중치(temporal weighting)**를 적용한다. 시간 가중치는 정보가 저장된 시점으로부터 얼마나 시간이 지났는지에 따라 중요도를 조절하는 방식이다.

시간 가중치는 지수 감쇠(exponential decay) 함수를 사용하여 계산한다.

```
가중치 = e^(-λ × Δt)
```

여기서 λ(람다)는 감쇠 상수이고, Δt는 현재 시점으로부터 경과한 시간이다. 예를 들어 λ=0.1, Δt=10일이라면, 가중치는 e^(-0.1×10) ≈ 0.368이 된다. 즉, 10일 전 정보는 현재 정보의 약 37% 중요도를 가진다.

다만 모든 정보가 같은 속도로 중요도가 감소하는 것은 아니다. 증상은 빠르게 변할 수 있으므로 λ 값을 크게 설정하여 오래된 증상의 가중치를 빠르게 낮춘다. 반대로 진단명은 천천히 변하므로 λ 값을 작게 설정하여 오래된 진단도 상당한 중요도를 유지하도록 한다.

구체적인 λ 값은 다음과 같이 설정하였다.
- demographics: λ=0.001 (거의 변하지 않음)
- conditions: λ=0.01 (천천히 변함)
- symptoms: λ=0.1 (빠르게 변함)
- medications: λ=0.05 (중간 속도로 변함)
- vitals: λ=0.1 (빠르게 변함)
- labs: λ=0.1 (빠르게 변함)

#### 3.2.2.3 중복 제거와 업데이트

대화가 진행되면서 같은 정보가 반복해서 언급될 수 있다. 예를 들어 첫 번째 대화에서 "저는 당뇨 환자입니다"라고 하고, 세 번째 대화에서 다시 "제가 당뇨가 있잖아요"라고 말할 수 있다. 이런 경우 당뇨병 정보를 두 번 저장할 필요는 없다.

본 시스템은 새로운 정보를 저장할 때 기존 정보와의 유사도를 비교한다. 만약 동일하거나 매우 유사한 정보가 이미 있다면, 새로 저장하지 않고 기존 정보의 타임스탬프만 업데이트한다. 이를 통해 "이 정보는 최근에도 다시 확인되었다"는 것을 표시한다.

유사도 판단은 UMLS 코드를 기반으로 한다. 예를 들어 '당뇨병'과 '당뇨'는 표현은 다르지만 같은 UMLS 코드(C0011849)를 가지므로 동일한 정보로 인식한다.

한편 수치 정보는 시간에 따라 변하는 것이 정상이다. 혈압이 어제는 140/90이었는데 오늘은 130/85라면, 이는 중복이 아니라 변화를 나타낸다. 따라서 vitals와 labs 슬롯의 경우, 같은 항목(예: 혈압)의 새로운 측정값은 중복 제거하지 않고 모두 저장한다. 대신 최신 값에 더 높은 가중치를 부여하여, 답변 생성 시 최신 정보가 우선적으로 사용되도록 한다.

#### 3.2.2.4 ProfileStore 구현

저장된 정보는 **ProfileStore**라는 메모리 관리 시스템에서 관리된다. ProfileStore는 각 사용자별로 독립적인 프로필을 유지하며, 사용자 ID를 키로 사용하여 빠르게 조회할 수 있도록 해시맵(hash map) 구조로 구현되었다.

ProfileStore의 주요 기능은 다음과 같다.

**저장(save)**: 새로운 정보를 적절한 슬롯에 추가한다. 이때 중복 검사와 시간 가중치 계산을 수행한다.

**조회(retrieve)**: 특정 사용자의 프로필을 가져온다. 조회 시 시간 가중치를 적용하여 중요도 순으로 정렬된 정보를 반환한다.

**업데이트(update)**: 기존 정보를 수정한다. 예를 들어 약물 복용을 중단했다면 해당 약물 정보를 제거한다.

**통합(consolidate)**: 여러 대화에 걸쳐 축적된 중복되거나 모순되는 정보를 정리한다.

ProfileStore는 메모리 상에 존재하지만, 필요시 파일이나 데이터베이스에 영구 저장할 수 있도록 직렬화(serialization) 기능도 제공한다. 이를 통해 시스템을 재시작해도 이전 대화 정보를 복원할 수 있다.

### 3.2.3 주입(Injection) 단계

#### 3.2.3.1 주입의 목적과 방법

주입 단계는 저장된 환자의 개인 정보를 실제 답변 생성에 활용하는 단계이다. 아무리 많은 정보를 추출하고 저장해도, 이를 LLM에게 적절히 전달하지 못하면 의미가 없다. 주입 단계의 핵심은 "어떤 정보를, 어떤 형식으로, LLM에게 전달할 것인가"를 결정하는 것이다.

일반적인 LLM 사용 방식은 사용자의 질문을 그대로 LLM에게 전달하는 것이다. 예를 들어 사용자가 "운동은 어떻게 하면 좋을까요?"라고 물으면, LLM도 이 질문만 받아서 답변을 생성한다. 하지만 이렇게 하면 LLM은 질문자가 누구인지, 어떤 건강 상태인지 알 수 없으므로 일반적인 답변만 할 수밖에 없다.

본 시스템의 주입 방식은 다르다. 같은 질문이라도 LLM에게는 다음과 같이 풍부한 맥락과 함께 전달한다.

```
[환자 정보]
- 나이: 65세
- 성별: 남성
- 질환: 당뇨병 (10년째), 고혈압 (5년째)
- 최근 검사: 혈당 180mg/dL (오늘 아침)
- 복용 약물: 메트포르민

[이전 대화]
- 턴 1: 당뇨 진단 시기 확인
- 턴 2: 혈당 관리 방법 상담
- 턴 3: 식이요법 조언

[검색된 의학 문서]
- 문서 1: 당뇨병 환자를 위한 운동 가이드라인 (대한당뇨병학회, 2023)
- 문서 2: 고령 당뇨 환자의 안전한 운동 방법 (Journal of Diabetes, 2024)
- 문서 3: 혈당 조절을 위한 유산소 운동의 효과 (내분비학회지, 2023)

[사용자 질문]
운동은 어떻게 하면 좋을까요?
```

이렇게 풍부한 맥락을 제공하면 LLM은 단순히 "당뇨 환자에게는 유산소 운동이 좋습니다"가 아니라, "선생님 같은 65세 남성 당뇨 환자의 경우, 최근 혈당이 180mg/dL로 다소 높은 편이므로, 급격한 운동보다는 가벼운 산책부터 시작하시는 것이 안전합니다"와 같이 개인화된 답변을 생성할 수 있다.

#### 3.2.3.2 컨텍스트 조립(Context Assembly)

하지만 여기서 문제가 하나 있다. LLM에게 전달할 수 있는 정보의 양에는 한계가 있다는 것이다. 이를 '토큰 제한(token limit)'이라고 한다. 토큰은 단어나 문자를 의미하는 단위인데, 대부분의 LLM은 한 번에 처리할 수 있는 토큰 수가 제한되어 있다. 예를 들어 GPT-4의 경우 약 8,000~128,000 토큰까지 처리할 수 있지만, 토큰이 많을수록 처리 속도가 느려지고 비용도 증가한다.

따라서 저장된 모든 정보를 무조건 LLM에게 전달할 수는 없다. 현재 질문과 관련성이 높은 정보를 선별하여 전달해야 한다. 이 과정을 **컨텍스트 조립(Context Assembly)**이라고 한다.

컨텍스트 조립은 다음과 같은 우선순위로 이루어진다.

**첫 번째 우선순위: 현재 질문과 직접 관련된 정보**이다. "운동은 어떻게 하면 좋을까요?"라는 질문이라면, 운동과 관련된 질환(당뇨, 고혈압), 체력과 관련된 나이(65세), 주의사항과 관련된 복용 약물 등이 여기에 해당한다. 반면 "5년 전에 감기를 앓았던 이력"처럼 현재 질문과 무관한 정보는 제외된다.

**두 번째 우선순위: 시간 가중치가 높은 정보**이다. 같은 종류의 정보라도 최근 것이 우선된다. 예를 들어 혈당 수치가 여러 번 측정되었다면, 가장 최근 측정값을 우선적으로 포함한다.

**세 번째 우선순위: 의학적으로 중요한 정보**이다. 나이, 성별, 주요 질환은 대부분의 질문에 중요하므로 가능한 포함한다.

이러한 우선순위에 따라 정보를 선별하되, 전체 토큰 수가 예산(budget) 내에 들어오도록 조절한다. 본 시스템에서는 토큰 예산을 다음과 같이 배분하였다.

- 환자 정보: 최대 500 토큰
- 이전 대화: 최대 1,000 토큰
- 검색된 문서: 최대 3,000 토큰
- 질문 및 지침: 약 500 토큰
- LLM 답변 생성: 최대 1,000 토큰

총합 약 6,000 토큰으로, 대부분의 LLM이 효율적으로 처리할 수 있는 범위이다.

#### 3.2.3.3 프롬프트 설계

조립된 컨텍스트는 프롬프트(prompt)라는 형태로 LLM에게 전달된다. 프롬프트는 LLM에게 "무엇을 어떻게 해달라"고 요청하는 일종의 지시문이다. 좋은 프롬프트는 LLM이 정확하고 유용한 답변을 생성하도록 유도한다.

본 시스템에서 사용하는 프롬프트 구조는 다음과 같다.

**역할 정의(Role Definition)**: LLM에게 어떤 역할을 맡을지 명확히 알려준다. "당신은 의학 지식이 풍부한 건강 상담 AI입니다. 환자의 개인 정보를 고려하여 안전하고 정확한 조언을 제공해야 합니다."

**환자 정보 제공(Patient Information)**: 앞서 선별한 환자의 개인 정보를 구조화된 형태로 제공한다.

**검색 문서 제공(Retrieved Documents)**: 의학 데이터베이스에서 검색한 신뢰할 수 있는 문서들을 제공한다. 각 문서에는 출처(논문명, 저자, 연도 등)를 명시한다.

**질문 제시(Question)**: 사용자의 원래 질문을 제시한다.

**답변 가이드라인(Guidelines)**: LLM이 답변을 생성할 때 지켜야 할 원칙을 제시한다. 예를 들어 "반드시 제공된 의학 문서에 근거하여 답변하고, 추측이나 불확실한 정보는 포함하지 마세요", "환자의 나이와 질환을 고려한 맞춤형 조언을 제공하세요" 등이다.

이러한 구조화된 프롬프트를 통해 LLM은 무엇을 해야 하는지, 어떤 정보를 사용해야 하는지, 어떤 원칙을 지켜야 하는지 명확히 이해하고 답변을 생성하게 된다.

### 3.2.4 검증(Verification) 단계

#### 3.2.4.1 검증의 필요성

LLM이 답변을 생성했다고 해서 그것이 항상 올바른 것은 아니다. LLM은 때때로 환각(hallucination)을 일으켜 사실이 아닌 내용을 생성하거나, 제공된 문서를 잘못 해석하거나, 환자의 개인 정보를 제대로 반영하지 못할 수 있다. 특히 의료 분야에서는 잘못된 정보가 환자의 건강에 직접적인 피해를 줄 수 있으므로, 생성된 답변을 자동으로 검증하는 메커니즘이 필수적이다.

검증 단계는 생성된 답변을 출력하기 전에 그 품질을 자동으로 평가하고, 기준에 미달하면 재생성을 요청하는 과정이다. 이는 제품 공장에서 불량품을 걸러내는 품질 검사(Quality Control)와 유사한 개념이다.

#### 3.2.4.2 품질 평가 기준

본 시스템은 세 가지 기준으로 답변의 품질을 평가한다.

**첫째, 근거성(Grounding)**이다. 답변이 검색된 의학 문서에 실제로 근거하고 있는가를 평가한다. 예를 들어 답변에서 "당뇨 환자에게는 하루 30분의 유산소 운동이 권장됩니다"라고 했다면, 제공된 문서에 실제로 이런 내용이 있는지 확인한다. 문서에 없는 내용을 LLM이 임의로 추가했다면 근거성이 낮다고 판단한다.

**둘째, 완전성(Completeness)**이다. 답변이 사용자의 질문에 충분히 답하고 있는가를 평가한다. "운동은 어떻게 하면 좋을까요?"라는 질문에 "운동이 중요합니다"라고만 답한다면, 비록 사실이긴 하지만 구체적인 방법을 제시하지 않았으므로 완전성이 낮다.

**셋째, 정확성(Accuracy)**이다. 답변이 환자의 개인 정보를 정확히 반영하고 있는가를 평가한다. 예를 들어 65세 환자에게 "젊으니까 격렬한 운동을 해도 됩니다"라고 답한다면, 환자 정보를 제대로 반영하지 못한 것이므로 정확성이 낮다.

이 세 가지 기준에 대해 각각 0~1점 사이의 점수를 매기고, 평균 점수가 일정 임계값(threshold) 이상이면 합격, 미만이면 불합격으로 판단한다. 본 시스템에서는 임계값을 0.7로 설정하였다. 즉, 평균 70점 이상이어야 답변을 출력한다.

#### 3.2.4.3 LLM 기반 자동 평가

흥미로운 점은 이 품질 평가도 LLM을 사용하여 수행한다는 것이다. 즉, "LLM이 생성한 답변을 LLM이 평가"하는 구조이다. 이것이 가능한 이유는 답변 생성과 평가가 서로 다른 관점에서 이루어지기 때문이다.

답변 생성 시에는 LLM에게 "환자에게 도움이 되는 조언을 제공하라"고 요청한다. 반면 평가 시에는 "이 답변이 제공된 문서에 근거하는지, 질문에 충분히 답하는지, 환자 정보를 정확히 반영하는지 객관적으로 평가하라"고 요청한다. 역할이 다르므로 같은 LLM이라도 다른 결과를 낼 수 있다.

구체적인 평가 프롬프트 예시는 다음과 같다.

```
당신은 의료 AI 답변의 품질을 평가하는 전문가입니다.

[제공된 의학 문서]
{검색된 문서들}

[환자 정보]
{환자 프로필}

[질문]
{사용자 질문}

[생성된 답변]
{LLM이 생성한 답변}

다음 기준으로 답변을 평가하고 0~1 사이의 점수를 부여하세요:

1. 근거성: 답변의 내용이 제공된 의학 문서에 실제로 나와 있는가?
2. 완전성: 답변이 질문에 충분히 답하고 있는가?
3. 정확성: 답변이 환자의 나이, 성별, 질환 등을 정확히 반영하는가?

각 기준에 대한 점수와 간단한 이유를 제시하세요.
```

LLM은 이 프롬프트에 따라 각 기준에 대한 점수와 평가 이유를 반환한다. 예를 들어 "근거성: 0.9 (답변의 대부분이 제공된 당뇨병학회 가이드라인에 근거함), 완전성: 0.8 (운동 방법은 잘 설명했으나 주의사항 언급이 부족), 정확성: 0.7 (환자의 나이는 반영했으나 혈당 수치는 고려하지 않음)" 같은 형태이다.

#### 3.2.4.4 자기개선 메커니즘(Self-Refine)

평가 결과 품질이 낮다고 판단되면, 시스템은 두 가지 조치를 취한다.

첫째는 **질의 재작성(Query Rewriting)**이다. 초기 검색에서 적절한 문서를 찾지 못했을 가능성이 있으므로, 질문을 다시 구성하여 재검색을 시도한다. 예를 들어 원래 질문이 "운동은 어떻게 하면 좋을까요?"였다면, 이를 "65세 남성 당뇨 환자에게 적합한 안전한 운동 방법"으로 구체화하여 재검색한다. 더 구체적인 질문은 더 관련성 높은 문서를 찾을 가능성이 크다.

둘째는 **재생성(Regeneration)**이다. 재검색으로 얻은 새로운 문서들, 또는 기존 문서들을 다시 활용하여 답변을 재생성한다. 이때 이전 평가에서 지적된 문제점(예: "혈당 수치를 고려하지 않음")을 프롬프트에 추가하여 같은 실수를 반복하지 않도록 한다.

이러한 과정을 **Self-Refine**이라고 부른다. 스스로(Self) 자신의 결과물을 평가하고 개선(Refine)한다는 의미이다. 본 시스템에서는 최대 2회까지 Self-Refine을 수행하도록 설정하였다. 2회를 초과하면 무한 루프에 빠질 위험이 있고, 대부분의 경우 2회 내에 품질이 개선되기 때문이다.

Self-Refine 과정은 다음과 같은 순환 구조로 이루어진다.

```
1. 문서 검색 → 답변 생성
2. 품질 평가
3. 품질이 낮으면:
   a. 질의 재작성
   b. 재검색
   c. 재생성
   d. 2번으로 돌아감
4. 품질이 높으면: 답변 출력
```

이를 통해 초기 검색이나 생성에 실패해도 자동으로 복구하여 최종적으로 고품질의 답변을 제공할 수 있다.

## 제3절 LangGraph 기반 순환식 시스템 아키텍처

### 3.3.1 LangGraph 프레임워크 소개

앞서 설명한 Context Engineering의 4단계(추출, 저장, 주입, 검증)와 Self-Refine 메커니즘을 실제로 구현하기 위해서는 복잡한 워크플로우를 체계적으로 관리할 수 있는 프레임워크가 필요하다. 본 연구에서는 **LangGraph**를 선택하였다.

LangGraph는 LangChain 팀이 개발한 상태 기반 워크플로우 프레임워크이다. 기존의 순차적 프로그래밍 방식과 달리, 작업을 여러 개의 노드(node)로 나누고 노드 간의 연결(edge)을 정의하여 그래프 형태로 워크플로우를 구성한다. 이는 마치 지하철 노선도처럼 각 역(노드)이 무엇을 하는지, 어떤 역으로 이동할 수 있는지(엣지)를 명확히 보여준다.

LangGraph의 핵심 개념은 **상태(state)**이다. 상태는 워크플로우가 진행되면서 변화하는 데이터를 담는 컨테이너이다. 예를 들어 현재 처리 중인 질문, 추출된 환자 정보, 검색된 문서, 생성된 답변 등이 모두 상태에 저장된다. 각 노드는 상태를 읽어서 작업을 수행하고, 결과를 다시 상태에 기록한다. 다음 노드는 업데이트된 상태를 읽어서 자신의 작업을 수행한다.

이러한 상태 기반 접근은 두 가지 장점이 있다. 첫째, 각 노드가 독립적으로 작동하므로 개별 노드를 쉽게 수정하거나 교체할 수 있다. 둘째, 상태를 추적하면 워크플로우의 어느 단계에서 문제가 발생했는지 쉽게 파악할 수 있다.

### 3.3.2 10개 노드 구성

본 시스템은 총 10개의 노드로 구성되어 있으며, 각 노드는 특정한 책임을 가진다. 각 노드를 차례로 설명하겠다.

**1. check_similarity 노드 (유사도 확인)**

이 노드는 현재 질문이 이전에 답변했던 질문과 유사한지 확인한다. 만약 매우 유사한 질문이 있고 그에 대한 검증된 답변이 캐시에 저장되어 있다면, 전체 파이프라인을 다시 실행할 필요 없이 캐시된 답변을 재사용한다. 다만 완전히 동일한 답변을 반환하면 기계적으로 느껴질 수 있으므로, 문체를 약간 변형하여 제공한다.

유사도는 질문을 벡터로 변환하고 코사인 유사도(cosine similarity)를 계산하여 측정한다. 유사도가 0.85 이상이면 "거의 같은 질문"으로 판단하여 캐시를 사용한다.

**2. classify_intent 노드 (의도 분류)**

이 노드는 질문의 복잡도를 분류한다. 질문은 크게 단순(simple), 보통(moderate), 복잡(complex)의 세 단계로 분류된다.

단순한 질문은 "정상 혈압은 얼마인가요?" 같이 단순한 사실 확인이다. 이런 질문은 적은 수의 문서(k=3)만 검색해도 충분히 답할 수 있다. 보통 질문은 "당뇨 환자는 어떤 음식을 먹어야 하나요?" 같이 약간의 맥락이 필요한 질문이다. 중간 정도의 문서(k=8)를 검색한다. 복잡한 질문은 "65세 당뇨 환자가 고혈압도 있을 때 안전한 운동 방법은?" 같이 여러 조건을 고려해야 하는 질문이다. 많은 문서(k=15)를 검색하여 다각도로 정보를 수집한다.

이러한 동적 k 조정을 **Active Retrieval(능동적 검색)**이라고 부른다.

**3. extract_slots 노드 (슬롯 추출)**

이 노드는 앞서 3.2.1절에서 설명한 추출 단계를 수행한다. MedCAT2와 정규표현식을 사용하여 질문에서 의학 정보를 추출하고, 6개 슬롯으로 분류한다.

**4. store_memory 노드 (메모리 저장)**

이 노드는 추출된 정보를 ProfileStore에 저장한다. 중복 검사, 시간 가중치 계산, 업데이트 등 3.2.2절에서 설명한 저장 로직을 수행한다.

**5. assemble_context 노드 (컨텍스트 조립)**

이 노드는 3.2.3절에서 설명한 주입 단계의 컨텍스트 조립을 수행한다. ProfileStore에서 환자 정보를 가져오고, 현재 질문과의 관련성과 시간 가중치를 고려하여 중요한 정보를 선별한다. 그리고 토큰 예산 내에서 프롬프트를 구성한다.

**6. retrieve 노드 (하이브리드 검색)**

이 노드는 의학 데이터베이스에서 관련 문서를 검색한다. 본 시스템은 BM25 키워드 검색과 FAISS 벡터 검색을 결합한 **하이브리드 검색**을 사용한다. 이에 대해서는 3.4.3절에서 자세히 설명한다.

**7. generate_answer 노드 (답변 생성)**

이 노드는 조립된 컨텍스트(환자 정보 + 검색 문서 + 질문)를 LLM에게 전달하여 답변을 생성한다. GPT-4, Claude, Gemini 등 다양한 LLM을 사용할 수 있도록 설계되었다.

**8. refine 노드 (품질 평가)**

이 노드는 3.2.4절에서 설명한 검증 단계를 수행한다. 생성된 답변을 근거성, 완전성, 정확성의 세 가지 기준으로 평가하고 점수를 매긴다. 또한 품질이 낮을 경우 어떻게 개선할지에 대한 피드백도 생성한다.

**9. quality_check 노드 (품질 확인 및 라우팅)**

이 노드는 refine 노드의 평가 결과를 받아서 다음 행동을 결정한다. 품질 점수가 임계값(0.7) 이상이면 답변을 최종 출력으로 승인한다. 미만이면 재검색 및 재생성을 지시한다. 이때 Self-Refine 반복 횟수가 최대치(2회)를 초과했는지도 확인하여 무한 루프를 방지한다.

**10. store_response 노드 (응답 캐싱)**

이 노드는 검증된 답변을 캐시에 저장한다. 나중에 유사한 질문이 들어왔을 때 check_similarity 노드가 이 캐시를 참조하여 빠르게 답변할 수 있도록 한다.

### 3.3.3 노드 간 연결과 조건부 라우팅

10개 노드는 단순히 순차적으로 연결되어 있지 않다. 상황에 따라 다른 경로를 선택하거나 이전 노드로 돌아가는 **조건부 라우팅(conditional routing)**을 사용한다.

주요 라우팅 로직은 다음과 같다.

**첫 번째 분기: check_similarity 이후**
- 유사도 ≥ 0.85: store_response → END (캐시 히트, 전체 파이프라인 스킵)
- 유사도 < 0.85: classify_intent → ... (일반 파이프라인 진행)

**두 번째 분기: quality_check 이후**
- 품질 ≥ 0.7: store_response → END (답변 승인)
- 품질 < 0.7 AND 반복 < 2: retrieve (재검색 및 재생성)
- 품질 < 0.7 AND 반복 ≥ 2: store_response → END (최선의 답변 출력)

이러한 순환 구조를 그림으로 표현하면 다음과 같다.

```
START
  ↓
check_similarity
  ↓               ↘ (유사도 높음)
classify_intent    store_response → END
  ↓
extract_slots
  ↓
store_memory
  ↓
assemble_context
  ↓
retrieve ←──────────┐ (품질 낮음 & 반복 < 2)
  ↓                 │
generate_answer     │
  ↓                 │
refine              │
  ↓                 │
quality_check ──────┘
  ↓ (품질 높음 OR 반복 ≥ 2)
store_response
  ↓
END
```

### 3.3.4 상태 관리

LangGraph의 상태는 Python의 TypedDict로 정의되어 있다. 주요 상태 필드는 다음과 같다.

```python
class AgentState(TypedDict):
    # 입력
    user_id: str              # 사용자 ID
    query: str                # 원본 질문

    # 추출 및 저장
    extracted_slots: Dict     # 추출된 의학 정보
    user_profile: Dict        # 사용자 프로필

    # 검색
    intent_complexity: str    # 질문 복잡도
    k_documents: int          # 검색할 문서 수
    retrieved_docs: List      # 검색된 문서들

    # 생성 및 검증
    answer: str               # 생성된 답변
    quality_scores: Dict      # 품질 점수
    feedback: str             # 개선 피드백

    # 제어
    refine_count: int         # 반복 횟수
    cache_hit: bool           # 캐시 히트 여부
```

각 노드는 이 상태를 읽고 수정한다. 상태를 일관된 형식으로 관리함으로써, 각 노드가 예상하는 입력과 출력이 명확해지고 디버깅도 쉬워진다.

## 제4절 구현 세부사항

### 3.4.1 응답 캐시 시스템(Response Cache)

#### 3.4.1.1 캐시의 필요성

실제 사용 환경에서 사용자들은 비슷한 질문을 반복해서 한다. "정상 혈압은 얼마인가요?", "혈압 정상 범위가 어떻게 되나요?" 등은 표현은 다르지만 본질적으로 같은 질문이다. 이런 질문마다 전체 파이프라인을 실행하면 불필요한 계산과 비용이 발생한다.

응답 캐시 시스템은 이전에 답변했던 질문과 답변을 저장해두고, 유사한 질문이 들어오면 저장된 답변을 재사용하는 메커니즘이다. 이를 통해 응답 속도를 크게 향상시키고(캐시 히트 시 85% 빠름) 처리 비용을 절감할 수 있다(30% 질의 감소).

#### 3.4.1.2 벡터 기반 유사도 측정

질문의 유사도를 측정하기 위해 **임베딩(embedding)**을 사용한다. 임베딩은 텍스트를 고차원 벡터로 변환하는 기술이다. 의미가 비슷한 문장은 벡터 공간에서 가까운 위치에 놓이게 된다.

예를 들어 "정상 혈압은 얼마인가요?"를 임베딩하면 [0.12, -0.34, 0.56, ...] 같은 1536차원 벡터가 된다. "혈압 정상 범위가 어떻게 되나요?"도 비슷한 벡터로 변환된다. 이 두 벡터 간의 **코사인 유사도(cosine similarity)**를 계산하면 0.92 같은 높은 값이 나온다.

본 시스템에서는 OpenAI의 text-embedding-3-small 모델을 사용하여 임베딩을 생성한다. 유사도 임계값은 0.85로 설정하였다. 즉, 코사인 유사도가 0.85 이상이면 "거의 같은 질문"으로 판단하여 캐시를 사용한다.

#### 3.4.1.3 스타일 변형(Style Variation)

캐시에서 답변을 가져와 그대로 반환하면, 사용자가 같은 질문을 했을 때 완전히 동일한 답변을 받게 된다. 이는 시스템이 기계적으로 느껴지게 만든다. 따라서 캐시된 답변의 핵심 내용은 유지하되, 문체를 약간 변형하여 제공한다.

이러한 스타일 변형도 LLM을 사용하여 수행한다. "다음 답변의 핵심 정보는 유지하되, 문체와 표현을 자연스럽게 변형해주세요"라는 간단한 프롬프트로 처리한다.

### 3.4.2 능동적 검색(Active Retrieval)

#### 3.4.2.1 능동적 검색의 개념

전통적인 RAG 시스템은 모든 질문에 대해 고정된 수의 문서를 검색한다. 하지만 이는 비효율적이다. 단순한 질문은 3개 문서로도 충분한데 10개를 검색하면 불필요한 비용이 발생한다. 반대로 복잡한 질문은 10개로는 부족한데, 더 검색하지 않으면 답변 품질이 떨어진다.

**능동적 검색(Active Retrieval)**은 질문의 복잡도를 자동으로 분석하고, 그에 맞게 검색할 문서 수(k)를 동적으로 조정하는 기법이다.

#### 3.4.2.2 3단계 복잡도 분류

본 시스템은 질문의 복잡도를 3단계로 분류한다.

**1단계: 규칙 기반(Rule-based) 분류**

먼저 간단한 규칙으로 단순한 질문을 걸러낸다. 다음과 같은 패턴에 해당하면 단순(simple)으로 분류한다.

- "~은 무엇인가요?", "~이란?" → 정의 질문
- "정상 범위는?", "기준은?" → 수치 확인 질문
- 질문 길이가 10단어 미만 → 짧은 질문

**2단계: 슬롯 기반(Slot-based) 분류**

규칙에 걸리지 않은 질문은 추출된 의학 정보의 양으로 판단한다. extract_slots 노드에서 추출한 슬롯을 확인한다.

- 슬롯 개수 ≤ 2: 보통(moderate)
- 슬롯 개수 > 2: 복잡(complex)

**3단계: 내용 기반(Content-based) 분류**

슬롯 개수로도 애매한 경우, LLM을 사용하여 질문의 내용을 분석한다. "이 질문이 답변하기 위해 얼마나 많은 배경 지식이 필요한가?"를 평가하여 복잡도를 결정한다.

#### 3.4.2.3 동적 k 결정

분류 결과에 따라 다음과 같이 k 값을 결정한다.

- simple: k = 3
- moderate: k = 8
- complex: k = 15

이를 통해 단순한 질문은 빠르고 저렴하게 처리하고, 복잡한 질문은 충분한 정보를 수집하여 품질 높은 답변을 생성한다. 실험 결과 이러한 동적 조정은 평균 레이턴시를 30% 감소시키고 비용을 40% 절감하는 효과를 보였다.

### 3.4.3 하이브리드 검색(Hybrid Retrieval)

#### 3.4.3.1 단일 검색 방식의 한계

문서 검색에는 크게 두 가지 방식이 있다.

**키워드 검색(Keyword Search)**은 질문과 문서에서 공통으로 나타나는 단어를 찾는 방식이다. BM25가 대표적인 알고리즘이다. 장점은 빠르고 정확하다는 것이다. 하지만 동의어를 인식하지 못한다는 단점이 있다. "당뇨"와 "diabetes"는 같은 의미인데, 키워드 검색은 이를 다른 단어로 본다.

**벡터 검색(Vector Search)**은 질문과 문서를 벡터로 변환하고 의미적 유사도를 계산하는 방식이다. FAISS가 대표적인 도구이다. 장점은 동의어와 유사 표현을 잘 인식한다는 것이다. 하지만 때때로 의미는 비슷하지만 실제로는 관련 없는 문서를 가져오기도 한다.

#### 3.4.3.2 하이브리드 검색 구조

**하이브리드 검색**은 키워드 검색과 벡터 검색을 결합하여 양쪽의 장점을 취하는 방식이다. 구체적인 절차는 다음과 같다.

**1단계: 병렬 검색**

키워드 검색(BM25)과 벡터 검색(FAISS)을 동시에 수행한다. 각각 상위 k개 문서를 반환한다.

**2단계: RRF 융합**

**RRF(Reciprocal Rank Fusion)**라는 기법으로 두 검색 결과를 통합한다. RRF는 순위를 기반으로 점수를 재계산하는 방식이다.

```
RRF 점수 = 1 / (k + 순위)
```

k는 상수(본 시스템에서는 60)이고, 순위는 각 검색 결과에서의 등수이다.

예를 들어 문서A가 BM25에서 1등, FAISS에서 3등이라면:
- BM25 RRF 점수: 1/(60+1) ≈ 0.0164
- FAISS RRF 점수: 1/(60+3) ≈ 0.0159
- 총 RRF 점수: 0.0164 + 0.0159 = 0.0323

모든 문서에 대해 이렇게 계산하고, 총 RRF 점수가 높은 순으로 최종 순위를 매긴다.

#### 3.4.3.3 BM25 최적화

본 시스템에서 BM25 구현은 성능 최적화를 위해 **heapq(힙 큐)**를 사용한다. 일반적인 정렬은 O(n log n) 시간이 걸리지만, 상위 k개만 필요한 경우 heapq를 사용하면 O(n log k)로 줄일 수 있다. n이 크고 k가 작을 때 큰 차이를 보인다.

#### 3.4.3.4 FAISS 인덱스

벡터 검색을 위해 **FAISS**를 사용한다. FAISS는 Facebook AI Research에서 개발한 고속 유사도 검색 라이브러리이다. 수백만 개의 벡터 중에서 유사한 벡터를 밀리초 단위로 찾아낼 수 있다.

본 시스템에서는 1536차원 임베딩 벡터를 사용한다. 의학 문서 코퍼스(약 100,000개 문서)를 모두 임베딩하여 FAISS 인덱스로 구축한다. 인덱스는 한 번 구축하면 재사용할 수 있으므로, 시스템 시작 시 로딩하여 메모리에 유지한다.

#### 3.4.3.5 하이브리드 검색의 효과

실험 결과 하이브리드 검색은 단일 검색 방식 대비 60% 높은 정밀도를 달성하였다. 특히 다음과 같은 경우에 효과적이었다.

- 동의어나 약어가 포함된 질문: "DM 환자"와 "당뇨병 환자" 모두 검색
- 전문 용어와 일반 용어 혼용: "고혈압"과 "높은 혈압" 모두 인식
- 복잡한 복합 질문: 키워드로 범위를 좁히고 벡터로 의미를 파악

### 3.4.4 자기개선 메커니즘(Self-Refine) 구현 상세

#### 3.4.4.1 품질 평가 프롬프트

refine 노드에서 사용하는 평가 프롬프트는 다음과 같은 구조이다.

```
역할: 의료 AI 답변 품질 평가자

제공 정보:
- 검색 문서: {retrieved_docs}
- 환자 정보: {user_profile}
- 질문: {query}
- 생성 답변: {answer}

평가 기준:
1. 근거성 (0~1): 답변이 제공된 문서에 명확히 근거하는가?
2. 완전성 (0~1): 답변이 질문에 충분히 답하는가?
3. 정확성 (0~1): 답변이 환자 정보를 정확히 반영하는가?

출력 형식 (JSON):
{
  "grounding": <점수>,
  "completeness": <점수>,
  "accuracy": <점수>,
  "feedback": "<개선 사항>"
}
```

#### 3.4.4.2 이중 안전장치

Self-Refine이 무한 루프에 빠지는 것을 방지하기 위해 두 가지 안전장치를 둔다.

**첫째, 반복 횟수 제한**이다. `refine_count` 상태 변수로 현재 몇 번째 반복인지 추적한다. 2회를 초과하면 현재 답변이 최선이라고 판단하고 출력한다.

**둘째, 중복 문서 감지**이다. 재검색했는데 이전과 완전히 동일한 문서들이 반환되면, 더 이상 개선 여지가 없다고 판단한다. 문서 ID 세트를 비교하여 중복을 감지한다.

## 제5절 성능 최적화

### 3.5.1 그래프 및 설정 캐싱

LangGraph 그래프를 매 요청마다 새로 생성하면 불필요한 오버헤드가 발생한다. 본 시스템은 다음과 같은 캐싱 전략을 사용한다.

**그래프 인스턴스 재사용**: 그래프는 한 번만 생성하고 전역 변수나 싱글톤 패턴으로 재사용한다. 이는 요청당 약 50ms를 절약한다.

**설정 파일 캐싱**: YAML 형식의 설정 파일(모델 설정, 임계값 등)은 시스템 시작 시 한 번만 로딩하고 메모리에 유지한다. 매번 파일을 읽지 않으므로 요청당 약 10ms를 절약한다.

### 3.5.2 ProfileStore 인덱싱

ProfileStore는 사용자 ID를 키로 사용하는 해시맵으로 구현되어 있다. 하지만 특정 슬롯의 정보를 검색할 때는 선형 탐색(O(n))이 필요했다.

이를 개선하기 위해 **역인덱스(inverted index)**를 추가하였다. UMLS 코드를 키로, 해당 정보를 가진 사용자 ID 리스트를 값으로 하는 인덱스이다.

이를 통해 검색 시간이 O(n)에서 O(1)로 개선되어 90% 이상 속도가 향상되었다.

### 3.5.3 배치 임베딩 처리

임베딩 생성은 상대적으로 비용이 큰 작업이다. 본 시스템은 **배치 처리**를 통해 이를 최적화한다.

예를 들어 10개 문서를 임베딩해야 할 때:
- 개별 처리: API 호출 10회, 총 1000ms
- 배치 처리: API 호출 1회, 총 200ms

약 5배 빠르다. OpenAI API는 한 번에 최대 2048개 텍스트를 배치로 처리할 수 있다.

### 3.5.4 비동기 처리

일부 작업은 서로 독립적이므로 병렬로 처리할 수 있다. 예를 들어 BM25 검색과 FAISS 검색은 동시에 수행 가능하다. Python의 `asyncio`를 사용하여 비동기 처리를 구현하였다.

```python
# 비동기 처리 (병렬)
bm25_task = async_bm25_search(query)
faiss_task = async_faiss_search(query)
bm25_results, faiss_results = await asyncio.gather(bm25_task, faiss_task)
```

약 2배 빠르다.

### 3.5.5 총 성능 개선 효과

이러한 최적화 기법들을 종합한 결과:

- **평균 레이턴시**: 2.0s → 1.0s (50% 감소)
- **처리 비용**: $0.10/요청 → $0.04/요청 (60% 감소)
- **동시 처리 용량**: 10 QPS → 25 QPS (150% 증가)

시스템이 실용적인 수준의 성능을 달성하였다.

---

**제3장 연구방법론을 완료하였습니다.**

본 장에서는 Context Engineering의 4단계(추출, 저장, 주입, 검증), LangGraph 기반의 10개 노드 아키텍처, 응답 캐시·능동적 검색·하이브리드 검색·자기개선 메커니즘 등 핵심 구현 사항, 그리고 성능 최적화 기법까지 상세히 설명하였다.

제4장에서는 이렇게 구축한 시스템을 실제로 평가한 실험 설계와 결과를 다룰 예정이며, 제5장에서는 연구의 결론과 의의, 한계점 및 향후 연구 방향을 제시할 것이다.

---


