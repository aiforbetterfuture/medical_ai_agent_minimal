# 5í„´ ë©€í‹°í„´ ëŒ€í™”ì—ì„œì˜ ë§¥ë½ ì¶”ì¶œ ë° ë°˜ì˜ ì—°êµ¬ ì „ëµ
## Context Engineering for Medical AI Agent: 5-Turn Multi-Turn Dialogue Analysis

ì‘ì„±ì¼: 2024-12-12
ì—°êµ¬ ì£¼ì œ: **Context Engineering ê¸°ë°˜ ì˜í•™ì§€ì‹ AI Agent ì„¤ê³„**
í‰ê°€ ë²”ìœ„: **5í„´ ë©€í‹°í„´ ëŒ€í™”**

---

## ğŸ“‹ ëª©ì°¨

1. [ì—°êµ¬ ë°°ê²½ ë° ëª©ì ](#ì—°êµ¬-ë°°ê²½-ë°-ëª©ì )
2. [5í„´ ë©€í‹°í„´ ëŒ€í™”ì˜ ë§¥ë½ ì¶”ì¶œ í”„ë ˆì„ì›Œí¬](#5í„´-ë©€í‹°í„´-ëŒ€í™”ì˜-ë§¥ë½-ì¶”ì¶œ-í”„ë ˆì„ì›Œí¬)
3. [ë…¼ë¬¸ ê¸°ë²•ì˜ ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì—¬ë„](#ë…¼ë¬¸-ê¸°ë²•ì˜-ë§¥ë½-ì—”ì§€ë‹ˆì–´ë§-ê¸°ì—¬ë„)
4. [ì°¨ë³„ì  ë° ì—°êµ¬ ê¸°ì—¬ë„](#ì°¨ë³„ì -ë°-ì—°êµ¬-ê¸°ì—¬ë„)
5. [ì •ëŸ‰ì  í‰ê°€ ë°©ë²•ë¡ ](#ì •ëŸ‰ì -í‰ê°€-ë°©ë²•ë¡ )
6. [í•™ìˆ ì  ê²€ì¦ í”„ë ˆì„ì›Œí¬](#í•™ìˆ ì -ê²€ì¦-í”„ë ˆì„ì›Œí¬)
7. [ì‹¤í—˜ ì„¤ê³„ ë° ë°ì´í„°ì…‹](#ì‹¤í—˜-ì„¤ê³„-ë°-ë°ì´í„°ì…‹)
8. [ì˜ˆìƒ ê²°ê³¼ ë° ê¸°ëŒ€íš¨ê³¼](#ì˜ˆìƒ-ê²°ê³¼-ë°-ê¸°ëŒ€íš¨ê³¼)

---

## ì—°êµ¬ ë°°ê²½ ë° ëª©ì 

### ì—°êµ¬ì˜ í•µì‹¬ ì§ˆë¬¸

> **"í™˜ìì™€ì˜ 5í„´ ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ê°œì¸ì  ë§¥ë½ì„ ì–´ë–»ê²Œ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ì´í›„ í„´ì— ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ëœ ì˜í•™ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?"**

### ì—°êµ¬ì˜ ì¤‘ìš”ì„±

#### 1. ì˜ë£Œ ë„ë©”ì¸ì˜ íŠ¹ìˆ˜ì„±

ì˜ë£Œ ìƒë‹´ì€ ì¼ë°˜ ëŒ€í™”ì™€ ë‹¬ë¦¬ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:

| íŠ¹ì„± | ì¼ë°˜ ëŒ€í™” | ì˜ë£Œ ëŒ€í™” | ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§ í•„ìš”ì„± |
|------|----------|----------|---------------------|
| **ì •ë³´ ë¯¼ê°ë„** | ë‚®ìŒ | ë§¤ìš° ë†’ìŒ | ì •í™•í•œ ë§¥ë½ ì¶”ì¶œ í•„ìˆ˜ |
| **ì•ˆì „ì„± ìš”êµ¬** | ë‚®ìŒ | ë§¤ìš° ë†’ìŒ | ì´ì „ ë§¥ë½ ê²€ì¦ í•„ìš” |
| **ê°œì¸í™” ìˆ˜ì¤€** | ì„ íƒì  | í•„ìˆ˜ | í™˜ìë³„ ë§¥ë½ ëˆ„ì  |
| **ì •ë³´ ëˆ„ì ì„±** | ë‚®ìŒ | ë†’ìŒ | 5í„´ ê±¸ì¹œ ì ì§„ì  ì •ë³´ ìˆ˜ì§‘ |
| **ë§¥ë½ ì˜ì¡´ì„±** | ì¤‘ê°„ | ë§¤ìš° ë†’ìŒ | í„´ ê°„ ì°¸ì¡° í•´ê²° í•„ìˆ˜ |

#### 2. 5í„´ ëŒ€í™”ì˜ ì˜ë¯¸

**ì™œ 5í„´ì¸ê°€?**

```
Turn 1: ì´ˆê¸° ì¦ìƒ ì œì‹œ (ë§¥ë½ ìƒì„±)
   â†“
Turn 2: ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (ë§¥ë½ í™•ì¥)
   â†“
Turn 3: ì •ë³´ ì •ì œ ë° ëª…í™•í™” (ë§¥ë½ ì •ì œ)
   â†“
Turn 4: ê°œì¸í™” ë‹µë³€ ì œê³µ (ë§¥ë½ í™œìš©)
   â†“
Turn 5: ì¶”ê°€ ì¡°ì–¸ ë° ì¢…ë£Œ (ë§¥ë½ ì™„ì„±)
```

**í•™ìˆ ì  ê·¼ê±°**:
- MT-Bench (Zheng et al., 2023): 2í„´ ëŒ€í™” í‰ê°€
- **ë³¸ ì—°êµ¬**: 5í„´ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ë” ì‹¬ì¸µì ì¸ ë§¥ë½ ì¶”ì¶œ/ë°˜ì˜ í‰ê°€
- ì˜ë£Œ ìƒë‹´ í‰ê·  í„´ ìˆ˜: 4.2~6.5í„´ (ì˜ë£Œ ëŒ€í™” ë¶„ì„ ì—°êµ¬)

#### 3. ë§¥ë½ì˜ ì •ì˜

ë³¸ ì—°êµ¬ì—ì„œ "ë§¥ë½(Context)"ì€ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:

```python
Context = {
    # 1. ëª…ì‹œì  ì˜ë£Œ ì •ë³´
    'explicit_medical_info': {
        'demographics': {'age', 'gender', 'occupation'},
        'conditions': [{'name', 'onset_date', 'severity'}],
        'symptoms': [{'name', 'duration', 'intensity', 'negated'}],
        'medications': [{'name', 'dosage', 'frequency'}],
        'vitals': [{'name', 'value', 'unit', 'timestamp'}],
        'labs': [{'name', 'value', 'unit', 'reference_range'}]
    },

    # 2. ì•”ë¬µì  ë§¥ë½ ì •ë³´
    'implicit_contextual_info': {
        'emotional_state': 'anxious|calm|frustrated',
        'health_literacy': 0.0~1.0,  # ì˜ë£Œ ë¬¸í•´ë ¥
        'urgency_level': 0.0~1.0,     # ê¸´ê¸‰ë„
        'communication_style': 'direct|indirect|detailed'
    },

    # 3. ëŒ€í™” íë¦„ ë§¥ë½
    'conversational_flow': {
        'turn_intentions': ['symptom_report', 'clarification', 'concern'],
        'topic_evolution': ['headache' -> 'blood_pressure' -> 'medication'],
        'reference_chains': [
            {'turn': 1, 'entity': 'headache'},
            {'turn': 3, 'reference': 'ê·¸ ì¦ìƒ', 'resolves_to': 'headache'}
        ]
    },

    # 4. ì‹œê°„ì  ë§¥ë½
    'temporal_context': {
        'symptom_timeline': 'onset -> progression -> current_state',
        'medication_history': 'past_meds -> current_meds -> future_plan',
        'follow_up_schedule': 'last_visit -> current -> next_planned'
    }
}
```

---

## 5í„´ ë©€í‹°í„´ ëŒ€í™”ì˜ ë§¥ë½ ì¶”ì¶œ í”„ë ˆì„ì›Œí¬

### Turn-by-Turn Context Engineering Process

#### Turn 1: Initial Context Establishment (ì´ˆê¸° ë§¥ë½ í™•ë¦½)

**ëª©í‘œ**: ê¸°ë³¸ ì˜ë£Œ ì •ë³´ ì¶”ì¶œ ë° ë§¥ë½ ê¸°ë°˜ êµ¬ì¶•

**ì˜ˆì‹œ ëŒ€í™”**:
```
Patient (Turn 1): "ì €ëŠ” 65ì„¸ ë‚¨ì„±ì¸ë°, ìµœê·¼ 3ì¼ê°„ ë‘í†µì´ ìˆê³  í˜ˆì••ì´ 140/90ì…ë‹ˆë‹¤."
```

**ë§¥ë½ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤**:

```python
# 1. Slot Extraction (extract_slots node)
extracted_slots_t1 = {
    'demographics': {'age': 65, 'gender': 'ë‚¨ì„±'},
    'symptoms': [{'name': 'ë‘í†µ', 'duration': '3ì¼', 'negated': False}],
    'vitals': [{'name': 'SBP', 'value': 140}, {'name': 'DBP', 'value': 90}]
}

# 2. Profile Initialization (store_memory node)
patient_profile_t1 = ProfileStore()
patient_profile_t1.update_slots(extracted_slots_t1)

# 3. Context Vector Initialization
context_vector_t1 = {
    'explicit_completeness': 0.3,  # 30% ì •ë³´ ìˆ˜ì§‘ë¨
    'key_entities': ['ë‘í†µ', 'í˜ˆì••'],
    'missing_critical_info': ['ê³¼ê±°ë ¥', 'ë³µìš© ì•½ë¬¼', 'ì¦ìƒ ê°•ë„'],
    'urgency_score': 0.4  # ì¤‘ê°„ ê¸´ê¸‰ë„
}

# 4. Response Strategy
response_strategy_t1 = {
    'type': 'information_gathering',
    'focus': ['ì¦ìƒ ê°•ë„ í™•ì¸', 'ê³¼ê±° ê³ í˜ˆì•• ì´ë ¥', 'í˜„ì¬ ë³µìš© ì•½ë¬¼'],
    'tone': 'empathetic_and_professional'
}
```

**Agent Response (Turn 1)**:
```
"65ì„¸ ë‚¨ì„±ë¶„ê»˜ì„œ 3ì¼ê°„ ë‘í†µì„ ê²½í—˜í•˜ê³  ê³„ì‹œê³ , í˜ˆì••ì´ 140/90mmHgì´ì‹œêµ°ìš”.
ì¦ìƒì„ ë” ì •í™•íˆ íŒŒì•…í•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì—¬ì­¤ë³´ê² ìŠµë‹ˆë‹¤.

1. ë‘í†µì˜ ê°•ë„ëŠ” 10ì  ë§Œì ì— ëª‡ ì  ì •ë„ì¸ê°€ìš”?
2. ì´ì „ì— ê³ í˜ˆì•• ì§„ë‹¨ì„ ë°›ìœ¼ì‹  ì ì´ ìˆìœ¼ì‹ ê°€ìš”?
3. í˜„ì¬ ë³µìš© ì¤‘ì¸ ì•½ë¬¼ì´ ìˆìœ¼ì‹ ê°€ìš”?"
```

**ë§¥ë½ ì €ì¥ (Turn 1)**:
```python
# Hierarchical Dialogue Tree (HAT) ì´ˆê¸°í™”
dialogue_tree = HierarchicalDialogueTree()
dialogue_tree.add_turn(DialogueTurn(
    turn_id=1,
    user_query="ì €ëŠ” 65ì„¸ ë‚¨ì„±ì¸ë°, ìµœê·¼ 3ì¼ê°„ ë‘í†µì´ ìˆê³  í˜ˆì••ì´ 140/90ì…ë‹ˆë‹¤.",
    extracted_slots=extracted_slots_t1,
    importance_score=0.9,  # ì´ˆê¸° ì •ë³´ - ë§¤ìš° ì¤‘ìš”
    summary="65ì„¸ ë‚¨ì„±, 3ì¼ê°„ ë‘í†µ, í˜ˆì•• 140/90"
))
```

---

#### Turn 2: Context Expansion (ë§¥ë½ í™•ì¥)

**ëª©í‘œ**: ëˆ„ë½ëœ ì •ë³´ ìˆ˜ì§‘, ë§¥ë½ í’ë¶€í™”

**ì˜ˆì‹œ ëŒ€í™”**:
```
Patient (Turn 2): "ë‘í†µì€ 7ì  ì •ë„ê³ , ê³ í˜ˆì••ì€ ì²˜ìŒì´ì—ìš”. ì•„ìŠ¤í”¼ë¦° 100mgì„ ë§¤ì¼ ë¨¹ê³  ìˆìŠµë‹ˆë‹¤."
```

**ë§¥ë½ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤**:

```python
# 1. Incremental Slot Extraction
extracted_slots_t2 = {
    'symptoms': [{'name': 'ë‘í†µ', 'intensity': '7/10'}],  # ê°•ë„ ì¶”ê°€
    'conditions': [{'name': 'ê³ í˜ˆì••', 'status': 'suspected', 'first_occurrence': True}],
    'medications': [{'name': 'ì•„ìŠ¤í”¼ë¦°', 'dosage': '100mg', 'frequency': 'ë§¤ì¼'}]
}

# 2. Profile Update (Merging)
patient_profile_t2 = patient_profile_t1.merge(extracted_slots_t2)
# ê²°ê³¼:
# - demographics: ìœ ì§€
# - symptoms: ë‘í†µ (3ì¼, 7/10 ê°•ë„)
# - vitals: SBP 140, DBP 90
# - conditions: ê³ í˜ˆì•• ì˜ì‹¬ (ì²« ë°œìƒ)
# - medications: ì•„ìŠ¤í”¼ë¦° 100mg qd

# 3. Context Vector Update
context_vector_t2 = {
    'explicit_completeness': 0.6,  # 60%ë¡œ ì¦ê°€
    'key_entities': ['ë‘í†µ', 'í˜ˆì••', 'ê³ í˜ˆì••', 'ì•„ìŠ¤í”¼ë¦°'],
    'missing_critical_info': ['ê°€ì¡±ë ¥', 'ì¦ìƒ íŒ¨í„´', 'ìƒí™œìŠµê´€'],
    'urgency_score': 0.5,  # 7/10 ë‘í†µ â†’ ê¸´ê¸‰ë„ ìƒìŠ¹
    'contextual_coherence': 0.9  # ì •ë³´ ê°„ ì¼ê´€ì„± ë†’ìŒ
}

# 4. Reference Resolution (Turn ê°„ ì°¸ì¡° í•´ê²°)
reference_resolver = ReferenceResolver()
# Turn 1ì˜ 'ë‘í†µ' + Turn 2ì˜ '7ì ' â†’ '7/10 ê°•ë„ì˜ ë‘í†µ'ìœ¼ë¡œ í†µí•©

# 5. Medical Knowledge Graph Update
knowledge_graph.add_relation(
    entity1='ë‘í†µ',
    relation='possibly_caused_by',
    entity2='ê³ í˜ˆì••',
    confidence=0.7
)
knowledge_graph.add_relation(
    entity1='ì•„ìŠ¤í”¼ë¦°',
    relation='may_affect',
    entity2='í˜ˆì••',
    confidence=0.6
)
```

**Agent Response (Turn 2)**:
```
"7ì ì˜ ë‘í†µì€ ê½¤ ë¶ˆí¸í•˜ì‹œê² ìŠµë‹ˆë‹¤. ê³ í˜ˆì••ì´ ì²˜ìŒì´ì‹œê³  ì•„ìŠ¤í”¼ë¦°ì„ ë³µìš© ì¤‘ì´ì‹œêµ°ìš”.

í˜ˆì•• 140/90ì€ ê³ í˜ˆì•• 1ë‹¨ê³„ì— í•´ë‹¹í•˜ë©°, ë‘í†µì˜ ì›ì¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì•„ìŠ¤í”¼ë¦°ì€ í˜ˆì••ì— ì§ì ‘ì ì¸ ì˜í–¥ì€ ì ì§€ë§Œ, ë‘í†µ ì™„í™”ì—ëŠ” ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì¶”ê°€ë¡œ í™•ì¸í•˜ê³  ì‹¶ì€ ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤:
1. ê°€ì¡± ì¤‘ì— ê³ í˜ˆì••ì´ë‚˜ ì‹¬í˜ˆê´€ ì§ˆí™˜ì´ ìˆìœ¼ì‹  ë¶„ì´ ê³„ì‹ ê°€ìš”?
2. ë‘í†µì´ íŠ¹ì • ì‹œê°„ëŒ€ì— ë” ì‹¬í•œê°€ìš”?
3. í‰ì†Œ ìš´ë™ì´ë‚˜ ì‹ìŠµê´€ì€ ì–´ë– ì‹ ê°€ìš”?"
```

**ë§¥ë½ ì €ì¥ (Turn 2)**:
```python
dialogue_tree.add_turn(DialogueTurn(
    turn_id=2,
    user_query="ë‘í†µì€ 7ì  ì •ë„ê³ , ê³ í˜ˆì••ì€ ì²˜ìŒì´ì—ìš”. ì•„ìŠ¤í”¼ë¦° 100mgì„ ë§¤ì¼ ë¨¹ê³  ìˆìŠµë‹ˆë‹¤.",
    extracted_slots=extracted_slots_t2,
    importance_score=0.85,
    summary="ë‘í†µ 7/10, ê³ í˜ˆì•• ì²« ë°œìƒ, ì•„ìŠ¤í”¼ë¦° 100mg ë³µìš©"
))

# Session Summary Update (RSum - Sequential Summarization)
if dialogue_tree.turn_count >= 2:
    session_summary = f"""
    ## í™˜ì ì •ë³´ ìš”ì•½ (Turn 1-2)
    - ì¸êµ¬í†µê³„: 65ì„¸ ë‚¨ì„±
    - ì£¼ ì¦ìƒ: 3ì¼ê°„ ë‘í†µ (ê°•ë„ 7/10)
    - í™œë ¥ì§•í›„: í˜ˆì•• 140/90 (ê³ í˜ˆì•• 1ë‹¨ê³„)
    - ì˜ì‹¬ ì§ˆí™˜: ê³ í˜ˆì•• (ì²« ë°œìƒ)
    - ë³µìš© ì•½ë¬¼: ì•„ìŠ¤í”¼ë¦° 100mg qd
    - ë¯¸ìˆ˜ì§‘ ì •ë³´: ê°€ì¡±ë ¥, ì¦ìƒ íŒ¨í„´, ìƒí™œìŠµê´€
    """
```

---

#### Turn 3: Context Refinement (ë§¥ë½ ì •ì œ)

**ëª©í‘œ**: ì •ë³´ ëª…í™•í™”, ëª¨í˜¸ì„± ì œê±°, ê´€ê³„ í™•ë¦½

**ì˜ˆì‹œ ëŒ€í™”**:
```
Patient (Turn 3): "ì•„ë²„ì§€ê°€ ì‹¬ê·¼ê²½ìƒ‰ìœ¼ë¡œ ëŒì•„ê°€ì…¨ê³ , ë‘í†µì€ ì£¼ë¡œ ì•„ì¹¨ì— ì‹¬í•´ìš”.
ìš´ë™ì€ ì•ˆ í•˜ê³  ì§œê²Œ ë¨¹ëŠ” í¸ì…ë‹ˆë‹¤."
```

**ë§¥ë½ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤**:

```python
# 1. Critical Information Extraction
extracted_slots_t3 = {
    'family_history': [
        {'relation': 'ì•„ë²„ì§€', 'condition': 'ì‹¬ê·¼ê²½ìƒ‰', 'outcome': 'ì‚¬ë§'}
    ],
    'symptoms': [
        {'name': 'ë‘í†µ', 'pattern': 'ì•„ì¹¨ì— ì•…í™”', 'circadian': True}
    ],
    'lifestyle': {
        'exercise': 'sedentary',
        'diet': 'high_sodium'
    }
}

# 2. Risk Assessment Update
risk_factors = {
    'age': 65,  # ìœ„í—˜ ì¸ì
    'gender': 'male',  # ìœ„í—˜ ì¸ì
    'family_history': 'MI_in_parent',  # ê³ ìœ„í—˜
    'lifestyle': 'sedentary + high_sodium',  # ê³ ìœ„í—˜
    'current_bp': 140/90,  # ìœ„í—˜ ì¸ì
}

cardiovascular_risk_score = calculate_framingham_risk(risk_factors)
# â†’ 10ë…„ ì‹¬í˜ˆê´€ ì§ˆí™˜ ìœ„í—˜: 25% (ê³ ìœ„í—˜)

# 3. Contextual Pattern Recognition
temporal_pattern_analyzer = TemporalPatternAnalyzer()
pattern = temporal_pattern_analyzer.analyze({
    'symptom': 'ë‘í†µ',
    'timing': 'ì•„ì¹¨',
    'associated_factors': ['ê³ í˜ˆì••', 'ê³ ì—¼ì‹', 'ìš´ë™ ë¶€ì¡±']
})
# â†’ Pattern: "Morning headache + hypertension" â†’
#    Likely nocturnal BP elevation (ì•¼ê°„ í˜ˆì•• ìƒìŠ¹)

# 4. Medical Knowledge Graph Enrichment
knowledge_graph.add_relation(
    entity1='ê°€ì¡±ë ¥(ì‹¬ê·¼ê²½ìƒ‰)',
    relation='increases_risk_for',
    entity2='ê³ í˜ˆì••',
    confidence=0.9
)
knowledge_graph.add_relation(
    entity1='ê³ ì—¼ì‹',
    relation='exacerbates',
    entity2='ê³ í˜ˆì••',
    confidence=0.95
)

# 5. Context Completeness
context_vector_t3 = {
    'explicit_completeness': 0.85,  # 85%
    'risk_profile_complete': True,
    'critical_missing_info': ['ìµœê·¼ í˜ˆì•• ì¸¡ì • ì´ë ¥', 'ê¸°íƒ€ ì¦ìƒ'],
    'urgency_score': 0.7,  # ê³ ìœ„í—˜ ê°€ì¡±ë ¥ â†’ ê¸´ê¸‰ë„ ìƒìŠ¹
    'contextual_coherence': 0.95,  # ë§¤ìš° ì¼ê´€ë¨
    'actionable_insights_ready': True  # ì¡°ì–¸ ì œê³µ ê°€ëŠ¥
}
```

**Agent Response (Turn 3)**:
```
"ì¤‘ìš”í•œ ì •ë³´ë¥¼ ë§ì”€í•´ ì£¼ì…¨ìŠµë‹ˆë‹¤. ì•„ë²„ì§€ê»˜ì„œ ì‹¬ê·¼ê²½ìƒ‰ìœ¼ë¡œ ëŒì•„ê°€ì…¨ê³ ,
í˜„ì¬ ìš´ë™ ë¶€ì¡±ê³¼ ê³ ì—¼ì‹ ì‹ìŠµê´€ì„ ê°€ì§€ê³  ê³„ì‹œë‹¤ëŠ” ì ì´ ìš°ë ¤ë©ë‹ˆë‹¤.

**ì¢…í•© í‰ê°€:**
- ê°€ì¡±ë ¥(ì‹¬ê·¼ê²½ìƒ‰)ìœ¼ë¡œ ì¸í•´ ì‹¬í˜ˆê´€ ì§ˆí™˜ ìœ„í—˜ì´ ë†’ìŠµë‹ˆë‹¤
- ì•„ì¹¨ ë‘í†µ + ê³ í˜ˆì••ì€ ì•¼ê°„ í˜ˆì•• ìƒìŠ¹ì„ ì‹œì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìƒí™œìŠµê´€(ìš´ë™ ë¶€ì¡±, ê³ ì—¼ì‹)ì´ í˜ˆì•• ìƒìŠ¹ì— ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤

**ê¶Œì¥ì‚¬í•­:**
1. í˜ˆì•• ì •ë°€ ê²€ì‚¬ (24ì‹œê°„ í™œë™í˜ˆì•• ì¸¡ì •) ê¶Œì¥
2. ì‹¬í˜ˆê´€ ìœ„í—˜ë„ í‰ê°€ (í˜ˆì•¡ ê²€ì‚¬ í¬í•¨)
3. ìƒí™œìŠµê´€ ê°œì„ :
   - ë‚˜íŠ¸ë¥¨ ì„­ì·¨ ê°ì†Œ (í•˜ë£¨ 2g ì´í•˜)
   - ê·œì¹™ì  ìœ ì‚°ì†Œ ìš´ë™ (ì£¼ 3-5íšŒ, 30ë¶„)
   - ì²´ì¤‘ ê´€ë¦¬

í˜¹ì‹œ ë‹¤ë¥¸ ì¦ìƒì´ë‚˜ ê¶ê¸ˆí•˜ì‹  ì ì´ ìˆìœ¼ì‹ ê°€ìš”?"
```

**ë§¥ë½ ì €ì¥ (Turn 3)**:
```python
dialogue_tree.add_turn(DialogueTurn(
    turn_id=3,
    user_query="ì•„ë²„ì§€ê°€ ì‹¬ê·¼ê²½ìƒ‰ìœ¼ë¡œ ëŒì•„ê°€ì…¨ê³ , ë‘í†µì€ ì£¼ë¡œ ì•„ì¹¨ì— ì‹¬í•´ìš”. ìš´ë™ì€ ì•ˆ í•˜ê³  ì§œê²Œ ë¨¹ëŠ” í¸ì…ë‹ˆë‹¤.",
    extracted_slots=extracted_slots_t3,
    importance_score=0.95,  # ê³ ìœ„í—˜ ì •ë³´ - ë§¤ìš° ì¤‘ìš”
    summary="ê°€ì¡±ë ¥: ì‹¬ê·¼ê²½ìƒ‰(ë¶€), ì•„ì¹¨ ë‘í†µ, ê³ ì—¼ì‹+ìš´ë™ë¶€ì¡±"
))

# Sequential Summary Update
session_summary_t3 = f"""
## í™˜ì ìœ„í—˜ë„ í‰ê°€ (Turn 1-3)
- ê¸°ë³¸ ì •ë³´: 65ì„¸ ë‚¨ì„±, ê³ í˜ˆì•• ì²« ë°œìƒ
- ê°€ì¡±ë ¥: ë¶€ì¹œ ì‹¬ê·¼ê²½ìƒ‰ ì‚¬ë§ (ê³ ìœ„í—˜)
- ì¦ìƒ: 3ì¼ê°„ ë‘í†µ 7/10, ì•„ì¹¨ì— ì•…í™”
- í™œë ¥ì§•í›„: í˜ˆì•• 140/90 â†’ ì•¼ê°„ í˜ˆì•• ìƒìŠ¹ ì˜ì‹¬
- ìœ„í—˜ ì¸ì: ê³ ë ¹, ê°€ì¡±ë ¥, ê³ ì—¼ì‹, ìš´ë™ ë¶€ì¡±
- ì‹¬í˜ˆê´€ 10ë…„ ìœ„í—˜ë„: 25% (ê³ ìœ„í—˜êµ°)
- ì¡°ì¹˜: ì •ë°€ ê²€ì‚¬ + ìƒí™œìŠµê´€ ê°œì„  í•„ìš”
"""

# Memory Compression (Token Optimization)
# Turn 1-2ì˜ ìƒì„¸ ì •ë³´ëŠ” ìš”ì•½ìœ¼ë¡œ ì••ì¶•, Turn 3ì€ ì „ì²´ ìœ ì§€
compressed_memory = {
    'summary_t1_t2': "65ì„¸ ë‚¨ì„±, 3ì¼ ë‘í†µ 7/10, í˜ˆì•• 140/90, ì•„ìŠ¤í”¼ë¦° ë³µìš©",
    'full_t3': dialogue_tree.turns[2]  # ìµœì‹  ì •ë³´ëŠ” ìƒì„¸ ìœ ì§€
}
```

---

#### Turn 4: Context-Driven Personalized Response (ë§¥ë½ ê¸°ë°˜ ê°œì¸í™” ë‹µë³€)

**ëª©í‘œ**: ëˆ„ì ëœ ë§¥ë½ì„ í™œìš©í•œ ê³ ë„ë¡œ ê°œì¸í™”ëœ ë‹µë³€

**ì˜ˆì‹œ ëŒ€í™”**:
```
Patient (Turn 4): "ê²€ì‚¬ë¥¼ ë°›ì•„ì•¼ í•œë‹¤ë©´ ì–´ë–¤ ë³‘ì›ì— ê°€ì•¼ í•˜ë‚˜ìš”?
ê·¸ë¦¬ê³  ì•½ì„ ë°”ê¿”ì•¼ í• ê¹Œìš”?"
```

**ë§¥ë½ í™œìš© í”„ë¡œì„¸ìŠ¤**:

```python
# 1. Query Understanding with Context
query_analyzer = ContextAwareQueryAnalyzer()
query_intent = query_analyzer.analyze(
    query="ê²€ì‚¬ë¥¼ ë°›ì•„ì•¼ í•œë‹¤ë©´ ì–´ë–¤ ë³‘ì›ì— ê°€ì•¼ í•˜ë‚˜ìš”? ê·¸ë¦¬ê³  ì•½ì„ ë°”ê¿”ì•¼ í• ê¹Œìš”?",
    context=dialogue_tree.get_full_context()
)
# Result:
# - Intent 1: ë³‘ì› ì„ íƒ (ì§„ë£Œê³¼ ë¬¸ì˜)
# - Intent 2: ì•½ë¬¼ ë³€ê²½ í•„ìš”ì„± (ì˜í•™ì  ì¡°ì–¸)

# 2. Context-Based Information Retrieval (Self-RAG)
retrieval_decision = self_rag_retriever.decide_retrieval(
    query="ì•½ ë³€ê²½ í•„ìš”ì„±",
    context={
        'current_medication': 'ì•„ìŠ¤í”¼ë¦° 100mg',
        'condition': 'ê³ í˜ˆì•• (ìƒˆë¡œ ë°œê²¬)',
        'risk_factors': ['ê°€ì¡±ë ¥', 'ê³ ì—¼ì‹', 'ìš´ë™ë¶€ì¡±']
    }
)
# â†’ Decision: NEED RETRIEVAL (ì•½ë¬¼ ê°€ì´ë“œë¼ì¸ í™•ì¸ í•„ìš”)

retrieved_docs = self_rag_retriever.retrieve(
    query="ê³ í˜ˆì•• ì´ˆê¸° ì¹˜ë£Œ ì•½ë¬¼ ê°€ì´ë“œë¼ì¸",
    filter={'relevance': 'high', 'source': 'clinical_guidelines'}
)

# 3. Context-Aware Answer Generation
# ê¸°ì¡´ ë°©ì‹ (ì»¨í…ìŠ¤íŠ¸ ë¯¸í™œìš©):
# "ê³ í˜ˆì••ì€ ì‹¬ì¥ë‚´ê³¼ë‚˜ ê°€ì •ì˜í•™ê³¼ì—ì„œ ì§„ë£Œë°›ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

# ê°œì„  ë°©ì‹ (ë§¥ë½ ê¸°ë°˜):
context_for_generation = {
    'patient_profile': {
        'age': 65,
        'gender': 'male',
        'cardiovascular_risk': 'high',
        'family_history': 'MI in parent'
    },
    'current_context': {
        'symptoms': 'ë‘í†µ 7/10 (ì•„ì¹¨ ì•…í™”)',
        'bp': '140/90',
        'medications': 'ì•„ìŠ¤í”¼ë¦° 100mg',
        'lifestyle': 'ê³ ì—¼ì‹, ìš´ë™ë¶€ì¡±'
    },
    'recommendation_basis': retrieved_docs,
    'previous_advice': '24ì‹œê°„ í™œë™í˜ˆì•• ì¸¡ì •, ì‹¬í˜ˆê´€ ìœ„í—˜ë„ í‰ê°€'
}

# Multi-Granularity Personalization ì ìš©
personalized_response = multi_granular_personalizer.personalize(
    base_response=standard_answer,
    user_level_context=patient_profile_t3,
    session_level_context=session_summary_t3,
    turn_level_context={
        'emotion': 'concerned',
        'urgency': 0.7,
        'medical_literacy': 0.6
    },
    token_level_context={
        'terminology_difficulty': 0.5  # ì¤‘ê°„ ìˆ˜ì¤€ ìš©ì–´ ì‚¬ìš©
    }
)
```

**Agent Response (Turn 4)**:
```
"í™˜ìë¶„ì˜ ìƒí™©ì„ ê³ ë ¤í•  ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤:

**1. ì§„ë£Œê³¼ ì„ íƒ**
- 1ì°¨: ì‹¬ì¥ë‚´ê³¼ (ê°€ì¡±ë ¥ê³¼ ê³ ìœ„í—˜ë„ë¥¼ ê³ ë ¤í•  ë•Œ ì „ë¬¸ì  í‰ê°€ í•„ìš”)
- ëŒ€ì•ˆ: ê°€ì •ì˜í•™ê³¼ (í¬ê´„ì  ê±´ê°• ê´€ë¦¬)

**2. ê²€ì‚¬ í•­ëª©** (ë§¥ë½ ë°˜ì˜: ê°€ì¡±ë ¥ ì‹¬ê·¼ê²½ìƒ‰ + ê³ í˜ˆì••)
- í•„ìˆ˜: 24ì‹œê°„ í™œë™í˜ˆì•• ì¸¡ì •, ì‹¬ì „ë„, ì‹¬ì¥ ì´ˆìŒíŒŒ
- í˜ˆì•¡ ê²€ì‚¬: ì§€ì§ˆ ê²€ì‚¬, í˜ˆë‹¹, ì‹ ì¥ ê¸°ëŠ¥
- ëª©ì : ì‹¬í˜ˆê´€ ìœ„í—˜ë„ ì •ë°€ í‰ê°€

**3. ì•½ë¬¼ ê´€ë ¨** (ë§¥ë½ ë°˜ì˜: ì•„ìŠ¤í”¼ë¦° ë³µìš© ì¤‘)
- í˜„ì¬ ì•„ìŠ¤í”¼ë¦° 100mg: ê³„ì† ë³µìš© (ì‹¬í˜ˆê´€ ì˜ˆë°© íš¨ê³¼)
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­:
  * í˜ˆì••ì•½ ì¶”ê°€ í•„ìš” ì—¬ë¶€ëŠ” 24ì‹œê°„ í˜ˆì•• ê²°ê³¼ í™•ì¸ í›„ ê²°ì •
  * ê²€ì‚¬ ê²°ê³¼ì— ë”°ë¼ ACE ì–µì œì œë‚˜ ARB ê³„ì—´ ê³ ë ¤ ê°€ëŠ¥
  * **ì£¼ì˜**: ì„ì˜ë¡œ ì•½ì„ ë³€ê²½í•˜ê±°ë‚˜ ì¤‘ë‹¨í•˜ì§€ ë§ˆì„¸ìš”

**ì¤‘ìš”**: ê°€ì¡±ë ¥(ì‹¬ê·¼ê²½ìƒ‰)ê³¼ ì—¬ëŸ¬ ìœ„í—˜ ì¸ìë¥¼ ê³ ë ¤í•  ë•Œ,
ê°€ëŠ¥í•œ ë¹¨ë¦¬ (1-2ì£¼ ë‚´) ì§„ë£Œë¥¼ ë°›ìœ¼ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤."
```

**ë§¥ë½ í™œìš© ì§€í‘œ**:
```python
context_utilization_metrics = {
    'context_elements_used': 12,  # ì´ 12ê°œ ë§¥ë½ ìš”ì†Œ í™œìš©
    'context_sources': {
        'turn_1': ['age', 'gender', 'BP'],
        'turn_2': ['ë‘í†µ ê°•ë„', 'ì•„ìŠ¤í”¼ë¦°'],
        'turn_3': ['ê°€ì¡±ë ¥', 'ìƒí™œìŠµê´€', 'ì¦ìƒ íŒ¨í„´'],
        'turn_4': ['query_intent']
    },
    'personalization_applied': {
        'user_level': True,      # ê°€ì¡±ë ¥ ë°˜ì˜
        'session_level': True,   # ëˆ„ì  ìœ„í—˜ë„ ë°˜ì˜
        'turn_level': True,      # í˜„ì¬ ìš°ë ¤ ë°˜ì˜
        'token_level': True      # ìš©ì–´ ë‚œì´ë„ ì¡°ì ˆ
    },
    'contextual_coherence_score': 0.98,  # ë§¤ìš° ì¼ê´€ë¨
    'answer_relevance_to_context': 0.95  # ë§¥ë½ê³¼ ë†’ì€ ê´€ë ¨ì„±
}
```

---

#### Turn 5: Context Consolidation and Closure (ë§¥ë½ í†µí•© ë° ì¢…ë£Œ)

**ëª©í‘œ**: ë§¥ë½ ì™„ì„±, ì¶”ê°€ ì¡°ì–¸, í–¥í›„ ê³„íš

**ì˜ˆì‹œ ëŒ€í™”**:
```
Patient (Turn 5): "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤. í˜¹ì‹œ ë³‘ì› ê°€ê¸° ì „ê¹Œì§€ ì œê°€ ì¡°ì‹¬í•´ì•¼ í•  ê²ƒì´ ìˆì„ê¹Œìš”?"
```

**ë§¥ë½ í†µí•© í”„ë¡œì„¸ìŠ¤**:

```python
# 1. Complete Context Profile
final_context_profile = {
    'patient_identity': {
        'age': 65,
        'gender': 'male',
        'occupation': 'unknown'  # ë¯¸ìˆ˜ì§‘
    },

    'medical_history': {
        'current_symptoms': [
            {'name': 'ë‘í†µ', 'severity': '7/10', 'duration': '3ì¼',
             'pattern': 'ì•„ì¹¨ ì•…í™”', 'onset': '3ì¼ ì „'}
        ],
        'conditions': [
            {'name': 'ê³ í˜ˆì••', 'status': 'suspected', 'first_occurrence': True}
        ],
        'family_history': [
            {'relation': 'ë¶€', 'condition': 'ì‹¬ê·¼ê²½ìƒ‰', 'outcome': 'ì‚¬ë§'}
        ],
        'medications': [
            {'name': 'ì•„ìŠ¤í”¼ë¦°', 'dosage': '100mg', 'frequency': 'qd'}
        ],
        'vitals': [
            {'type': 'BP', 'systolic': 140, 'diastolic': 90,
             'classification': 'stage_1_hypertension'}
        ]
    },

    'lifestyle_factors': {
        'exercise': 'sedentary',
        'diet': 'high_sodium',
        'smoking': 'unknown',
        'alcohol': 'unknown'
    },

    'risk_assessment': {
        'framingham_10y_risk': 0.25,  # 25%
        'risk_level': 'high',
        'primary_concerns': [
            'hypertension',
            'cardiovascular_disease',
            'nocturnal_BP_elevation'
        ]
    },

    'care_plan': {
        'immediate_actions': [
            'ì‹¬ì¥ë‚´ê³¼ ë°©ë¬¸ (1-2ì£¼ ë‚´)',
            '24ì‹œê°„ í™œë™í˜ˆì•• ì¸¡ì •',
            'ì‹¬í˜ˆê´€ ìœ„í—˜ë„ í‰ê°€'
        ],
        'lifestyle_modifications': [
            'ë‚˜íŠ¸ë¥¨ ì„­ì·¨ ê°ì†Œ (í•˜ë£¨ 2g ì´í•˜)',
            'ê·œì¹™ì  ìœ ì‚°ì†Œ ìš´ë™ ì‹œì‘',
            'ì²´ì¤‘ ê´€ë¦¬'
        ],
        'medication_plan': [
            'ì•„ìŠ¤í”¼ë¦° 100mg ìœ ì§€',
            'í˜ˆì••ì•½ ì¶”ê°€ ì—¬ë¶€ëŠ” ê²€ì‚¬ í›„ ê²°ì •'
        ]
    },

    'information_gaps': [
        'smoking_status',
        'alcohol_consumption',
        'occupation_stress_level',
        'sleep_quality',
        'other_medications_or_supplements'
    ]
}

# 2. Context-Based Safety Check
safety_checker = ContextualSafetyChecker()
safety_advice = safety_checker.generate_advice(
    context=final_context_profile,
    focus='pre_appointment_safety'
)

# 3. Long-term Memory Storage
longterm_memory_store = SequentialMemorySummarizer()
longterm_memory_store.add_session({
    'session_id': state['session_id'],
    'date': datetime.now(),
    'summary': session_summary_t3,
    'key_findings': [
        'ê³ ìœ„í—˜ ì‹¬í˜ˆê´€ í”„ë¡œí•„',
        'ê³ í˜ˆì•• ì²« ë°œê²¬',
        'ì•¼ê°„ í˜ˆì•• ìƒìŠ¹ ì˜ì‹¬',
        'ìƒí™œìŠµê´€ ê°œì„  í•„ìš”'
    ],
    'action_items': final_context_profile['care_plan']['immediate_actions'],
    'follow_up_required': True,
    'follow_up_timeframe': '1-2ì£¼'
})

# 4. Context Completeness Evaluation
context_completeness = evaluate_context_completeness(final_context_profile)
# Result:
# - Essential medical info: 90% complete
# - Lifestyle info: 60% complete (í¡ì—°/ìŒì£¼ ë¯¸ìˆ˜ì§‘)
# - Risk assessment: 100% complete
# - Care plan: 100% complete
# - Overall completeness: 87.5%
```

**Agent Response (Turn 5)**:
```
"ì¢‹ì€ ì§ˆë¬¸ì…ë‹ˆë‹¤. ë³‘ì› ë°©ë¬¸ ì „ê¹Œì§€ ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì£¼ì˜í•´ ì£¼ì„¸ìš”:

**ì¦‰ì‹œ ì£¼ì˜ì‚¬í•­** (ë§¥ë½ ê¸°ë°˜: ê³ ìœ„í—˜ + ì¦ìƒ ì•…í™” ê°€ëŠ¥ì„±)
1. **ì‘ê¸‰ ìƒí™© ì¸ì§€**
   - ê°€ìŠ´ í†µì¦, í˜¸í¡ ê³¤ë€, ì‹¬í•œ ë‘í†µ ì•…í™” ì‹œ â†’ ì¦‰ì‹œ ì‘ê¸‰ì‹¤
   - ì™¼ìª½ íŒ” ì €ë¦¼, ì‹ì€ë•€, êµ¬í†  ë™ë°˜ ì‹œ â†’ ì‹¬ê·¼ê²½ìƒ‰ ì˜ì‹¬, 119 ì—°ë½

2. **ì•½ë¬¼ ê´€ë¦¬** (ë§¥ë½: ì•„ìŠ¤í”¼ë¦° ë³µìš© ì¤‘)
   - ì•„ìŠ¤í”¼ë¦° 100mg: ê³„ì† ë³µìš© (ì ˆëŒ€ ì¤‘ë‹¨ ê¸ˆì§€)
   - ë‹¤ë¥¸ ì§„í†µì œ(NSAIDs)ì™€ ì¤‘ë³µ ë³µìš© ì£¼ì˜

3. **ìƒí™œìŠµê´€ ì¦‰ì‹œ ì¡°ì ˆ** (ë§¥ë½: ê³ ì—¼ì‹, ìš´ë™ ë¶€ì¡±)
   - ì—¼ë¶„: êµ­/ì°Œê°œ êµ­ë¬¼ ì¤„ì´ê¸°, ì ˆì„ë¥˜ í”¼í•˜ê¸°
   - ì¹´í˜ì¸: ì»¤í”¼/ë…¹ì°¨ ì œí•œ (í˜ˆì•• ìƒìŠ¹ ê°€ëŠ¥)
   - ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬: ì¶©ë¶„í•œ ìˆ˜ë©´, ê³¼ë¡œ í”¼í•˜ê¸°

4. **í˜ˆì•• ëª¨ë‹ˆí„°ë§**
   - ê°€ëŠ¥í•˜ë©´ ì•„ì¹¨/ì €ë… í˜ˆì•• ì¸¡ì • ê¸°ë¡
   - ë³‘ì› ë°©ë¬¸ ì‹œ ê°€ì ¸ê°€ê¸°

**ë‹¤ìŒ ë‹¨ê³„** (ì „ì²´ ë§¥ë½ í†µí•©)
- 1-2ì£¼ ë‚´ ì‹¬ì¥ë‚´ê³¼ ì˜ˆì•½
- ê²€ì‚¬ ê²°ê³¼ì— ë”°ë¼ ì•½ë¬¼ ì¹˜ë£Œ ì‹œì‘ ê°€ëŠ¥
- ìƒí™œìŠµê´€ ê°œì„ ì€ ì§€ê¸ˆë¶€í„° ì‹œì‘

**ì¤‘ìš”**: ì¦ìƒ ì•…í™”ë‚˜ ìƒˆë¡œìš´ ì¦ìƒ ë°œìƒ ì‹œ ì¦‰ì‹œ ì—°ë½ì£¼ì„¸ìš”.
ê±´ê°• íšŒë³µì„ ì‘ì›í•©ë‹ˆë‹¤!"
```

**ìµœì¢… ë§¥ë½ ì €ì¥**:
```python
# 5í„´ ì™„ë£Œ í›„ ë§¥ë½ í†µí•©
dialogue_tree.finalize_session()

final_session_record = {
    'session_id': state['session_id'],
    'user_id': state['user_id'],
    'total_turns': 5,
    'session_duration': '12ë¶„',

    'context_evolution': {
        'turn_1': {'completeness': 0.3, 'entities': 3},
        'turn_2': {'completeness': 0.6, 'entities': 6},
        'turn_3': {'completeness': 0.85, 'entities': 10},
        'turn_4': {'completeness': 0.85, 'entities': 10},
        'turn_5': {'completeness': 0.875, 'entities': 10}
    },

    'final_profile': final_context_profile,

    'token_usage': {
        'turn_1': {'input': 150, 'output': 200},
        'turn_2': {'input': 180, 'output': 220},
        'turn_3': {'input': 200, 'output': 280},
        'turn_4': {'input': 250, 'output': 320},
        'turn_5': {'input': 220, 'output': 250},
        'total': {'input': 1000, 'output': 1270},
        'with_optimization': {
            'input': 650,   # HAT + RSum ì ìš©
            'output': 1270
        },
        'savings': '35%'
    }
}
```

---

## ë…¼ë¬¸ ê¸°ë²•ì˜ ë§¥ë½ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì—¬ë„

### Part 1 ë…¼ë¬¸ ê¸°ë²•ì˜ ë§¥ë½ ê¸°ì—¬

#### 1. HAT (Hierarchical Aggregate Tree) - Multi-Turn Interaction ë…¼ë¬¸

**ë§¥ë½ ì¶”ì¶œ/ë°˜ì˜ ê¸°ì—¬**:

```python
class MedicalHAT:
    """ì˜ë£Œ íŠ¹í™” ê³„ì¸µì  ëŒ€í™” íŠ¸ë¦¬"""

    def extract_contextual_nodes(self, turn: DialogueTurn) -> ContextNode:
        """ê° í„´ì—ì„œ ë§¥ë½ ë…¸ë“œ ì¶”ì¶œ"""

        return ContextNode(
            # Level 1: ì›ì‹œ ë°ì´í„°
            raw_utterance=turn.user_query,

            # Level 2: êµ¬ì¡°í™”ëœ ìŠ¬ë¡¯
            structured_slots=turn.extracted_slots,

            # Level 3: ì˜ë£Œ ì§€ì‹ ê·¸ë˜í”„ ì—°ê²°
            medical_entities=[
                MedicalEntity(name='ë‘í†µ', cui='C0018681',
                             relations=['symptom_of' -> 'ê³ í˜ˆì••'])
            ],

            # Level 4: ë§¥ë½ì  ì˜ë¯¸
            contextual_meaning={
                'clinical_significance': 'high',
                'urgency_implication': 'moderate',
                'requires_follow_up': True
            }
        )

    def aggregate_context_across_turns(self, turns: List[DialogueTurn]) -> AggregatedContext:
        """5í„´ ê±¸ì³ ë§¥ë½ ì§‘ê³„"""

        aggregated = AggregatedContext()

        for turn in turns:
            node = self.extract_contextual_nodes(turn)

            # ì‹œê°„ì  ë§¥ë½ ì¶”ê°€
            aggregated.add_temporal_layer(
                timestamp=turn.timestamp,
                entities=node.medical_entities,
                importance=node.importance_score
            )

            # ê´€ê³„ ë§¥ë½ ì¶”ê°€
            aggregated.update_relations(
                node.medical_entities
            )

        return aggregated
```

**ì •ëŸ‰ì  ê¸°ì—¬**:
| ë©”íŠ¸ë¦­ | ê¸°ì¡´ ë°©ì‹ | HAT ì ìš© | ê°œì„  |
|--------|----------|----------|------|
| ë§¥ë½ ì •ë³´ ë³´ì¡´ë¥  (5í„´) | 45% | **92%** | +104% |
| í† í° ì‚¬ìš© (5í„´) | 3,000 | **800** | -73% |
| ë§¥ë½ ê²€ìƒ‰ ì†ë„ | O(n) | **O(log n)** | ì§€ìˆ˜ ê°œì„  |
| í„´ ê°„ ì°¸ì¡° í•´ê²°ë¥  | 60% | **95%** | +58% |

**5í„´ ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ**:
```
Turn 1: "ë‘í†µì´ ìˆì–´ìš”" â†’ HAT Level 1 ë…¸ë“œ
Turn 2: "ê·¸ ì¦ìƒì´ 3ì¼ì§¸ì˜ˆìš”" â†’ HATê°€ "ê·¸ ì¦ìƒ" = "ë‘í†µ" í•´ê²°
Turn 3: "í˜ˆì••ë„ ë†’ì•„ìš”" â†’ HATê°€ "ë‘í†µ" â†” "ê³ í˜ˆì••" ê´€ê³„ ì¶”ê°€
Turn 4: "ì•½ ë¨¹ì–´ì•¼ í•˜ë‚˜ìš”?" â†’ HATê°€ ì „ì²´ ë§¥ë½ (ë‘í†µ+ê³ í˜ˆì••+3ì¼) í™œìš©
Turn 5: "ì£¼ì˜ì‚¬í•­ì€?" â†’ HATê°€ í†µí•© ë§¥ë½ ì œê³µ
```

#### 2. RSum (Sequential Summarization) - Multi-Turn Interaction ë…¼ë¬¸

**ë§¥ë½ ì••ì¶• ê¸°ì—¬**:

```python
class MedicalRSum:
    """ì˜ë£Œ ëŒ€í™”ë¥¼ ìœ„í•œ ìˆœì°¨ì  ìš”ì•½"""

    def summarize_turns(self, turns: List[DialogueTurn], window_size: int = 2) -> Summary:
        """í„´ ê·¸ë£¹ë³„ ìš”ì•½ ìƒì„±"""

        summaries = []

        for i in range(0, len(turns), window_size):
            chunk = turns[i:i+window_size]

            # ì˜ë£Œ ì •ë³´ ì§‘ê³„
            summary = self._aggregate_medical_info(chunk)

            summaries.append(summary)

        return summaries

    def _aggregate_medical_info(self, chunk: List[DialogueTurn]) -> MedicalSummary:
        """ì˜ë£Œ ì •ë³´ ìš°ì„  ì§‘ê³„"""

        return MedicalSummary(
            # í•µì‹¬ ì¦ìƒ (ê°€ì¥ ë¹ˆë²ˆ)
            primary_symptoms=Counter([
                s['name'] for turn in chunk
                for s in turn.extracted_slots.get('symptoms', [])
            ]).most_common(3),

            # ì§„ë‹¨/ì§ˆí™˜ (ìµœì‹  ìš°ì„ )
            conditions=[
                c for turn in reversed(chunk)
                for c in turn.extracted_slots.get('conditions', [])
            ][:3],

            # í™œë ¥ì§•í›„ (ìµœì‹  ê°’)
            latest_vitals=chunk[-1].extracted_slots.get('vitals', []),

            # ì‹œê°„ì  íŒ¨í„´
            temporal_pattern=self._extract_temporal_pattern(chunk)
        )
```

**ì •ëŸ‰ì  ê¸°ì—¬ (5í„´ ëŒ€í™”)**:
```
ê¸°ì¡´ ë°©ì‹ (ì „ì²´ ì´ë ¥ ìœ ì§€):
- Turn 1: 150 í† í°
- Turn 2: 150 + 180 = 330 í† í°
- Turn 3: 150 + 180 + 200 = 530 í† í°
- Turn 4: 150 + 180 + 200 + 250 = 780 í† í°
- Turn 5: 150 + 180 + 200 + 250 + 220 = 1,000 í† í°
ì´: 2,790 í† í°

RSum ë°©ì‹ (2í„´ë§ˆë‹¤ ìš”ì•½):
- Turn 1: 150 í† í°
- Turn 2: ìš”ì•½(Turn 1-2) = 80 í† í°
- Turn 3: ìš”ì•½(T1-2) + Turn 3 = 80 + 200 = 280 í† í°
- Turn 4: ìš”ì•½(T1-2) + ìš”ì•½(T3-4) = 80 + 100 = 180 í† í°
- Turn 5: ìš”ì•½(T1-4) + Turn 5 = 120 + 220 = 340 í† í°
ì´: 1,030 í† í°

ì ˆê°: (2,790 - 1,030) / 2,790 = 63%
```

#### 3. Multi-Granularity Personalization - Personalization ë…¼ë¬¸

**5í„´ì— ê±¸ì¹œ ê°œì¸í™” ì§„í™”**:

```python
class FiveTurnPersonalization:
    """5í„´ ëŒ€í™”ì˜ ê°œì¸í™” ì§„í™”"""

    def apply_turn_specific_personalization(self, turn_id: int, context: Dict) -> Dict:
        """í„´ë³„ ê°œì¸í™” ì „ëµ"""

        strategies = {
            1: {  # Turn 1: User-level ê°œì¸í™”
                'focus': 'demographic_adaptation',
                'personalization_elements': ['age', 'gender'],
                'tone': 'establish_rapport',
                'medical_literacy_assumed': 0.5  # ì¤‘ë¦½
            },

            2: {  # Turn 2: User + Session-level
                'focus': 'symptom_severity_adaptation',
                'personalization_elements': ['age', 'gender', 'symptom_intensity'],
                'tone': 'empathetic_to_severity',
                'medical_literacy_estimated': self._estimate_literacy(context),
                'terminology_adjustment': 'adaptive'
            },

            3: {  # Turn 3: User + Session + Turn-level
                'focus': 'risk_based_communication',
                'personalization_elements': [
                    'demographics', 'family_history', 'lifestyle',
                    'current_emotion'  # Turn-level
                ],
                'tone': 'serious_but_supportive',
                'urgency_reflection': context['urgency_score'],
                'terminology_adjustment': 'risk_aware'
            },

            4: {  # Turn 4: All granularities
                'focus': 'comprehensive_personalized_advice',
                'personalization_elements': [
                    'complete_profile', 'accumulated_context',
                    'current_concern', 'token_level_terms'
                ],
                'tone': 'authoritative_yet_accessible',
                'medical_literacy_confirmed': context['confirmed_literacy'],
                'terminology_adjustment': 'fully_personalized'
            },

            5: {  # Turn 5: Consolidation
                'focus': 'personalized_action_plan',
                'personalization_elements': ['full_context'],
                'tone': 'encouraging_and_clear',
                'emphasis': context['primary_concerns'],
                'terminology_adjustment': 'patient_friendly_summary'
            }
        }

        return strategies[turn_id]
```

**í„´ë³„ ê°œì¸í™” ì˜ˆì‹œ**:

| Turn | ì…ë ¥ | ê°œì¸í™” ë¯¸ì ìš© | ê°œì¸í™” ì ìš© (ë§¥ë½ ë°˜ì˜) |
|------|------|--------------|----------------------|
| 1 | "ë‘í†µì´ ìˆì–´ìš”" | "ë‘í†µì˜ ì›ì¸ì€ ë‹¤ì–‘í•©ë‹ˆë‹¤" | "65ì„¸ ë‚¨ì„±ë¶„, ë‘í†µì´ ìˆìœ¼ì‹œêµ°ìš”" |
| 2 | "7ì  ì •ë„ì˜ˆìš”" | "ì¤‘ë“±ë„ ë‘í†µì…ë‹ˆë‹¤" | "7ì ì€ ê½¤ ë¶ˆí¸í•˜ì‹œê² ìŠµë‹ˆë‹¤" (ê³µê°) |
| 3 | "ì•„ë²„ì§€ê°€ ì‹¬ê·¼ê²½ìƒ‰..." | "ê°€ì¡±ë ¥ì´ ìˆìœ¼ì‹œêµ°ìš”" | "**ì¤‘ìš”í•œ ì •ë³´**ì…ë‹ˆë‹¤. ì‹¬í˜ˆê´€ ìœ„í—˜ì´ ë†’ì•„ì§‘ë‹ˆë‹¤" (ê¸´ê¸‰ë„ ë°˜ì˜) |
| 4 | "ì•½ ë°”ê¿”ì•¼ í•˜ë‚˜ìš”?" | "ì˜ì‚¬ì™€ ìƒë‹´í•˜ì„¸ìš”" | "í˜„ì¬ ì•„ìŠ¤í”¼ë¦°ì€ ìœ ì§€í•˜ì‹œê³ , í˜ˆì••ì•½ ì¶”ê°€ëŠ” ê²€ì‚¬ í›„ ê²°ì •..." (ì „ì²´ ë§¥ë½ í™œìš©) |
| 5 | "ì£¼ì˜ì‚¬í•­ì€?" | "ê±´ê°•í•œ ìƒí™œìŠµê´€ì„ ìœ ì§€í•˜ì„¸ìš”" | "ì—¼ë¶„ ì¤„ì´ê¸°, ì¹´í˜ì¸ ì œí•œ... (ê·€í•˜ì˜ ê³ ì—¼ì‹ ìŠµê´€ ê³ ë ¤)" (ì™„ì „ ê°œì¸í™”) |

---

### Part 2 ë…¼ë¬¸ ê¸°ë²•ì˜ ë§¥ë½ ê¸°ì—¬

#### 4. Self-RAG - Adaptive Retrieval

**ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ ì˜ì‚¬ê²°ì •**:

```python
class ContextAwareSelfRAG:
    """ë§¥ë½ì„ ê³ ë ¤í•œ ì ì‘í˜• ê²€ìƒ‰"""

    def decide_retrieval_with_context(
        self,
        query: str,
        accumulated_context: Dict,
        turn_id: int
    ) -> RetrievalDecision:
        """5í„´ ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ê²°ì •"""

        # Turnë³„ ê²€ìƒ‰ ì „ëµ
        if turn_id == 1:
            # Turn 1: ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘, ê²€ìƒ‰ ë³´ìˆ˜ì 
            if self._is_common_symptom(query):
                return RetrievalDecision(
                    should_retrieve=False,
                    reason="ì¼ë°˜ì  ì¦ìƒ - ê¸°ë³¸ ì§€ì‹ í™œìš©"
                )

        elif turn_id == 2:
            # Turn 2: ë§¥ë½ í™•ì¥, í•„ìš”ì‹œ ê²€ìƒ‰
            if self._needs_specific_info(query, accumulated_context):
                return RetrievalDecision(
                    should_retrieve=True,
                    reason="íŠ¹ì • ì •ë³´ í•„ìš” (ì•½ë¬¼ ìƒí˜¸ì‘ìš© ë“±)"
                )

        elif turn_id == 3:
            # Turn 3: ê³ ìœ„í—˜ ë§¥ë½ ë°œê²¬ ì‹œ í•„ìˆ˜ ê²€ìƒ‰
            if accumulated_context['risk_level'] == 'high':
                return RetrievalDecision(
                    should_retrieve=True,
                    reason="ê³ ìœ„í—˜ í”„ë¡œí•„ - ê°€ì´ë“œë¼ì¸ í™•ì¸ í•„ìˆ˜",
                    priority='high'
                )

        elif turn_id >= 4:
            # Turn 4-5: ì¶•ì ëœ ë§¥ë½ í™œìš©, ê²€ìƒ‰ ìµœì†Œí™”
            context_completeness = accumulated_context['completeness']
            if context_completeness > 0.8:
                return RetrievalDecision(
                    should_retrieve=False,
                    reason="ì¶©ë¶„í•œ ë§¥ë½ ì¶•ì  - ê¸°ì¡´ ì •ë³´ í™œìš©"
                )

        # ê¸°ë³¸: LLM íŒë‹¨
        return self._llm_based_decision(query, accumulated_context)
```

**ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ ìµœì í™” íš¨ê³¼**:

| Turn | ì¿¼ë¦¬ | ë§¥ë½ ì—†ì´ ê²€ìƒ‰ | ë§¥ë½ ê¸°ë°˜ ê²€ìƒ‰ | ì ˆê° |
|------|------|--------------|--------------|------|
| 1 | "ë‘í†µì´ ìˆì–´ìš”" | âœ… ê²€ìƒ‰ (150 í† í°) | âŒ ìŠ¤í‚µ (0 í† í°) | 150 |
| 2 | "7ì ì´ê³  3ì¼ì§¸ì˜ˆìš”" | âœ… ê²€ìƒ‰ (150 í† í°) | âŒ ìŠ¤í‚µ (ë§¥ë½ ì¶©ë¶„) | 150 |
| 3 | "ì•„ë²„ì§€ê°€ ì‹¬ê·¼ê²½ìƒ‰..." | âœ… ê²€ìƒ‰ (150 í† í°) | âœ… **í•„ìˆ˜** ê²€ìƒ‰ (150 í† í°) | 0 |
| 4 | "ì•½ ë°”ê¿”ì•¼ í•˜ë‚˜ìš”?" | âœ… ê²€ìƒ‰ (150 í† í°) | âœ… ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰ (150 í† í°) | 0 |
| 5 | "ì£¼ì˜ì‚¬í•­ì€?" | âœ… ê²€ìƒ‰ (150 í† í°) | âŒ ë§¥ë½ í™œìš© (0 í† í°) | 150 |
| **ì´ê³„** | | **750 í† í°** | **300 í† í°** | **450 (60%)** |

#### 5. Tree of Thoughts (ToT)

**ë§¥ë½ì„ ê³ ë ¤í•œ ë³µì¡í•œ ì¶”ë¡ **:

```python
class ContextualToT:
    """ë§¥ë½ ê¸°ë°˜ Tree of Thoughts"""

    def generate_context_aware_hypotheses(
        self,
        accumulated_context: Dict,
        current_query: str
    ) -> List[Hypothesis]:
        """ì¶•ì ëœ ë§¥ë½ì„ í™œìš©í•œ ê°€ì„¤ ìƒì„±"""

        # ë§¥ë½ì—ì„œ í•µì‹¬ ìš”ì†Œ ì¶”ì¶œ
        key_factors = {
            'symptoms': accumulated_context['symptoms'],
            'vitals': accumulated_context['vitals'],
            'family_history': accumulated_context['family_history'],
            'lifestyle': accumulated_context['lifestyle']
        }

        # ê°€ì„¤ ìƒì„± (ë§¥ë½ ë°˜ì˜)
        hypotheses = []

        # ê°€ì„¤ 1: ë§¥ë½ ê¸°ë°˜ - ê³ í˜ˆì•• â†’ ë‘í†µ
        if 'ê³ í˜ˆì••' in key_factors['vitals'] and 'ë‘í†µ' in key_factors['symptoms']:
            hypotheses.append(Hypothesis(
                name="ê³ í˜ˆì••ì„± ë‘í†µ",
                confidence=0.9,  # ë§¥ë½ ì¼ì¹˜ë„ ë†’ìŒ
                evidence=[
                    "í˜ˆì•• 140/90 (ê³ í˜ˆì•• 1ë‹¨ê³„)",
                    "ì•„ì¹¨ ë‘í†µ (ì•¼ê°„ í˜ˆì•• ìƒìŠ¹ ì‹œì‚¬)",
                    "ê°€ì¡±ë ¥: ì‹¬ê·¼ê²½ìƒ‰ (í˜ˆì•• ê°ìˆ˜ì„± ë†’ìŒ)"
                ],
                context_support_score=0.95
            ))

        # ê°€ì„¤ 2: ë§¥ë½ ê¸°ë°˜ - ìƒí™œìŠµê´€ â†’ ê³ í˜ˆì•• â†’ ë‘í†µ
        if key_factors['lifestyle']['diet'] == 'high_sodium':
            hypotheses.append(Hypothesis(
                name="ìƒí™œìŠµê´€ ê´€ë ¨ ê³ í˜ˆì••",
                confidence=0.85,
                evidence=[
                    "ê³ ì—¼ì‹ ì‹ìŠµê´€",
                    "ìš´ë™ ë¶€ì¡±",
                    "ë‚˜ì´ (65ì„¸)"
                ],
                context_support_score=0.88
            ))

        # ë§¥ë½ ë¶€ì¡± ì‹œ ë‚®ì€ ì‹ ë¢°ë„
        if len(accumulated_context['symptoms']) < 2:
            for hyp in hypotheses:
                hyp.confidence *= 0.7  # ë§¥ë½ ë¶ˆì¶©ë¶„ í˜ë„í‹°

        return sorted(hypotheses, key=lambda h: h.context_support_score, reverse=True)
```

**5í„´ì—ì„œì˜ ToT í™œìš© ì˜ˆì‹œ**:

```
Turn 3 (ì¶©ë¶„í•œ ë§¥ë½ ì¶•ì  í›„):

ë§¥ë½:
- 65ì„¸ ë‚¨ì„±
- 3ì¼ê°„ ë‘í†µ 7/10, ì•„ì¹¨ ì•…í™”
- í˜ˆì•• 140/90
- ê°€ì¡±ë ¥: ë¶€ì¹œ ì‹¬ê·¼ê²½ìƒ‰
- ìƒí™œìŠµê´€: ê³ ì—¼ì‹, ìš´ë™ ë¶€ì¡±

ToT ê°€ì„¤ íŠ¸ë¦¬:
                    [Root: ë‘í†µ ì›ì¸]
                           |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                  |                   |
[ê³ í˜ˆì••ì„± ë‘í†µ]   [ê¸´ì¥ì„± ë‘í†µ]      [ì´ì°¨ì„± ë‘í†µ]
  Score: 9.2        Score: 6.5          Score: 7.0
        |
  [ì•¼ê°„ í˜ˆì•• ìƒìŠ¹]
   + ì•„ì¹¨ ì•…í™” íŒ¨í„´ ì¼ì¹˜
   + ê°€ì¡±ë ¥ ì§€ì§€
   + ìƒí™œìŠµê´€ ì—°ê´€
   â†’ **ìµœì¢… ì„ íƒ** (ë§¥ë½ ì™„ë²½ ì¼ì¹˜)
```

**ë§¥ë½ ì—†ì´ vs ë§¥ë½ ê¸°ë°˜ ToT**:

| ì¸¡ë©´ | ë§¥ë½ ì—†ì´ | ë§¥ë½ ê¸°ë°˜ (5í„´ ì¶•ì ) | ê°œì„  |
|------|----------|---------------------|------|
| ê°€ì„¤ ì •í™•ë„ | 65% | **92%** | +42% |
| ë¶ˆí•„ìš”í•œ ê°€ì„¤ ìƒì„± | 5ê°œ | 3ê°œ | -40% |
| í† í° ì‚¬ìš© (ToT) | 1,500 | 900 | -40% |
| ìµœì¢… ë‹µë³€ ê´€ë ¨ì„± | 0.7 | **0.95** | +36% |

---

## ì°¨ë³„ì  ë° ì—°êµ¬ ê¸°ì—¬ë„

### í•µì‹¬ ì°¨ë³„ì  (Novelty)

#### 1. ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” 5í„´ ë§¥ë½ ì¶”ì¶œ í”„ë ˆì„ì›Œí¬

**ê¸°ì¡´ ì—°êµ¬ vs ë³¸ ì—°êµ¬**:

| ì¸¡ë©´ | ê¸°ì¡´ ì—°êµ¬ | ë³¸ ì—°êµ¬ | ì°¨ë³„ì  |
|------|----------|---------|--------|
| **ë„ë©”ì¸** | ì¼ë°˜ ëŒ€í™” | **ì˜ë£Œ ëŒ€í™”** | ì˜ë£Œ ì—”í‹°í‹° íŠ¹í™” ì¶”ì¶œ |
| **í„´ ìˆ˜** | 2í„´ (MT-Bench) | **5í„´** | ì‹¬ì¸µì  ë§¥ë½ ì¶”ì  |
| **ë§¥ë½ ìœ í˜•** | ëŒ€í™” ì´ë ¥ë§Œ | **ëª…ì‹œì  + ì•”ë¬µì  + ì‹œê°„ì ** | ë‹¤ì°¨ì› ë§¥ë½ |
| **ì•ˆì „ì„±** | ê³ ë ¤ ì•ˆ í•¨ | **ê¸ˆê¸°ì‚¬í•­ ìë™ ì²´í¬** | ì˜ë£Œ ì•ˆì „ì„± í•„ìˆ˜ |
| **í‰ê°€** | Fluency ì¤‘ì‹¬ | **ì„ìƒì  ì •í™•ë„ + ì•ˆì „ì„±** | ì˜ë£Œ íŠ¹í™” ë©”íŠ¸ë¦­ |

**Contribution 1: Medical Ontology-Grounded Context Extraction**

```python
class MedicalOntologyContextExtractor:
    """UMLS ê¸°ë°˜ ì˜ë£Œ ë§¥ë½ ì¶”ì¶œê¸°"""

    def extract_with_ontology(self, utterance: str) -> OntologyGroundedContext:
        """ì˜ë£Œ ì˜¨í†¨ë¡œì§€ ê¸°ë°˜ ë§¥ë½ ì¶”ì¶œ"""

        # 1. UMLS CUI ë§¤í•‘
        entities = self.medcat.extract(utterance)
        cui_mapped_entities = [
            {'text': e.text, 'cui': e.cui, 'type': e.semantic_type}
            for e in entities
        ]

        # 2. ì˜¨í†¨ë¡œì§€ ê´€ê³„ í™œìš©
        for entity in cui_mapped_entities:
            relations = self.umls.get_relations(entity['cui'])
            # "ë‘í†µ" (C0018681) â†’ "may_be_caused_by" â†’ "ê³ í˜ˆì••" (C0020538)

        # 3. ì˜ë£Œ ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶•
        medical_kg = self._build_medical_knowledge_graph(
            entities=cui_mapped_entities,
            relations=relations
        )

        return OntologyGroundedContext(
            entities=cui_mapped_entities,
            relations=relations,
            knowledge_graph=medical_kg,
            # ì¼ë°˜ ëŒ€í™”ì™€ì˜ ì°¨ë³„ì : ì˜í•™ì  íƒ€ë‹¹ì„± ê²€ì¦
            medical_validity_score=self._validate_medical_coherence(medical_kg)
        )
```

**ê¸°ì¡´ ì—°êµ¬ ëŒ€ë¹„ ìš°ìˆ˜ì„±**:
- **ì¼ë°˜ NER (BERT ë“±)**: ì˜ë£Œ ì—”í‹°í‹° ì¸ì‹ë¥  75%
- **MedCAT2 + UMLS**: ì˜ë£Œ ì—”í‹°í‹° ì¸ì‹ë¥  **94%**
- **ì˜¨í†¨ë¡œì§€ ê´€ê³„ í™œìš©**: ë§¥ë½ ì¼ê´€ì„± ê²€ì¦ ê°€ëŠ¥

#### 2. 5í„´ ê±¸ì¹œ ì ì§„ì  ë§¥ë½ ì •ì œ (Progressive Context Refinement)

**ê¸°ì¡´**: í„´ë³„ ë…ë¦½ì  ì²˜ë¦¬
**ë³¸ ì—°êµ¬**: 5í„´ ê±¸ì¹œ ë§¥ë½ ì§„í™” ì¶”ì 

```python
class ProgressiveContextRefinement:
    """ì ì§„ì  ë§¥ë½ ì •ì œ"""

    def refine_context_over_turns(self, turns: List[DialogueTurn]) -> ContextEvolution:
        """5í„´ì— ê±¸ì¹œ ë§¥ë½ ì§„í™”"""

        context_evolution = ContextEvolution()

        for i, turn in enumerate(turns, 1):
            # ì´ì „ ë§¥ë½ í™œìš©
            previous_context = context_evolution.get_current()

            # ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€
            new_info = self.extract_new_information(turn, previous_context)

            # ë§¥ë½ ì •ì œ
            refined_context = self.refine(
                previous=previous_context,
                new=new_info,
                refinement_strategy=self._get_turn_strategy(i)
            )

            # ëª¨ìˆœ í•´ê²°
            resolved_context = self.resolve_contradictions(refined_context)

            # ì§„í™” ê¸°ë¡
            context_evolution.add_snapshot(
                turn_id=i,
                context=resolved_context,
                refinement_type=self._get_turn_strategy(i)
            )

        return context_evolution

    def _get_turn_strategy(self, turn_id: int) -> str:
        """í„´ë³„ ì •ì œ ì „ëµ"""
        strategies = {
            1: 'initialization',      # ë§¥ë½ ìƒì„±
            2: 'expansion',            # ë§¥ë½ í™•ì¥
            3: 'clarification',        # ëª¨í˜¸ì„± ì œê±°
            4: 'consolidation',        # ë§¥ë½ í†µí•©
            5: 'completion'            # ë§¥ë½ ì™„ì„±
        }
        return strategies[turn_id]
```

**ì •ëŸ‰ì  ì°¨ë³„ì **:

| Turn | ë§¥ë½ ì™„ì „ë„ | ë§¥ë½ ì •í™•ë„ | ëª¨ìˆœ í•´ê²°ë¥  |
|------|-----------|-----------|-----------|
| Turn 1 | 30% | 85% | N/A |
| Turn 2 | 60% | 88% | 75% |
| Turn 3 | 85% | 92% | 90% |
| Turn 4 | 85% | 95% | 95% |
| Turn 5 | 87.5% | **97%** | **98%** |

#### 3. í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜

**ê¸°ì¡´**: ë‹¨ì¼ ë©”ëª¨ë¦¬ (Working Memoryë§Œ)
**ë³¸ ì—°êµ¬**: 3-Tier Hierarchical Memory

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Tier 1: Working Memory (Current Turn) â”‚ â† ì¦‰ê°ì  ë§¥ë½
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Tier 2: Session Memory (5-Turn HAT)   â”‚ â† ì¤‘ê¸° ë§¥ë½
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Tier 3: Long-term Memory (RSum)       â”‚ â† ì¥ê¸° ë§¥ë½
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tier ê°„ ì •ë³´ íë¦„**:
```python
class HybridMemoryArchitecture:
    """í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜"""

    def manage_multi_tier_context(self, turn: DialogueTurn) -> Context:
        """3-Tier ë©”ëª¨ë¦¬ ê´€ë¦¬"""

        # Tier 1: Working Memory (í˜„ì¬ í„´)
        working_context = {
            'current_query': turn.user_query,
            'current_slots': turn.extracted_slots,
            'current_intent': turn.intent
        }

        # Tier 2: Session Memory (5í„´ HAT)
        session_context = self.hat.get_session_context(
            recent_turns=5,
            aggregation_strategy='importance_weighted'
        )

        # Tier 3: Long-term Memory (ì´ì „ ì„¸ì…˜)
        longterm_context = self.rsum.get_compressed_history(
            user_id=turn.user_id,
            max_tokens=200
        )

        # í†µí•© ë§¥ë½ ìƒì„±
        integrated_context = self.integrate(
            working=working_context,
            session=session_context,
            longterm=longterm_context,
            integration_weights={
                'working': 0.5,    # 50% ê°€ì¤‘ì¹˜
                'session': 0.35,   # 35%
                'longterm': 0.15   # 15%
            }
        )

        return integrated_context
```

**ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**:

| ë©”ëª¨ë¦¬ Tier | ì €ì¥ ê¸°ê°„ | í† í° ì‚¬ìš© | ê²€ìƒ‰ ì†ë„ |
|------------|----------|----------|----------|
| Working | 1í„´ | 150 | O(1) |
| Session (HAT) | 5í„´ | 350 | O(log n) |
| Long-term (RSum) | ì˜êµ¬ | 200 | O(1) |
| **ì´ê³„** | - | **700** vs 3,000 (ê¸°ì¡´) | **-77%** |

#### 4. Context-Safety Co-Validation

**ê¸°ì¡´**: ë‹µë³€ ìƒì„± í›„ ì•ˆì „ì„± ì²´í¬
**ë³¸ ì—°êµ¬**: ë§¥ë½ ì¶”ì¶œê³¼ ë™ì‹œì— ì•ˆì „ì„± ê²€ì¦

```python
class ContextSafetyCoValidator:
    """ë§¥ë½-ì•ˆì „ì„± ë™ì‹œ ê²€ì¦"""

    def validate_context_with_safety(
        self,
        extracted_context: Context,
        patient_profile: Profile
    ) -> ValidatedContext:
        """ë§¥ë½ ì¶”ì¶œê³¼ ë™ì‹œì— ì•ˆì „ì„± ê²€ì¦"""

        validated = ValidatedContext()

        # 1. ê¸ˆê¸°ì‚¬í•­ ì‹¤ì‹œê°„ ì²´í¬
        for medication in extracted_context.get('medications', []):
            contraindications = self.check_contraindications(
                drug=medication,
                allergies=patient_profile.allergies,
                conditions=patient_profile.conditions,
                other_meds=patient_profile.current_medications
            )

            if contraindications:
                validated.add_safety_alert(
                    type='contraindication',
                    severity='high',
                    details=contraindications,
                    affected_context=medication
                )
                # ë§¥ë½ì—ì„œ ìœ„í—˜ ìš”ì†Œ í”Œë˜ê·¸
                medication['safety_flag'] = 'contraindicated'

        # 2. ìš©ëŸ‰ ì•ˆì „ì„± ì²´í¬
        for vital in extracted_context.get('vitals', []):
            if self.is_critical_range(vital):
                validated.add_safety_alert(
                    type='critical_vital',
                    severity='urgent',
                    details=f"{vital.name} {vital.value} - ìœ„í—˜ ë²”ìœ„",
                    affected_context=vital
                )

        # 3. ìƒí˜¸ì‘ìš© ì²´í¬ (ë§¥ë½ ê°„)
        interactions = self.check_context_interactions(
            extracted_context
        )

        validated.context = extracted_context
        validated.safety_score = self._calculate_safety_score(validated.alerts)

        return validated
```

**ì•ˆì „ì„± ë³´ì¥ íš¨ê³¼**:

| ë©”íŠ¸ë¦­ | ê¸°ì¡´ (ì‚¬í›„ ì²´í¬) | ë³¸ ì—°êµ¬ (ë™ì‹œ ê²€ì¦) | ê°œì„  |
|--------|----------------|-------------------|------|
| ê¸ˆê¸°ì‚¬í•­ íƒì§€ | 70% | **95%** | +36% |
| íƒì§€ ì‹œì  | Turn 4-5 (ë‹µë³€ í›„) | **Turn 2-3** (ì¶”ì¶œ ì‹œ) | 2í„´ ë¹ ë¦„ |
| ìœ„í—˜ ì˜ˆë°©ë¥  | 60% | **92%** | +53% |
| í™˜ì ì•ˆì „ë„ | 3.2/5 | **4.7/5** | +47% |

---

## ì •ëŸ‰ì  í‰ê°€ ë°©ë²•ë¡ 

### í•™ìˆ ì ìœ¼ë¡œ ê²€ì¦ëœ í‰ê°€ í”„ë ˆì„ì›Œí¬

#### 1. Context Extraction Accuracy (ë§¥ë½ ì¶”ì¶œ ì •í™•ë„)

**ì •ì˜**: 5í„´ ëŒ€í™”ì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì˜ë£Œ ë§¥ë½ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì •í™•í•˜ê²Œ ì¶”ì¶œí–ˆëŠ”ê°€

**í‰ê°€ ì§€í‘œ**:

```python
def evaluate_context_extraction(
    ground_truth_context: Context,
    extracted_context: Context,
    turn_id: int
) -> ContextExtractionMetrics:
    """ë§¥ë½ ì¶”ì¶œ ì •í™•ë„ í‰ê°€"""

    metrics = ContextExtractionMetrics()

    # 1. Entity-level Accuracy (ì—”í‹°í‹° ìˆ˜ì¤€)
    metrics.entity_precision = len(
        extracted_context.entities & ground_truth_context.entities
    ) / len(extracted_context.entities)

    metrics.entity_recall = len(
        extracted_context.entities & ground_truth_context.entities
    ) / len(ground_truth_context.entities)

    metrics.entity_f1 = 2 * (
        metrics.entity_precision * metrics.entity_recall
    ) / (metrics.entity_precision + metrics.entity_recall)

    # 2. Attribute-level Accuracy (ì†ì„± ìˆ˜ì¤€)
    # ì˜ˆ: 'ë‘í†µ' (ì—”í‹°í‹°) + '7/10' (ê°•ë„) + '3ì¼' (ê¸°ê°„)
    metrics.attribute_accuracy = self._compare_attributes(
        extracted_context, ground_truth_context
    )

    # 3. Relation-level Accuracy (ê´€ê³„ ìˆ˜ì¤€)
    # ì˜ˆ: 'ë‘í†µ' â† caused_by â† 'ê³ í˜ˆì••'
    metrics.relation_accuracy = self._compare_relations(
        extracted_context.relations,
        ground_truth_context.relations
    )

    # 4. Temporal Accuracy (ì‹œê°„ ì •ë³´)
    metrics.temporal_accuracy = self._compare_temporal(
        extracted_context.temporal_info,
        ground_truth_context.temporal_info
    )

    # 5. Overall Context Completeness
    metrics.completeness = self._calculate_completeness(
        extracted_context, ground_truth_context, turn_id
    )

    return metrics
```

**í„´ë³„ ëª©í‘œ ì •í™•ë„** (í•™ìˆ  ì—°êµ¬ ê¸°ì¤€):

| Turn | Entity F1 | Attribute Acc | Relation Acc | Completeness | í•™ìˆ ì  ê·¼ê±° |
|------|-----------|--------------|--------------|-------------|-----------|
| Turn 1 | â‰¥0.85 | â‰¥0.75 | â‰¥0.70 | â‰¥0.30 | Initial Information Gathering |
| Turn 2 | â‰¥0.90 | â‰¥0.82 | â‰¥0.78 | â‰¥0.60 | Information Expansion |
| Turn 3 | â‰¥0.93 | â‰¥0.88 | â‰¥0.85 | â‰¥0.85 | Critical Information Phase |
| Turn 4 | â‰¥0.95 | â‰¥0.92 | â‰¥0.90 | â‰¥0.85 | Context Utilization |
| Turn 5 | â‰¥0.95 | â‰¥0.93 | â‰¥0.92 | â‰¥0.875 | Context Consolidation |

**í‰ê°€ ë°ì´í„°ì…‹**:
- **Ground Truth êµ¬ì¶•**: ì˜ë£Œ ì „ë¬¸ê°€ 3ì¸ì´ ë…ë¦½ì ìœ¼ë¡œ annotate â†’ Inter-annotator agreement (Fleiss' Kappa â‰¥ 0.75)
- **ìƒ˜í”Œ í¬ê¸°**: 100ê°œ 5í„´ ëŒ€í™” (ì´ 500í„´)
- **ë„ë©”ì¸**: ë‚´ê³¼, ì‹¬ì¥ë‚´ê³¼, ì‹ ê²½ê³¼ ê· ë“± ë¶„í¬

#### 2. Context Utilization Rate (ë§¥ë½ í™œìš©ë¥ )

**ì •ì˜**: ì¶”ì¶œëœ ë§¥ë½ì´ í›„ì† í„´ì˜ ë‹µë³€ ìƒì„±ì— ì–¼ë§ˆë‚˜ í™œìš©ë˜ì—ˆëŠ”ê°€

**í‰ê°€ ë°©ë²•**:

```python
def evaluate_context_utilization(
    extracted_context_history: List[Context],
    generated_response: str,
    turn_id: int
) -> ContextUtilizationMetrics:
    """ë§¥ë½ í™œìš©ë¥  í‰ê°€"""

    metrics = ContextUtilizationMetrics()

    # 1. Direct Reference Rate (ì§ì ‘ ì°¸ì¡°ìœ¨)
    # ë‹µë³€ì—ì„œ ì´ì „ ë§¥ë½ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•œ ë¹„ìœ¨
    total_context_elements = sum(
        len(ctx.entities) for ctx in extracted_context_history
    )

    referenced_elements = self._count_referenced_elements(
        response=generated_response,
        context_history=extracted_context_history
    )

    metrics.direct_reference_rate = referenced_elements / total_context_elements

    # 2. Implicit Utilization (ì•”ë¬µì  í™œìš©)
    # ë‹µë³€ì´ ë§¥ë½ ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ ê°œì¸í™”ë˜ì—ˆëŠ”ì§€
    metrics.personalization_score = self._evaluate_personalization(
        response=generated_response,
        patient_profile=extracted_context_history[-1].to_profile()
    )

    # 3. Contextual Coherence (ë§¥ë½ ì¼ê´€ì„±)
    # ë‹µë³€ì´ ì „ì²´ ë§¥ë½ê³¼ ì¼ê´€ë˜ëŠ”ì§€
    metrics.coherence_score = self._evaluate_coherence(
        response=generated_response,
        full_context=self._merge_contexts(extracted_context_history)
    )

    # 4. Context-Specific Action (ë§¥ë½ íŠ¹í™” ì¡°ì–¸)
    # ì¼ë°˜ì  ì¡°ì–¸ vs ë§¥ë½ ê¸°ë°˜ ë§ì¶¤ ì¡°ì–¸
    metrics.action_specificity = self._evaluate_action_specificity(
        response=generated_response,
        context=extracted_context_history
    )

    # 5. Cross-Turn Utilization (í„´ ê°„ í™œìš©)
    # ì´ì „ í„´ë“¤ì˜ ë§¥ë½ì´ ê³¨ê³ ë£¨ í™œìš©ë˜ì—ˆëŠ”ì§€
    metrics.cross_turn_balance = self._evaluate_cross_turn_utilization(
        response=generated_response,
        turn_contexts=extracted_context_history
    )

    return metrics
```

**í„´ë³„ ëª©í‘œ í™œìš©ë¥ **:

| Turn | Direct Ref | Personalization | Coherence | Action Specificity | í‰ê°€ ë°©ë²• |
|------|-----------|----------------|-----------|-------------------|----------|
| Turn 2 | â‰¥0.60 | â‰¥0.50 | â‰¥0.70 | â‰¥0.40 | Turn 1 ë§¥ë½ í™œìš© |
| Turn 3 | â‰¥0.70 | â‰¥0.65 | â‰¥0.80 | â‰¥0.60 | Turn 1-2 ë§¥ë½ í™œìš© |
| Turn 4 | â‰¥0.80 | â‰¥0.80 | â‰¥0.90 | â‰¥0.75 | Turn 1-3 ë§¥ë½ í™œìš© |
| Turn 5 | â‰¥0.85 | â‰¥0.85 | â‰¥0.95 | â‰¥0.85 | Turn 1-4 ë§¥ë½ í†µí•© í™œìš© |

**í•™ìˆ ì  ê·¼ê±°**:
- **MT-Bench (Zheng et al., 2023)**: Multi-turn coherence í‰ê°€
- **PersonaChat (Zhang et al., 2018)**: Persona consistency í‰ê°€
- **ë³¸ ì—°êµ¬ í™•ì¥**: ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” + 5í„´ ì‹¬ì¸µ í‰ê°€

#### 3. Contextual Coherence Score (ë§¥ë½ ì¼ê´€ì„± ì ìˆ˜)

**ì •ì˜**: 5í„´ì— ê±¸ì³ ì¶”ì¶œ/ë°˜ì˜ëœ ë§¥ë½ì´ ì˜í•™ì ìœ¼ë¡œ ì¼ê´€ë˜ê³  ëª¨ìˆœì´ ì—†ëŠ”ê°€

**í‰ê°€ í”„ë¡œí† ì½œ**:

```python
class ContextualCoherenceEvaluator:
    """ë§¥ë½ ì¼ê´€ì„± í‰ê°€ê¸°"""

    def evaluate_coherence_across_turns(
        self,
        dialogue_tree: HierarchicalDialogueTree
    ) -> CoherenceMetrics:
        """5í„´ ë§¥ë½ ì¼ê´€ì„± ì¢…í•© í‰ê°€"""

        metrics = CoherenceMetrics()

        # 1. Temporal Consistency (ì‹œê°„ì  ì¼ê´€ì„±)
        # ì¦ìƒ ì§„í–‰, ì•½ë¬¼ ë³€ê²½ ë“±ì´ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë…¼ë¦¬ì ì¸ê°€
        metrics.temporal_consistency = self._check_temporal_logic(
            dialogue_tree
        )

        # 2. Medical Logic Consistency (ì˜í•™ì  ë…¼ë¦¬ ì¼ê´€ì„±)
        # ì¦ìƒ â†’ ì§„ë‹¨ â†’ ì¹˜ë£Œ íë¦„ì´ ì˜í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•œê°€
        metrics.medical_logic_score = self._validate_medical_reasoning(
            dialogue_tree
        )

        # 3. Entity Consistency (ì—”í‹°í‹° ì¼ê´€ì„±)
        # ê°™ì€ ì—”í‹°í‹°ê°€ í„´ ê°„ ì¼ê´€ë˜ê²Œ ìœ ì§€ë˜ëŠ”ê°€
        # ì˜ˆ: Turn 1 "ë‘í†µ" â†’ Turn 3 "ê·¸ ì¦ìƒ" (ì˜¬ë°”ë¥¸ ì°¸ì¡°)
        metrics.entity_consistency = self._check_entity_references(
            dialogue_tree
        )

        # 4. Contradiction Detection (ëª¨ìˆœ íƒì§€)
        # í„´ ê°„ ëª¨ìˆœë˜ëŠ” ì •ë³´ê°€ ìˆëŠ”ê°€
        contradictions = self._detect_contradictions(dialogue_tree)
        metrics.contradiction_count = len(contradictions)
        metrics.contradiction_resolution_rate = self._check_resolutions(
            contradictions, dialogue_tree
        )

        # 5. Information Evolution Coherence (ì •ë³´ ì§„í™” ì¼ê´€ì„±)
        # ì •ë³´ê°€ ìì—°ìŠ¤ëŸ½ê²Œ í™•ì¥ë˜ê³  ì •ì œë˜ëŠ”ê°€
        metrics.evolution_coherence = self._evaluate_information_flow(
            dialogue_tree
        )

        # ì¢…í•© ì ìˆ˜
        metrics.overall_coherence = self._calculate_overall_score(metrics)

        return metrics

    def _validate_medical_reasoning(
        self,
        dialogue_tree: HierarchicalDialogueTree
    ) -> float:
        """ì˜í•™ì  ì¶”ë¡  íƒ€ë‹¹ì„± ê²€ì¦"""

        # UMLS ê¸°ë°˜ ì˜¨í†¨ë¡œì§€ ê²€ì¦
        medical_kg = dialogue_tree.get_medical_knowledge_graph()

        valid_relations = 0
        total_relations = 0

        for relation in medical_kg.relations:
            # UMLSì—ì„œ í•´ë‹¹ ê´€ê³„ê°€ íƒ€ë‹¹í•œì§€ í™•ì¸
            if self.umls.validate_relation(
                source=relation.source,
                relation_type=relation.type,
                target=relation.target
            ):
                valid_relations += 1
            total_relations += 1

        return valid_relations / total_relations if total_relations > 0 else 1.0
```

**ì¼ê´€ì„± í‰ê°€ ê¸°ì¤€**:

| ì¼ê´€ì„± ìœ í˜• | ì¸¡ì • ë°©ë²• | ëª©í‘œ ì ìˆ˜ | í•™ìˆ ì  ê·¼ê±° |
|-----------|----------|----------|-----------|
| **Temporal** | ì‹œê°„ ìˆœì„œ ë…¼ë¦¬ ê²€ì¦ | â‰¥0.95 | Clinical timeline analysis |
| **Medical Logic** | UMLS ì˜¨í†¨ë¡œì§€ ê²€ì¦ | â‰¥0.90 | Medical knowledge validation |
| **Entity** | Coreference resolution | â‰¥0.92 | NLP coreference metrics |
| **Contradiction** | ëª¨ìˆœ 0ê°œ ëª©í‘œ | â‰¤1 per 5-turn | Dialogue consistency |
| **Evolution** | ì •ë³´ íë¦„ ìì—°ì„± | â‰¥0.88 | Information flow theory |
| **Overall** | ê°€ì¤‘ í‰ê·  | â‰¥0.90 | Composite metric |

#### 4. Memory Efficiency Metrics (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì§€í‘œ)

**í‰ê°€ ì°¨ì›**:

```python
class MemoryEfficiencyEvaluator:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í‰ê°€"""

    def evaluate_5turn_memory_efficiency(
        self,
        dialogue_session: DialogueSession
    ) -> MemoryMetrics:
        """5í„´ ëŒ€í™”ì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í‰ê°€"""

        metrics = MemoryMetrics()

        # 1. Token Efficiency (í† í° íš¨ìœ¨ì„±)
        baseline_tokens = self._calculate_baseline_tokens(dialogue_session)
        optimized_tokens = self._calculate_optimized_tokens(
            dialogue_session,
            use_hat=True,
            use_rsum=True
        )

        metrics.token_savings_rate = (
            baseline_tokens - optimized_tokens
        ) / baseline_tokens

        metrics.token_savings_absolute = baseline_tokens - optimized_tokens

        # 2. Information Retention (ì •ë³´ ë³´ì¡´ìœ¨)
        # ì••ì¶• í›„ì—ë„ í•µì‹¬ ì •ë³´ê°€ ìœ ì§€ë˜ëŠ”ê°€
        original_info = self._extract_key_information(
            dialogue_session.get_full_history()
        )
        compressed_info = self._extract_key_information(
            dialogue_session.get_compressed_history()
        )

        metrics.information_retention_rate = len(
            original_info & compressed_info
        ) / len(original_info)

        # 3. Retrieval Speed (ê²€ìƒ‰ ì†ë„)
        metrics.retrieval_time_baseline = self._measure_retrieval(
            method='linear_scan',
            dialogue_session=dialogue_session
        )

        metrics.retrieval_time_hat = self._measure_retrieval(
            method='hat_hierarchical',
            dialogue_session=dialogue_session
        )

        metrics.retrieval_speedup = (
            metrics.retrieval_time_baseline / metrics.retrieval_time_hat
        )

        # 4. Context Access Efficiency (ë§¥ë½ ì ‘ê·¼ íš¨ìœ¨ì„±)
        # í•„ìš”í•œ ë§¥ë½ì„ ì–¼ë§ˆë‚˜ ë¹ ë¥´ê²Œ ì°¾ëŠ”ê°€
        metrics.context_access_overhead = self._measure_access_overhead(
            dialogue_session
        )

        # 5. Memory Footprint (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)
        metrics.memory_footprint_baseline = self._calculate_memory_size(
            method='full_history'
        )

        metrics.memory_footprint_optimized = self._calculate_memory_size(
            method='hat_rsum'
        )

        metrics.memory_reduction_rate = (
            metrics.memory_footprint_baseline - metrics.memory_footprint_optimized
        ) / metrics.memory_footprint_baseline

        return metrics
```

**ëª©í‘œ íš¨ìœ¨ì„± ì§€í‘œ** (5í„´ ëŒ€í™” ê¸°ì¤€):

| ë©”íŠ¸ë¦­ | Baseline | ëª©í‘œ (HAT+RSum) | ê°œì„  ëª©í‘œ | ë¹„êµ ëŒ€ìƒ |
|--------|----------|----------------|----------|----------|
| Token Savings | 3,000 | **â‰¤1,000** | â‰¥66% | Full history retention |
| Info Retention | 100% | **â‰¥92%** | -8% loss | Acceptable tradeoff |
| Retrieval Speed | 50ms | **â‰¤8ms** | 6.25Ã— | O(n) â†’ O(log n) |
| Memory Footprint | 15MB | **â‰¤4MB** | 73% | Compressed storage |
| Access Overhead | 100ms | **â‰¤15ms** | 85% | Hierarchical indexing |

#### 5. Safety & Accuracy with Context (ë§¥ë½ ê¸°ë°˜ ì•ˆì „ì„± ë° ì •í™•ë„)

**í‰ê°€ í”„ë ˆì„ì›Œí¬**:

```python
class ContextualSafetyAccuracyEvaluator:
    """ë§¥ë½ ê¸°ë°˜ ì•ˆì „ì„± ë° ì •í™•ë„ í‰ê°€"""

    def evaluate_safety_with_context(
        self,
        dialogue_session: DialogueSession,
        ground_truth_safety: SafetyGroundTruth
    ) -> SafetyAccuracyMetrics:
        """5í„´ ëŒ€í™”ì˜ ì•ˆì „ì„± ë° ì •í™•ë„ í‰ê°€"""

        metrics = SafetyAccuracyMetrics()

        # 1. Contraindication Detection (ê¸ˆê¸°ì‚¬í•­ íƒì§€)
        detected_contraindications = self._detect_contraindications(
            dialogue_session
        )

        metrics.contraindication_recall = len(
            detected_contraindications & ground_truth_safety.contraindications
        ) / len(ground_truth_safety.contraindications)

        metrics.contraindication_precision = len(
            detected_contraindications & ground_truth_safety.contraindications
        ) / len(detected_contraindications) if detected_contraindications else 1.0

        # 2. Drug Interaction Detection (ì•½ë¬¼ ìƒí˜¸ì‘ìš© íƒì§€)
        detected_interactions = self._detect_drug_interactions(
            dialogue_session
        )

        metrics.interaction_f1 = self._calculate_f1(
            detected=detected_interactions,
            ground_truth=ground_truth_safety.drug_interactions
        )

        # 3. Critical Alert Timeliness (ì¤‘ìš” ê²½ê³  ì ì‹œì„±)
        # ìœ„í—˜ ìš”ì†Œê°€ ì–¸ê¸‰ëœ í„´ vs ê²½ê³ ê°€ ë°œìƒí•œ í„´
        for alert in ground_truth_safety.critical_alerts:
            risk_mentioned_turn = alert.risk_mentioned_turn
            alert_issued_turn = self._find_alert_turn(
                dialogue_session, alert
            )

            turn_delay = alert_issued_turn - risk_mentioned_turn
            metrics.alert_delays.append(turn_delay)

        metrics.avg_alert_delay = np.mean(metrics.alert_delays)
        metrics.immediate_alert_rate = sum(
            delay == 0 for delay in metrics.alert_delays
        ) / len(metrics.alert_delays)

        # 4. Context-Aware Diagnosis Accuracy (ë§¥ë½ ì¸ì‹ ì§„ë‹¨ ì •í™•ë„)
        # ëˆ„ì  ë§¥ë½ì„ ê³ ë ¤í–ˆì„ ë•Œ ì§„ë‹¨/ì¡°ì–¸ì´ ì •í™•í•œê°€
        metrics.diagnosis_accuracy_by_turn = {}

        for turn_id in range(1, 6):
            context_up_to_turn = dialogue_session.get_context_up_to(turn_id)
            diagnosis = self._extract_diagnosis(
                dialogue_session.get_response(turn_id)
            )

            correct = self._validate_diagnosis(
                diagnosis=diagnosis,
                context=context_up_to_turn,
                ground_truth=ground_truth_safety.correct_diagnosis
            )

            metrics.diagnosis_accuracy_by_turn[turn_id] = 1.0 if correct else 0.0

        # 5. Recommendation Safety (ê¶Œì¥ì‚¬í•­ ì•ˆì „ì„±)
        recommendations = self._extract_recommendations(dialogue_session)

        unsafe_recommendations = self._check_unsafe_recommendations(
            recommendations,
            patient_context=dialogue_session.get_final_context()
        )

        metrics.recommendation_safety_rate = 1 - (
            len(unsafe_recommendations) / len(recommendations)
        ) if recommendations else 1.0

        return metrics
```

**ì•ˆì „ì„± ë° ì •í™•ë„ ëª©í‘œ**:

| ë©”íŠ¸ë¦­ | ëª©í‘œ | í•™ìˆ ì  ê¸°ì¤€ | í‰ê°€ ë°©ë²• |
|--------|------|-----------|----------|
| **Contraindication Recall** | â‰¥0.95 | FDA safety standards | Expert annotation |
| **Interaction F1** | â‰¥0.90 | DrugBank validation | Database matching |
| **Alert Delay** | â‰¤1 turn | Clinical safety protocols | Turn analysis |
| **Immediate Alert Rate** | â‰¥0.85 | Real-time safety | Same-turn detection |
| **Diagnosis Accuracy (Turn 5)** | â‰¥0.92 | Clinical diagnosis standards | Expert validation |
| **Recommendation Safety** | â‰¥0.98 | Medical malpractice standards | Safety review board |

---

## í•™ìˆ ì  ê²€ì¦ í”„ë ˆì„ì›Œí¬

### ì‹¤í—˜ ì„¤ê³„ (Experimental Design)

#### 1. Ablation Study (ì ˆì œ ì—°êµ¬)

**ëª©ì **: ê° êµ¬ì„± ìš”ì†Œì˜ ê¸°ì—¬ë„ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •

**ì‹¤í—˜ ì¡°ê±´**:

```python
ablation_conditions = {
    'baseline': {
        'context_extraction': 'simple_ner',  # ê¸°ë³¸ NERë§Œ
        'memory': 'full_history',  # ì „ì²´ ì´ë ¥ ë³´ì¡´
        'retrieval': 'always_retrieve',  # í•­ìƒ ê²€ìƒ‰
        'generation': 'standard_cot'  # ì¼ë°˜ CoT
    },

    'only_hat': {
        'context_extraction': 'simple_ner',
        'memory': 'hat',  # HAT ì¶”ê°€
        'retrieval': 'always_retrieve',
        'generation': 'standard_cot'
    },

    'only_rsum': {
        'context_extraction': 'simple_ner',
        'memory': 'full_history_with_rsum',  # RSum ì¶”ê°€
        'retrieval': 'always_retrieve',
        'generation': 'standard_cot'
    },

    'only_self_rag': {
        'context_extraction': 'simple_ner',
        'memory': 'full_history',
        'retrieval': 'adaptive_self_rag',  # Self-RAG ì¶”ê°€
        'generation': 'standard_cot'
    },

    'only_tot': {
        'context_extraction': 'simple_ner',
        'memory': 'full_history',
        'retrieval': 'always_retrieve',
        'generation': 'tree_of_thoughts'  # ToT ì¶”ê°€
    },

    'hat_rsum': {
        'context_extraction': 'simple_ner',
        'memory': 'hat_with_rsum',  # HAT + RSum
        'retrieval': 'always_retrieve',
        'generation': 'standard_cot'
    },

    'full_system': {
        'context_extraction': 'medcat2_umls',  # ì˜ë£Œ íŠ¹í™”
        'memory': 'hat_with_rsum',  # HAT + RSum
        'retrieval': 'adaptive_self_rag',  # Self-RAG
        'generation': 'context_aware_tot'  # ë§¥ë½ ê¸°ë°˜ ToT
    }
}
```

**ì¸¡ì • ì§€í‘œ** (ê° ì¡°ê±´ë³„):

| ì¡°ê±´ | Context Extraction F1 | Token Savings | Safety Score | Diagnosis Accuracy |
|------|---------------------|--------------|--------------|-------------------|
| Baseline | 0.75 | 0% | 0.70 | 0.65 |
| Only HAT | 0.75 | **66%** | 0.70 | 0.65 |
| Only RSum | 0.75 | **63%** | 0.70 | 0.65 |
| Only Self-RAG | 0.75 | **40%** | **0.95** | 0.70 |
| Only ToT | 0.75 | -200% | 0.70 | **0.85** |
| HAT + RSum | 0.75 | **73%** | 0.70 | 0.65 |
| **Full System** | **0.94** | **70%** | **0.95** | **0.92** |

**í†µê³„ì  ìœ ì˜ì„± ê²€ì¦**:
- **ë°©ë²•**: Paired t-test (ê° ablation vs baseline)
- **ìƒ˜í”Œ**: 100ê°œ 5í„´ ëŒ€í™” Ã— 7ê°œ ì¡°ê±´ = 700 ì‹¤í—˜
- **ìœ ì˜ ìˆ˜ì¤€**: p < 0.01 (Bonferroni correction ì ìš©)

#### 2. Comparative Study (ë¹„êµ ì—°êµ¬)

**ë¹„êµ ëŒ€ìƒ**:

```python
comparison_systems = {
    'baseline_chatgpt': {
        'model': 'GPT-4',
        'method': 'zero_shot',
        'context_management': 'full_history'
    },

    'baseline_medpalm': {
        'model': 'Med-PaLM 2',
        'method': 'few_shot',
        'context_management': 'full_history'
    },

    'rag_baseline': {
        'model': 'GPT-4',
        'method': 'standard_rag',
        'context_management': 'full_history',
        'retrieval': 'always_retrieve'
    },

    'our_system': {
        'model': 'GPT-4',
        'method': 'context_engineering',
        'context_management': 'hat_rsum',
        'retrieval': 'self_rag',
        'generation': 'context_aware_tot'
    }
}
```

**ë¹„êµ ë©”íŠ¸ë¦­**:

| ì‹œìŠ¤í…œ | Context F1 | Token â†“ | Time â†“ | Safety | Accuracy | Cost â†“ |
|--------|-----------|---------|--------|--------|----------|--------|
| ChatGPT | 0.72 | 0% | 0% | 0.68 | 0.70 | 0% |
| Med-PaLM 2 | 0.78 | 0% | 0% | 0.75 | 0.82 | 0% |
| RAG Baseline | 0.76 | -30% | +20% | 0.72 | 0.75 | -30% |
| **Our System** | **0.94** | **70%** | **19%** | **0.95** | **0.92** | **70%** |

**í†µê³„ì  ê²€ì¦**:
- **ë°©ë²•**: ANOVA + Tukey HSD post-hoc
- **ìƒ˜í”Œ**: 200ê°œ 5í„´ ëŒ€í™” (overlap with ablation)
- **ìœ ì˜ ìˆ˜ì¤€**: p < 0.001

#### 3. Human Evaluation (ì¸ê°„ í‰ê°€)

**í‰ê°€ì**:
- **ì˜ë£Œ ì „ë¬¸ê°€** 3ëª… (ë‚´ê³¼ ì „ë¬¸ì˜, ì‹¬ì¥ë‚´ê³¼ ì „ë¬¸ì˜, ê°€ì •ì˜í•™ê³¼ ì „ë¬¸ì˜)
- **í™˜ì ëŒ€í‘œ** 5ëª… (ë‹¤ì–‘í•œ ì—°ë ¹ëŒ€ ë° ì˜ë£Œ ë¬¸í•´ë ¥)

**í‰ê°€ í”„ë¡œí† ì½œ**:

```python
class HumanEvaluationProtocol:
    """ì¸ê°„ í‰ê°€ í”„ë¡œí† ì½œ"""

    def evaluate_dialogue(
        self,
        dialogue_session: DialogueSession,
        evaluator_type: str  # 'medical_expert' or 'patient'
    ) -> HumanEvaluation:
        """5í„´ ëŒ€í™” ì¸ê°„ í‰ê°€"""

        evaluation = HumanEvaluation()

        if evaluator_type == 'medical_expert':
            # ì˜ë£Œ ì „ë¬¸ê°€ í‰ê°€
            evaluation.clinical_accuracy = self._rate_1_to_5(
                "ì§„ë‹¨ ë° ê¶Œì¥ì‚¬í•­ì˜ ì„ìƒì  ì •í™•ë„"
            )

            evaluation.safety_appropriateness = self._rate_1_to_5(
                "í™˜ì ì•ˆì „ì„± ê³ ë ¤ ì ì ˆì„±"
            )

            evaluation.context_utilization = self._rate_1_to_5(
                "í™˜ì ë§¥ë½ í™œìš© ì ì ˆì„±"
            )

            evaluation.guideline_adherence = self._rate_1_to_5(
                "ì„ìƒ ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ ì •ë„"
            )

        elif evaluator_type == 'patient':
            # í™˜ì í‰ê°€
            evaluation.understandability = self._rate_1_to_5(
                "ë‹µë³€ì˜ ì´í•´ ìš©ì´ì„±"
            )

            evaluation.empathy = self._rate_1_to_5(
                "ê³µê° ë° ë°°ë ¤ ìˆ˜ì¤€"
            )

            evaluation.actionability = self._rate_1_to_5(
                "ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ ì œê³µ"
            )

            evaluation.trust = self._rate_1_to_5(
                "ì‹œìŠ¤í…œì— ëŒ€í•œ ì‹ ë¢°ë„"
            )

        # ê³µí†µ í‰ê°€
        evaluation.overall_satisfaction = self._rate_1_to_5(
            "ì „ë°˜ì  ë§Œì¡±ë„"
        )

        evaluation.preference_ranking = self._rank_systems([
            'baseline_chatgpt',
            'rag_baseline',
            'our_system'
        ])

        return evaluation
```

**Inter-Rater Reliability**:
- **Fleiss' Kappa** (3ëª… ì „ë¬¸ê°€ ê°„): ëª©í‘œ â‰¥0.75 (substantial agreement)
- **Krippendorff's Alpha** (5ëª… í™˜ì ê°„): ëª©í‘œ â‰¥0.70

**ëª©í‘œ ì¸ê°„ í‰ê°€ ì ìˆ˜**:

| í‰ê°€ í•­ëª© | ì „ë¬¸ê°€ í‰ê°€ | í™˜ì í‰ê°€ | í•™ìˆ ì  ê¸°ì¤€ |
|----------|-----------|----------|-----------|
| Clinical Accuracy | â‰¥4.5/5 | N/A | Expert validation |
| Safety | â‰¥4.7/5 | N/A | Medical safety |
| Context Utilization | â‰¥4.3/5 | N/A | Personalization |
| Understandability | N/A | â‰¥4.2/5 | Patient-centered care |
| Empathy | N/A | â‰¥4.0/5 | Communication quality |
| Overall Satisfaction | â‰¥4.4/5 | â‰¥4.3/5 | Combined metric |
| Preference (1st rank %) | â‰¥70% | â‰¥65% | Comparative preference |

---

## ì‹¤í—˜ ì„¤ê³„ ë° ë°ì´í„°ì…‹

### ë°ì´í„°ì…‹ êµ¬ì¶•

#### 1. 5-Turn Medical Dialogue Dataset

**ë°ì´í„° ìˆ˜ì§‘**:

```python
class MedicalDialogueDataset:
    """5í„´ ì˜ë£Œ ëŒ€í™” ë°ì´í„°ì…‹"""

    def __init__(self):
        self.dialogues = []
        self.total_turns = 0
        self.domains = ['internal_medicine', 'cardiology', 'neurology']

    def create_synthetic_dialogues(self, count: int = 100) -> List[Dialogue]:
        """í•©ì„± ëŒ€í™” ìƒì„± (GPT-4 ê¸°ë°˜)"""

        synthetic_dialogues = []

        for i in range(count):
            # í…œí”Œë¦¿ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤
            scenario = self._generate_scenario(
                domain=random.choice(self.domains),
                complexity=random.choice(['simple', 'moderate', 'complex']),
                risk_level=random.choice(['low', 'medium', 'high'])
            )

            # GPT-4ë¡œ 5í„´ ëŒ€í™” ìƒì„±
            dialogue = self._generate_5turn_dialogue(scenario)

            # ì˜ë£Œ ì „ë¬¸ê°€ ê²€ì¦
            validated_dialogue = self._expert_validation(dialogue)

            synthetic_dialogues.append(validated_dialogue)

        return synthetic_dialogues

    def collect_real_dialogues(self, count: int = 50) -> List[Dialogue]:
        """ì‹¤ì œ ëŒ€í™” ìˆ˜ì§‘ (IRB ìŠ¹ì¸ í•„ìš”)"""

        # Note: ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ìœ„í•œ de-identification í•„ìˆ˜
        real_dialogues = []

        # í˜‘ë ¥ ë³‘ì›/í´ë¦¬ë‹‰ì—ì„œ ìµëª…í™”ëœ ëŒ€í™” ìˆ˜ì§‘
        # í™˜ì ë™ì˜ ë° IRB ìŠ¹ì¸ í•˜ì—

        for dialogue in self._load_from_hospital_records():
            # ê°œì¸ ì‹ë³„ ì •ë³´ ì œê±°
            anonymized = self._anonymize(dialogue)

            # 5í„´ìœ¼ë¡œ ì •ê·œí™” (í•„ìš”ì‹œ padding/truncation)
            normalized = self._normalize_to_5turns(anonymized)

            real_dialogues.append(normalized)

        return real_dialogues
```

**ë°ì´í„°ì…‹ êµ¬ì„±**:

| ìœ í˜• | ê°œìˆ˜ | ì¶œì²˜ | ìš©ë„ |
|------|------|------|------|
| Synthetic (GPT-4) | 100 | Template + GPT-4 generation | Training/Development |
| Real (Anonymized) | 50 | Hospital records (IRB approved) | Validation/Test |
| Expert-Crafted | 20 | Medical experts | Edge cases/Gold standard |
| **Total** | **170** | Mixed | Comprehensive evaluation |

**5í„´ ëŒ€í™” ì˜ˆì‹œ (ë°ì´í„°ì…‹)**:

```json
{
  "dialogue_id": "D001",
  "domain": "cardiology",
  "complexity": "moderate",
  "risk_level": "medium",
  "turns": [
    {
      "turn_id": 1,
      "patient": "ì €ëŠ” 65ì„¸ ë‚¨ì„±ì¸ë°, ìµœê·¼ 3ì¼ê°„ ë‘í†µì´ ìˆê³  í˜ˆì••ì´ 140/90ì…ë‹ˆë‹¤.",
      "agent": "65ì„¸ ë‚¨ì„±ë¶„ê»˜ì„œ 3ì¼ê°„ ë‘í†µì„ ê²½í—˜í•˜ê³  ê³„ì‹œê³ , í˜ˆì••ì´ 140/90mmHgì´ì‹œêµ°ìš”...",
      "ground_truth_context": {
        "demographics": {"age": 65, "gender": "male"},
        "symptoms": [{"name": "headache", "duration": "3days", "severity": "unknown"}],
        "vitals": [{"type": "BP", "systolic": 140, "diastolic": 90}]
      }
    },
    {
      "turn_id": 2,
      "patient": "ë‘í†µì€ 7ì  ì •ë„ê³ , ê³ í˜ˆì••ì€ ì²˜ìŒì´ì—ìš”. ì•„ìŠ¤í”¼ë¦° 100mgì„ ë§¤ì¼ ë¨¹ê³  ìˆìŠµë‹ˆë‹¤.",
      "agent": "7ì ì˜ ë‘í†µì€ ê½¤ ë¶ˆí¸í•˜ì‹œê² ìŠµë‹ˆë‹¤...",
      "ground_truth_context": {
        "symptoms": [{"name": "headache", "severity": "7/10"}],
        "conditions": [{"name": "hypertension", "status": "suspected", "first_occurrence": true}],
        "medications": [{"name": "aspirin", "dosage": "100mg", "frequency": "qd"}]
      }
    },
    // ... Turn 3-5
  ],
  "ground_truth_safety": {
    "contraindications": [],
    "drug_interactions": ["aspirin + future_BP_med (monitor)"],
    "critical_alerts": [
      {
        "type": "family_history_CVD",
        "risk_mentioned_turn": 3,
        "expected_alert_turn": 3
      }
    ]
  },
  "expert_annotations": {
    "correct_diagnosis": "hypertension_stage1",
    "appropriate_actions": [
      "24hr_ABPM",
      "cardiovascular_risk_assessment",
      "lifestyle_modification"
    ]
  }
}
```

#### 2. Evaluation Scenarios

**ì‹œë‚˜ë¦¬ì˜¤ ë¶„ë¥˜**:

```python
evaluation_scenarios = {
    'simple_information_gathering': {
        'description': 'ê°„ë‹¨í•œ ì¦ìƒ ë¬¸ì˜',
        'expected_turns': 3-4,
        'complexity': 'low',
        'example': 'ê°ê¸° ì¦ìƒ ë¬¸ì˜'
    },

    'moderate_diagnosis_support': {
        'description': 'ì¤‘ë“±ë„ ì§„ë‹¨ ì§€ì›',
        'expected_turns': 4-5,
        'complexity': 'medium',
        'example': 'ê³ í˜ˆì•• + ë‘í†µ (ë³¸ ì˜ˆì‹œ)'
    },

    'complex_multi_condition': {
        'description': 'ë³µí•© ì§ˆí™˜ ê´€ë¦¬',
        'expected_turns': 5,
        'complexity': 'high',
        'example': 'ë‹¹ë‡¨ + ê³ í˜ˆì•• + ê°€ì¡±ë ¥'
    },

    'safety_critical': {
        'description': 'ì•ˆì „ì„± ì¤‘ìš” ìƒí™©',
        'expected_turns': 3-5,
        'complexity': 'varies',
        'example': 'ì•½ë¬¼ ìƒí˜¸ì‘ìš©, ì•Œë ˆë¥´ê¸°'
    },

    'context_heavy': {
        'description': 'ë§¥ë½ ì˜ì¡´ë„ ë†’ìŒ',
        'expected_turns': 5,
        'complexity': 'medium-high',
        'example': 'ì´ì „ ì§„ë‹¨ ê¸°ë¡ í™œìš©'
    }
}
```

---

## ì˜ˆìƒ ê²°ê³¼ ë° ê¸°ëŒ€íš¨ê³¼

### ì •ëŸ‰ì  ì˜ˆìƒ ê²°ê³¼

#### ì£¼ìš” ë©”íŠ¸ë¦­ ëª©í‘œ

```python
expected_results = {
    'context_extraction': {
        'entity_f1_turn5': 0.95,
        'attribute_accuracy_turn5': 0.93,
        'relation_accuracy_turn5': 0.92,
        'completeness_turn5': 0.875,
        'improvement_over_baseline': '+26% (F1)'
    },

    'context_utilization': {
        'direct_reference_rate_turn5': 0.85,
        'personalization_score_turn5': 0.85,
        'coherence_score_turn5': 0.95,
        'improvement_over_baseline': '+42% (personalization)'
    },

    'memory_efficiency': {
        'token_savings': '70%',
        'information_retention': '92%',
        'retrieval_speedup': '6.25x',
        'memory_reduction': '73%'
    },

    'safety_accuracy': {
        'contraindication_recall': 0.95,
        'interaction_f1': 0.90,
        'immediate_alert_rate': 0.85,
        'diagnosis_accuracy_turn5': 0.92,
        'recommendation_safety': 0.98
    },

    'human_evaluation': {
        'expert_clinical_accuracy': 4.5,
        'expert_safety': 4.7,
        'patient_satisfaction': 4.3,
        'preference_1st_rank': '70%'
    }
}
```

### í•™ìˆ ì  ê¸°ì—¬ë„

#### 1. ì´ë¡ ì  ê¸°ì—¬

**Contribution 1**: ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” 5í„´ ë§¥ë½ ì¶”ì¶œ í”„ë ˆì„ì›Œí¬
- **ì°¨ë³„ì **: ì¼ë°˜ ëŒ€í™” â†’ ì˜ë£Œ ëŒ€í™”, 2í„´ â†’ 5í„´
- **ì˜í–¥**: ì˜ë£Œ AI ëŒ€í™” ì‹œìŠ¤í…œ ì—°êµ¬ì˜ ìƒˆë¡œìš´ ê¸°ì¤€ ì œì‹œ

**Contribution 2**: ê³„ì¸µì  ë©”ëª¨ë¦¬ + ì ì§„ì  ë§¥ë½ ì •ì œ í†µí•©
- **ì°¨ë³„ì **: ë‹¨ì¼ ë©”ëª¨ë¦¬ â†’ 3-Tier í•˜ì´ë¸Œë¦¬ë“œ
- **ì˜í–¥**: í† í° íš¨ìœ¨ì„±ê³¼ ì •ë³´ ë³´ì¡´ì˜ ê· í˜• ë‹¬ì„±

**Contribution 3**: ë§¥ë½-ì•ˆì „ì„± ë™ì‹œ ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜
- **ì°¨ë³„ì **: ì‚¬í›„ ê²€ì¦ â†’ ì‹¤ì‹œê°„ ë™ì‹œ ê²€ì¦
- **ì˜í–¥**: ì˜ë£Œ AI ì•ˆì „ì„± í‘œì¤€ í–¥ìƒ

#### 2. ì‹¤ìš©ì  ê¸°ì—¬

**ê¸°ëŒ€ íš¨ê³¼ 1**: ì˜ë£Œ ìƒë‹´ ì±—ë´‡ í’ˆì§ˆ í–¥ìƒ
- ë§¥ë½ ê¸°ë°˜ ê°œì¸í™”ë¡œ í™˜ì ë§Œì¡±ë„ +50%
- ì§„ë‹¨ ì •í™•ë„ +42% (0.65 â†’ 0.92)

**ê¸°ëŒ€ íš¨ê³¼ 2**: ì˜ë£Œ ë¹„ìš© ì ˆê°
- í† í° ì‚¬ìš© 70% ê°ì†Œ â†’ API ë¹„ìš© ì ˆê°
- ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ 60% ê°ì†Œ â†’ ì‹œìŠ¤í…œ íš¨ìœ¨ í–¥ìƒ

**ê¸°ëŒ€ íš¨ê³¼ 3**: ì˜ë£Œ ì•ˆì „ì„± ë³´ì¥
- ê¸ˆê¸°ì‚¬í•­ íƒì§€ +36% (0.70 â†’ 0.95)
- ìœ„í—˜ ì˜ˆë°©ë¥  +53% (0.60 â†’ 0.92)

---

## ê²°ë¡  ë° í–¥í›„ ì—°êµ¬

### í•µì‹¬ ìš”ì•½

ë³¸ ì—°êµ¬ëŠ” **"Context Engineering ê¸°ë°˜ ì˜í•™ì§€ì‹ AI Agent"**ë¥¼ 5í„´ ë©€í‹°í„´ ëŒ€í™” í‰ê°€ë¥¼ í†µí•´ ê²€ì¦í•©ë‹ˆë‹¤.

**í•µì‹¬ ì°¨ë³„ì **:
1. **ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™”**: UMLS ê¸°ë°˜ ì˜¨í†¨ë¡œì§€ ë§¥ë½ ì¶”ì¶œ
2. **5í„´ ì‹¬ì¸µ í‰ê°€**: ì ì§„ì  ë§¥ë½ ì •ì œ ì¶”ì 
3. **í•˜ì´ë¸Œë¦¬ë“œ ë©”ëª¨ë¦¬**: HAT + RSumìœ¼ë¡œ 70% í† í° ì ˆê°
4. **ë§¥ë½-ì•ˆì „ì„± í†µí•©**: ì‹¤ì‹œê°„ ê¸ˆê¸°ì‚¬í•­ ì²´í¬

**ì •ëŸ‰ì  ëª©í‘œ**:
- ë§¥ë½ ì¶”ì¶œ F1: **0.95** (+26%)
- ë§¥ë½ í™œìš© ê°œì¸í™”: **0.85** (+42%)
- í† í° ì ˆê°: **70%**
- ì§„ë‹¨ ì •í™•ë„: **0.92** (+42%)
- ì•ˆì „ì„±: **0.95** (+36%)

**í•™ìˆ ì  ê²€ì¦**:
- Ablation study (7 conditions)
- Comparative study (4 systems)
- Human evaluation (3 experts + 5 patients)
- Statistical significance (p < 0.01)

### í–¥í›„ ì—°êµ¬ ë°©í–¥

1. **10í„´+ ì¥ê¸° ëŒ€í™” í™•ì¥**
2. **ë‹¤êµ­ì–´ ë§¥ë½ ì¶”ì¶œ** (ì˜ì–´, ì¤‘êµ­ì–´ ë“±)
3. **ë©€í‹°ëª¨ë‹¬ ë§¥ë½** (ì´ë¯¸ì§€, ìŒì„± í†µí•©)
4. **ì—°í•© í•™ìŠµ ì ìš©** (ê°œì¸ì •ë³´ ë³´í˜¸)

---

*ì‘ì„±ì¼: 2024-12-12*
*ì‘ì„±ì: Medical AI Research Team*
*ë²„ì „: 1.0*