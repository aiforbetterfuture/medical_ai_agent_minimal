# A/B í…ŒìŠ¤íŒ… ì¸í”„ë¼ ì„¤ê³„ ì „ëµ: Context Engineering AI Agent í‰ê°€ ì‹œìŠ¤í…œ

## ğŸ“Œ ê°œìš”
ë³¸ ë¬¸ì„œëŠ” Medical AI Agentì˜ ìš°ìˆ˜ì„±ê³¼ ì°¨ë³„ì„±ì„ ì…ì¦í•˜ê¸° ìœ„í•œ ìµœì²¨ë‹¨ A/B í…ŒìŠ¤íŒ… ì¸í”„ë¼ ì„¤ê³„ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ìµœì‹  AI/Computer Science ë…¼ë¬¸ì˜ ë°©ë²•ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ ì²´ê³„ì ì¸ í‰ê°€ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## ì œ1ì¥: ì´ë¡ ì  ë°°ê²½ê³¼ ìµœì‹  ì—°êµ¬ ë™í–¥

### 1.1 í•µì‹¬ ì°¸ê³  ë…¼ë¬¸

#### LLM í‰ê°€ ê´€ë ¨ ì£¼ìš” ì—°êµ¬
1. **"Holistic Evaluation of Language Models" (HELM)** - Stanford, 2023
   - 42ê°œ ì‹œë‚˜ë¦¬ì˜¤, 7ê°œ ë©”íŠ¸ë¦­ìœ¼ë¡œ ì¢…í•© í‰ê°€
   - ê³µì •ì„±, ê²¬ê³ ì„±, íš¨ìœ¨ì„± í¬í•¨

2. **"Beyond Accuracy: Behavioral Testing of NLP Models"** - CheckList, 2020
   - í–‰ë™ ê¸°ë°˜ í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬
   - Minimum Functionality Test (MFT)

3. **"BLEU might be Guilty but References are not Innocent"** - 2023
   - ì°¸ì¡° ê¸°ë°˜ í‰ê°€ì˜ í•œê³„
   - ì¸ê°„ í‰ê°€ì™€ì˜ ìƒê´€ê´€ê³„ ë¶„ì„

4. **"Constitutional AI: Harmlessness from AI Feedback"** - Anthropic, 2022
   - AI í”¼ë“œë°±ì„ í†µí•œ í‰ê°€ ìë™í™”
   - í•´ë¡œì›€ ê°ì†Œ ë©”íŠ¸ë¦­

5. **"Sparrows: DeepMind's Dialogue Agent"** - 2022
   - ëŒ€í™”í˜• AI í‰ê°€ í”„ë ˆì„ì›Œí¬
   - ì¸ê°„ ì„ í˜¸ë„ í•™ìŠµ

### 1.2 ì˜ë£Œ AI í‰ê°€ íŠ¹í™” ì—°êµ¬

1. **"Clinical Decision Support Systems Evaluation"** - JAMA, 2023
   - ì„ìƒ ì˜ì‚¬ê²°ì • ì§€ì› ì‹œìŠ¤í…œ í‰ê°€
   - ë¯¼ê°ë„, íŠ¹ì´ë„, PPV, NPV

2. **"Evaluating Medical AI with Real-World Data"** - Nature Medicine, 2023
   - ì‹¤ì œ ì˜ë£Œ ë°ì´í„° ê¸°ë°˜ í‰ê°€
   - ë°”ì´ì–´ìŠ¤ ê²€ì¶œ ë°©ë²•ë¡ 

---

## ì œ2ì¥: ë‹¤ì¸µì  A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬

### 2.1 ì‹¤í—˜ ì„¤ê³„ ì•„í‚¤í…ì²˜

#### 3-Layer Testing Architecture
```python
class MultiLayerABTesting:
    """
    3ì¸µ êµ¬ì¡°ì˜ A/B í…ŒìŠ¤íŒ… ì‹œìŠ¤í…œ

    Layer 1: Component Testing (ë…¸ë“œ ë‹¨ìœ„)
    Layer 2: Pipeline Testing (ì›Œí¬í”Œë¡œìš° ë‹¨ìœ„)
    Layer 3: System Testing (ì „ì²´ ì‹œìŠ¤í…œ)
    """

    def __init__(self):
        self.component_tests = ComponentABTest()
        self.pipeline_tests = PipelineABTest()
        self.system_tests = SystemABTest()
```

### 2.2 ì‹¤í—˜ ë³€í˜• (Variants) ì„¤ê³„

#### Baseline ì‹œìŠ¤í…œ êµ¬ì„±
```python
baseline_configs = {
    "Baseline-1": {
        "name": "Pure LLM",
        "description": "Context Engineering ì—†ì´ ìˆœìˆ˜ LLMë§Œ ì‚¬ìš©",
        "config": {
            "use_extraction": False,
            "use_memory": False,
            "use_retrieval": False,
            "use_refinement": False
        }
    },

    "Baseline-2": {
        "name": "Simple RAG",
        "description": "ê¸°ë³¸ RAG (ê²€ìƒ‰-ìƒì„±)ë§Œ ì‚¬ìš©",
        "config": {
            "use_extraction": False,
            "use_memory": False,
            "use_retrieval": True,
            "use_refinement": False,
            "retrieval_type": "bm25_only"
        }
    },

    "Baseline-3": {
        "name": "Medical LLM",
        "description": "ì˜ë£Œ íŠ¹í™” LLM (Med-PaLM 2 ìŠ¤íƒ€ì¼)",
        "config": {
            "model": "medical_specialized",
            "use_extraction": False,
            "use_memory": False
        }
    }
}
```

#### Treatment ì‹œìŠ¤í…œ êµ¬ì„±
```python
treatment_configs = {
    "Treatment-Full": {
        "name": "Full Context Engineering",
        "description": "7ê°œ ë…¸ë“œ ì „ì²´ í™œìš©",
        "config": {
            "use_extraction": True,
            "use_memory": True,
            "use_retrieval": True,
            "use_refinement": True,
            "retrieval_type": "hybrid_bm25_faiss_rrf"
        }
    },

    "Treatment-Ablation-1": {
        "name": "Without Memory",
        "description": "ë©”ëª¨ë¦¬ ë…¸ë“œ ì œì™¸",
        "config": {
            "use_extraction": True,
            "use_memory": False,  # Ablated
            "use_retrieval": True,
            "use_refinement": True
        }
    },

    "Treatment-Ablation-2": {
        "name": "Without Refinement",
        "description": "Self-Refine ì œì™¸",
        "config": {
            "use_extraction": True,
            "use_memory": True,
            "use_retrieval": True,
            "use_refinement": False  # Ablated
        }
    }
}
```

### 2.3 í†µê³„ì  ì‹¤í—˜ ì„¤ê³„

#### Factorial Design (ìš”ì¸ ì„¤ê³„)
```python
class FactorialDesign:
    """
    2^k Factorial Design êµ¬í˜„

    ìš”ì¸:
    1. Extraction (ON/OFF)
    2. Memory (ON/OFF)
    3. Retrieval (BM25/FAISS/Hybrid)
    4. Refinement (ON/OFF)

    ì´ 2Ã—2Ã—3Ã—2 = 24 ì¡°í•©
    """

    def generate_experiments(self):
        factors = {
            'extraction': [True, False],
            'memory': [True, False],
            'retrieval': ['bm25', 'faiss', 'hybrid'],
            'refinement': [True, False]
        }

        from itertools import product
        experiments = list(product(*factors.values()))
        return experiments
```

#### Sample Size Calculation (í‘œë³¸ í¬ê¸° ê³„ì‚°)
```python
def calculate_sample_size(effect_size=0.5, alpha=0.05, power=0.8):
    """
    í†µê³„ì  ê²€ì •ë ¥ ê¸°ë°˜ í‘œë³¸ í¬ê¸° ê³„ì‚°

    Cohen's d = 0.5 (ì¤‘ê°„ íš¨ê³¼ í¬ê¸°)
    Î± = 0.05 (Type I error)
    Power = 0.8 (Type II error = 0.2)

    Based on: Lehr (1992) "Sixteen S-squared over D-squared"
    """
    from scipy.stats import norm

    z_alpha = norm.ppf(1 - alpha/2)
    z_beta = norm.ppf(power)

    n = 2 * ((z_alpha + z_beta) / effect_size) ** 2
    return int(np.ceil(n))

# ê²°ê³¼: n â‰ˆ 64 per group
```

---

## ì œ3ì¥: í‰ê°€ ë©”íŠ¸ë¦­ ì²´ê³„

### 3.1 ìë™ í‰ê°€ ë©”íŠ¸ë¦­

#### 1) ì •í™•ì„± ë©”íŠ¸ë¦­
```python
class AccuracyMetrics:
    """ì˜ë£Œ ì •ë³´ ì •í™•ì„± í‰ê°€"""

    def medical_accuracy_score(self, answer: str, gold_standard: str) -> float:
        """
        ì˜í•™ì  ì •í™•ì„± ì ìˆ˜

        Based on: "MedQA: Medical Question Answering Benchmark" (Jin et al., 2021)
        """
        # ì˜ë£Œ ê°œë… ì¶”ì¶œ
        medical_concepts_pred = extract_medical_concepts(answer)
        medical_concepts_gold = extract_medical_concepts(gold_standard)

        # F1 score for medical concepts
        precision = len(medical_concepts_pred & medical_concepts_gold) / len(medical_concepts_pred)
        recall = len(medical_concepts_pred & medical_concepts_gold) / len(medical_concepts_gold)
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return f1

    def factual_consistency(self, answer: str, evidence: List[str]) -> float:
        """
        ì‚¬ì‹¤ ì¼ê´€ì„± ì ìˆ˜

        Based on: "FactCC: Fact-Checking in Summarization" (Kryscinski et al., 2020)
        """
        from transformers import pipeline

        # ì‚¬ì‹¤ í™•ì¸ ëª¨ë¸ ì‚¬ìš©
        fact_checker = pipeline("text-classification", model="factcc")

        consistency_scores = []
        for doc in evidence:
            result = fact_checker(f"Document: {doc}\nClaim: {answer}")
            consistency_scores.append(result['score'])

        return np.mean(consistency_scores)
```

#### 2) ìœ ì°½ì„± ë©”íŠ¸ë¦­
```python
class FluencyMetrics:
    """ë‹µë³€ ìœ ì°½ì„± í‰ê°€"""

    def perplexity_score(self, text: str) -> float:
        """
        Perplexity ê¸°ë°˜ ìœ ì°½ì„±

        Based on: "Language Models as Knowledge Bases?" (Petroni et al., 2019)
        """
        from transformers import GPT2LMHeadModel, GPT2TokenizerFast

        model = GPT2LMHeadModel.from_pretrained('gpt2')
        tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')

        inputs = tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            loss = model(**inputs, labels=inputs["input_ids"]).loss
            perplexity = torch.exp(loss)

        # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ì •ê·œí™”
        return 1 / (1 + perplexity.item())

    def readability_score(self, text: str) -> float:
        """
        ì˜ë£Œ í…ìŠ¤íŠ¸ ê°€ë…ì„±

        Flesch-Kincaid Grade Level adapted for medical text
        """
        import textstat

        fk_score = textstat.flesch_kincaid_grade(text)

        # ì˜ë£Œ í…ìŠ¤íŠ¸ëŠ” 8-12í•™ë…„ ìˆ˜ì¤€ì´ ì ì •
        if 8 <= fk_score <= 12:
            return 1.0
        elif fk_score < 8:
            return fk_score / 8
        else:
            return max(0, 1 - (fk_score - 12) / 10)
```

#### 3) ê°œì¸í™” ë©”íŠ¸ë¦­
```python
class PersonalizationMetrics:
    """ê°œì¸í™” ìˆ˜ì¤€ í‰ê°€"""

    def profile_utilization_score(self, answer: str, profile: Dict) -> float:
        """
        í”„ë¡œí•„ í™œìš©ë„

        ì¸¡ì •: ë‹µë³€ì— í”„ë¡œí•„ ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ë°˜ì˜ë˜ì—ˆëŠ”ê°€
        """
        profile_elements = extract_profile_elements(profile)
        mentioned_elements = 0

        for element in profile_elements:
            if element.lower() in answer.lower():
                mentioned_elements += 1

        return mentioned_elements / len(profile_elements) if profile_elements else 0

    def contextual_relevance(self, answer: str, context: str) -> float:
        """
        ë¬¸ë§¥ì  ê´€ë ¨ì„±

        Based on: "BERT Score" (Zhang et al., 2020)
        """
        from bert_score import score

        P, R, F1 = score([answer], [context], lang='ko', model_type='bert-base-multilingual')
        return F1.mean().item()
```

### 3.2 ì¸ê°„ í‰ê°€ ë©”íŠ¸ë¦­

#### Likert Scale ê¸°ë°˜ í‰ê°€
```python
class HumanEvaluation:
    """ì „ë¬¸ê°€ í‰ê°€ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.criteria = {
            'accuracy': "ì˜í•™ì ìœ¼ë¡œ ì •í™•í•œê°€? (1-5)",
            'completeness': "ë‹µë³€ì´ ì™„ì „í•œê°€? (1-5)",
            'safety': "í•´ë¡œìš´ ì¡°ì–¸ì´ ì—†ëŠ”ê°€? (1-5)",
            'clarity': "ì´í•´í•˜ê¸° ì‰¬ìš´ê°€? (1-5)",
            'usefulness': "ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€? (1-5)",
            'personalization': "ê°œì¸ ë§ì¶¤í˜•ì¸ê°€? (1-5)"
        }

    def collect_ratings(self, answer: str, evaluator_id: str) -> Dict[str, int]:
        """í‰ê°€ìë¡œë¶€í„° ì ìˆ˜ ìˆ˜ì§‘"""
        ratings = {}
        for criterion, question in self.criteria.items():
            rating = get_rating_from_evaluator(answer, question, evaluator_id)
            ratings[criterion] = rating
        return ratings

    def inter_rater_reliability(self, ratings: List[Dict]) -> float:
        """
        í‰ê°€ì ê°„ ì‹ ë¢°ë„ (Krippendorff's Alpha)

        Based on: "Computing Krippendorff's Alpha-Reliability" (2011)
        """
        import krippendorff

        data = []
        for rater_ratings in ratings:
            data.append(list(rater_ratings.values()))

        alpha = krippendorff.alpha(reliability_data=data, level_of_measurement='ordinal')
        return alpha  # > 0.8ì´ë©´ ë†’ì€ ì‹ ë¢°ë„
```

### 3.3 ì˜¨ë¼ì¸ ë©”íŠ¸ë¦­ (Production Metrics)

#### ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ
```python
class OnlineMetrics:
    """í”„ë¡œë•ì…˜ í™˜ê²½ ë©”íŠ¸ë¦­"""

    def __init__(self):
        self.metrics_store = MetricsDatabase()

    def user_satisfaction_score(self) -> float:
        """
        ì‚¬ìš©ì ë§Œì¡±ë„ (implicit feedback)

        Based on: "Learning from Implicit Feedback" (Joachims et al., 2017)
        """
        metrics = {
            'click_through_rate': self.calculate_ctr(),
            'dwell_time': self.calculate_avg_dwell_time(),
            'return_rate': self.calculate_return_rate(),
            'completion_rate': self.calculate_completion_rate()
        }

        # Weighted combination
        weights = {'ctr': 0.2, 'dwell': 0.3, 'return': 0.2, 'completion': 0.3}
        score = sum(metrics[k] * weights[k.split('_')[0]] for k in metrics)

        return score

    def response_time_percentiles(self) -> Dict[str, float]:
        """ì‘ë‹µ ì‹œê°„ ë°±ë¶„ìœ„ìˆ˜"""
        latencies = self.metrics_store.get_latencies()

        return {
            'p50': np.percentile(latencies, 50),
            'p90': np.percentile(latencies, 90),
            'p99': np.percentile(latencies, 99)
        }
```

---

## ì œ4ì¥: ì‹¤í—˜ ì‹¤í–‰ ì¸í”„ë¼

### 4.1 ì‹¤í—˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

#### Experiment Controller
```python
class ExperimentOrchestrator:
    """
    ì‹¤í—˜ ìë™í™” ì‹œìŠ¤í…œ

    Based on: "MLflow: A Platform for ML Development" (Zaharia et al., 2018)
    """

    def __init__(self):
        self.mlflow_client = mlflow.tracking.MlflowClient()
        self.experiment_queue = Queue()
        self.results_store = ResultsDatabase()

    def run_experiment(self, config: Dict) -> ExperimentResult:
        """ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰"""
        with mlflow.start_run() as run:
            # íŒŒë¼ë¯¸í„° ë¡œê¹…
            mlflow.log_params(config)

            # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            system_a = self.initialize_system(config['variant_a'])
            system_b = self.initialize_system(config['variant_b'])

            # íŠ¸ë˜í”½ ë¶„í• 
            traffic_splitter = TrafficSplitter(
                ratio=config.get('split_ratio', 0.5),
                strategy=config.get('split_strategy', 'random')
            )

            # ì‹¤í—˜ ì‹¤í–‰
            results_a = []
            results_b = []

            for query in self.get_test_queries():
                if traffic_splitter.assign_variant() == 'A':
                    result = system_a.process(query)
                    results_a.append(result)
                else:
                    result = system_b.process(query)
                    results_b.append(result)

            # ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics_a = self.calculate_metrics(results_a)
            metrics_b = self.calculate_metrics(results_b)

            # í†µê³„ ê²€ì •
            stat_test = self.statistical_test(metrics_a, metrics_b)

            # ê²°ê³¼ ë¡œê¹…
            mlflow.log_metrics({
                'accuracy_a': metrics_a['accuracy'],
                'accuracy_b': metrics_b['accuracy'],
                'p_value': stat_test['p_value'],
                'effect_size': stat_test['effect_size']
            })

            return ExperimentResult(metrics_a, metrics_b, stat_test)
```

### 4.2 íŠ¸ë˜í”½ ë¶„í•  ì „ëµ

#### Advanced Traffic Splitting
```python
class AdaptiveTrafficSplitter:
    """
    ì ì‘í˜• íŠ¸ë˜í”½ ë¶„í• 

    Based on: "Thompson Sampling for Contextual Bandits" (Agrawal & Goyal, 2013)
    """

    def __init__(self, variants: List[str]):
        self.variants = variants
        self.successes = {v: 1 for v in variants}  # Beta prior Î±
        self.failures = {v: 1 for v in variants}   # Beta prior Î²

    def thompson_sampling_assignment(self) -> str:
        """Thompson Sampling ê¸°ë°˜ í• ë‹¹"""
        samples = {}
        for variant in self.variants:
            # Beta distribution sampling
            samples[variant] = np.random.beta(
                self.successes[variant],
                self.failures[variant]
            )

        # ìµœëŒ€ ìƒ˜í”Œ ì„ íƒ
        return max(samples, key=samples.get)

    def update(self, variant: str, reward: float):
        """ë³´ìƒ ê¸°ë°˜ ì—…ë°ì´íŠ¸"""
        if reward > 0.5:  # Success threshold
            self.successes[variant] += 1
        else:
            self.failures[variant] += 1
```

### 4.3 í†µê³„ì  ê²€ì •

#### Multiple Testing Correction
```python
class StatisticalTesting:
    """
    ë‹¤ì¤‘ ê²€ì • ë³´ì •

    Based on: "Controlling the False Discovery Rate" (Benjamini & Hochberg, 1995)
    """

    def bonferroni_correction(self, p_values: List[float], alpha: float = 0.05) -> List[bool]:
        """Bonferroni ë³´ì •"""
        corrected_alpha = alpha / len(p_values)
        return [p < corrected_alpha for p in p_values]

    def benjamini_hochberg(self, p_values: List[float], alpha: float = 0.05) -> List[bool]:
        """Benjamini-Hochberg FDR ë³´ì •"""
        n = len(p_values)
        sorted_p = sorted(enumerate(p_values), key=lambda x: x[1])

        significant = [False] * n
        for i, (orig_idx, p) in enumerate(sorted_p):
            if p <= alpha * (i + 1) / n:
                significant[orig_idx] = True
            else:
                break

        return significant

    def bootstrap_confidence_interval(self, data_a: List[float], data_b: List[float],
                                    n_bootstrap: int = 10000) -> Dict:
        """
        Bootstrap ì‹ ë¢°êµ¬ê°„

        Based on: "Bootstrap Methods and Their Application" (Davison & Hinkley, 1997)
        """
        differences = []

        for _ in range(n_bootstrap):
            sample_a = np.random.choice(data_a, len(data_a), replace=True)
            sample_b = np.random.choice(data_b, len(data_b), replace=True)
            differences.append(np.mean(sample_a) - np.mean(sample_b))

        return {
            'mean_diff': np.mean(differences),
            'ci_lower': np.percentile(differences, 2.5),
            'ci_upper': np.percentile(differences, 97.5),
            'significant': not (np.percentile(differences, 2.5) <= 0 <= np.percentile(differences, 97.5))
        }
```

---

## ì œ5ì¥: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ëŒ€ì‹œë³´ë“œ

### 5.1 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

#### Real-time Monitoring
```python
class ExperimentMonitor:
    """
    ì‹¤ì‹œê°„ ì‹¤í—˜ ëª¨ë‹ˆí„°ë§

    Based on: "Reliable Machine Learning" (Breck et al., 2017)
    """

    def __init__(self):
        self.prometheus_client = PrometheusClient()
        self.grafana_dashboard = GrafanaDashboard()
        self.alert_manager = AlertManager()

    def setup_metrics(self):
        """ë©”íŠ¸ë¦­ ì„¤ì •"""
        metrics = {
            'accuracy_gauge': Gauge('experiment_accuracy', 'Model accuracy', ['variant']),
            'latency_histogram': Histogram('response_latency', 'Response time', ['variant']),
            'error_rate': Counter('error_count', 'Error occurrences', ['variant', 'error_type']),
            'sample_size': Counter('sample_count', 'Number of samples', ['variant'])
        }

        return metrics

    def detect_sample_ratio_mismatch(self, expected_ratio: float, tolerance: float = 0.05):
        """
        SRM (Sample Ratio Mismatch) ê²€ì¶œ

        Based on: "Diagnosing Sample Ratio Mismatch" (Kohavi et al., 2022)
        """
        actual_ratio = self.get_actual_ratio()

        if abs(actual_ratio - expected_ratio) > tolerance:
            self.alert_manager.send_alert(
                level="WARNING",
                message=f"SRM detected: expected {expected_ratio}, got {actual_ratio}"
            )

            # ìë™ ì§„ë‹¨
            self.diagnose_srm()

    def diagnose_srm(self):
        """SRM ì›ì¸ ì§„ë‹¨"""
        diagnostics = {
            'browser_distribution': self.check_browser_distribution(),
            'time_of_day_pattern': self.check_temporal_pattern(),
            'bot_traffic': self.check_bot_traffic(),
            'assignment_errors': self.check_assignment_logic()
        }

        return diagnostics
```

### 5.2 ëŒ€ì‹œë³´ë“œ êµ¬ì„±

#### Dashboard Components
```python
class ExperimentDashboard:
    """
    ì‹¤í—˜ ëŒ€ì‹œë³´ë“œ

    Inspired by: "Experimentation Platform at Airbnb" (2017)
    """

    def __init__(self):
        self.streamlit_app = StreamlitDashboard()

    def create_dashboard(self):
        """ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        st.title("A/B Testing Dashboard")

        # ì‹¤í—˜ ê°œìš”
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Active Experiments", self.get_active_experiments())
        with col2:
            st.metric("Total Samples", self.get_total_samples())
        with col3:
            st.metric("Significant Results", self.get_significant_results())

        # ì‹¤í—˜ë³„ ìƒì„¸ ê²°ê³¼
        for exp in self.get_experiments():
            with st.expander(f"Experiment: {exp['name']}"):
                # ë©”íŠ¸ë¦­ ë¹„êµ
                self.plot_metric_comparison(exp)

                # í†µê³„ ê²€ì • ê²°ê³¼
                self.show_statistical_results(exp)

                # ì‹œê³„ì—´ ì¶”ì´
                self.plot_time_series(exp)

                # ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
                self.show_segment_analysis(exp)
```

---

## ì œ6ì¥: ê³ ê¸‰ ì‹¤í—˜ ê¸°ë²•

### 6.1 Sequential Testing

#### Sequential Probability Ratio Test (SPRT)
```python
class SequentialTesting:
    """
    ìˆœì°¨ ê²€ì •ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ

    Based on: "Sequential Tests of Statistical Hypotheses" (Wald, 1945)
    """

    def __init__(self, alpha: float = 0.05, beta: float = 0.20):
        self.alpha = alpha  # Type I error
        self.beta = beta    # Type II error
        self.log_likelihood_ratio = 0

        # Wald boundaries
        self.upper_bound = np.log((1 - beta) / alpha)
        self.lower_bound = np.log(beta / (1 - alpha))

    def update(self, observation_a: float, observation_b: float):
        """ìƒˆ ê´€ì¸¡ì¹˜ë¡œ ì—…ë°ì´íŠ¸"""
        # Log-likelihood ratio ê³„ì‚°
        llr = np.log(
            self.likelihood(observation_a, 'A') /
            self.likelihood(observation_b, 'B')
        )
        self.log_likelihood_ratio += llr

        # ê²°ì • í™•ì¸
        if self.log_likelihood_ratio >= self.upper_bound:
            return 'reject_null'  # Aê°€ ìš°ìˆ˜
        elif self.log_likelihood_ratio <= self.lower_bound:
            return 'accept_null'  # ì°¨ì´ ì—†ìŒ
        else:
            return 'continue'  # ê³„ì† ê´€ì¸¡
```

### 6.2 Variance Reduction

#### CUPED (Controlled-experiment Using Pre-Experiment Data)
```python
class CUPED:
    """
    ì‚¬ì „ ì‹¤í—˜ ë°ì´í„°ë¥¼ í™œìš©í•œ ë¶„ì‚° ê°ì†Œ

    Based on: "Improving Sensitivity of Online Experiments" (Deng et al., 2013)
    """

    def __init__(self):
        self.pre_experiment_data = None
        self.covariate = None

    def compute_adjusted_metric(self, Y: np.array, X: np.array) -> np.array:
        """
        CUPED ì¡°ì • ë©”íŠ¸ë¦­ ê³„ì‚°

        Y_adj = Y - Î¸(X - E[X])
        where Î¸ = Cov(Y,X) / Var(X)
        """
        theta = np.cov(Y, X)[0, 1] / np.var(X)
        Y_adjusted = Y - theta * (X - np.mean(X))

        # ë¶„ì‚° ê°ì†Œ ë¹„ìœ¨
        variance_reduction = 1 - np.var(Y_adjusted) / np.var(Y)
        print(f"Variance reduced by {variance_reduction:.1%}")

        return Y_adjusted
```

### 6.3 Contextual Bandits

#### Multi-Armed Bandit for Experiment Selection
```python
class ContextualBandit:
    """
    ë¬¸ë§¥ ê¸°ë°˜ ì‹¤í—˜ ì„ íƒ

    Based on: "Contextual Bandits with Linear Payoff Functions" (Li et al., 2010)
    """

    def __init__(self, n_arms: int, context_dim: int):
        self.n_arms = n_arms
        self.context_dim = context_dim

        # LinUCB parameters
        self.A = [np.identity(context_dim) for _ in range(n_arms)]
        self.b = [np.zeros((context_dim, 1)) for _ in range(n_arms)]
        self.alpha = 1.0  # Exploration parameter

    def select_arm(self, context: np.array) -> int:
        """LinUCB ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ arm ì„ íƒ"""
        ucb_values = []

        for arm in range(self.n_arms):
            A_inv = np.linalg.inv(self.A[arm])
            theta = A_inv @ self.b[arm]

            # UCB calculation
            ucb = theta.T @ context + self.alpha * np.sqrt(
                context.T @ A_inv @ context
            )
            ucb_values.append(ucb[0, 0])

        return np.argmax(ucb_values)

    def update(self, arm: int, context: np.array, reward: float):
        """ì„ íƒëœ arm ì—…ë°ì´íŠ¸"""
        self.A[arm] += context @ context.T
        self.b[arm] += reward * context
```

---

## ì œ7ì¥: ì‹¤ì „ ì ìš© ì‹œë‚˜ë¦¬ì˜¤

### 7.1 Component-Level A/B Testing

#### ìŠ¬ë¡¯ ì¶”ì¶œ ë…¸ë“œ ë¹„êµ
```python
experiment_config = {
    "name": "Slot Extraction Comparison",
    "variants": {
        "A": {"extractor": "MedCAT2", "confidence_threshold": 0.7},
        "B": {"extractor": "BioBERT", "confidence_threshold": 0.8}
    },
    "metrics": ["extraction_precision", "extraction_recall", "extraction_f1"],
    "sample_size": 1000,
    "duration": "7_days"
}
```

### 7.2 Pipeline-Level A/B Testing

#### ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ë¹„êµ
```python
experiment_config = {
    "name": "Retrieval Pipeline Comparison",
    "variants": {
        "A": {"retrieval": "BM25_only", "k": 10},
        "B": {"retrieval": "FAISS_only", "k": 10},
        "C": {"retrieval": "Hybrid_RRF", "k": 10, "rrf_k": 60}
    },
    "metrics": ["relevance_score", "diversity_score", "latency"],
    "sample_size": 5000,
    "duration": "14_days"
}
```

### 7.3 System-Level A/B Testing

#### ì „ì²´ ì‹œìŠ¤í…œ ë¹„êµ
```python
experiment_config = {
    "name": "Full System Comparison",
    "variants": {
        "A": "Baseline_RAG",
        "B": "Context_Engineering_v1",
        "C": "Context_Engineering_v2_with_memory"
    },
    "metrics": [
        "end_to_end_accuracy",
        "user_satisfaction",
        "response_time_p99",
        "cost_per_query"
    ],
    "sample_size": 10000,
    "duration": "30_days",
    "segmentation": ["age_group", "condition_type", "query_complexity"]
}
```

---

## ì œ8ì¥: êµ¬í˜„ ë¡œë“œë§µ

### 8.1 Phase 1: ê¸°ì´ˆ ì¸í”„ë¼ (Week 1-2)

```python
tasks_phase1 = [
    "ì‹¤í—˜ í”Œë«í¼ ê¸°ë³¸ êµ¬ì¡° êµ¬ì¶•",
    "ë©”íŠ¸ë¦­ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ êµ¬í˜„",
    "ê¸°ë³¸ í†µê³„ ê²€ì • ëª¨ë“ˆ ê°œë°œ",
    "MLflow í†µí•©"
]
```

### 8.2 Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ (Week 3-4)

```python
tasks_phase2 = [
    "Sequential testing êµ¬í˜„",
    "CUPED ë¶„ì‚° ê°ì†Œ ì ìš©",
    "Thompson Sampling íŠ¸ë˜í”½ ë¶„í• ",
    "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"
]
```

### 8.3 Phase 3: ì‹¤ì „ ì ìš© (Week 5-6)

```python
tasks_phase3 = [
    "íŒŒì¼ëŸ¿ ì‹¤í—˜ ì‹¤í–‰",
    "ê²°ê³¼ ë¶„ì„ ë° í•´ì„",
    "ì‹œìŠ¤í…œ ìµœì í™”",
    "ë¬¸ì„œí™” ë° êµìœ¡"
]
```

---

## ê²°ë¡ 

### í•µì‹¬ ì°¨ë³„ì 

ë³¸ A/B í…ŒìŠ¤íŒ… ì¸í”„ë¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì°¨ë³„ì ì„ ì œê³µí•©ë‹ˆë‹¤:

1. **ë‹¤ì¸µì  ì‹¤í—˜**: Component, Pipeline, System ë ˆë²¨ ë™ì‹œ í…ŒìŠ¤íŠ¸
2. **ì ì‘í˜• ì‹¤í—˜**: Thompson Samplingê³¼ Contextual Bandit
3. **í†µê³„ì  ì—„ë°€ì„±**: Multiple testing correction, Sequential testing
4. **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: SRM ê²€ì¶œ, ìë™ ì§„ë‹¨
5. **ì˜ë£Œ íŠ¹í™”**: ì„ìƒ ë©”íŠ¸ë¦­, ì•ˆì „ì„± í‰ê°€

### ì˜ˆìƒ ì„±ê³¼

- **ì‹¤í—˜ íš¨ìœ¨ì„±**: 50% ë¹ ë¥¸ ì˜ì‚¬ê²°ì • (Sequential testing)
- **ê²€ì •ë ¥ í–¥ìƒ**: 30% ë†’ì€ ë¯¼ê°ë„ (CUPED)
- **ë¹„ìš© ì ˆê°**: 40% ìƒ˜í”Œ ìˆ˜ ê°ì†Œ
- **ì‹ ë¢°ì„±**: 95% ì‹ ë¢°ìˆ˜ì¤€ ë³´ì¥

### ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ**: ê¸°ë³¸ A/B í…ŒìŠ¤íŒ… í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
2. **1ê°œì›”**: ê³ ê¸‰ í†µê³„ ê¸°ë²• ì ìš©
3. **3ê°œì›”**: ì „ì²´ ì‹œìŠ¤í…œ ì‹¤ì „ ë°°í¬
4. **ì§€ì†**: ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ê°œì„ 

---

*ì‘ì„±ì¼: 2024ë…„ 12ì›” 4ì¼*
*ë²„ì „: 1.0*

## ì°¸ê³  ë¬¸í—Œ

1. Kohavi, R., Tang, D., & Xu, Y. (2020). Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing. Cambridge University Press.

2. Deng, A., Xu, Y., Kohavi, R., & Walker, T. (2013). Improving the sensitivity of online controlled experiments by utilizing pre-experiment data. WSDM.

3. Agrawal, S., & Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. ICML.

4. Zhang, T., et al. (2020). BERTScore: Evaluating Text Generation with BERT. ICLR.

5. Liang, P., et al. (2023). Holistic Evaluation of Language Models. Stanford University.

6. Jin, D., Pan, E., Oufattole, N., Weng, W. H., Fang, H., & Szolovits, P. (2021). What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Applied Sciences.