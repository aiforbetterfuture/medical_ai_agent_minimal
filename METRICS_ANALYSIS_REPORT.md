# í‰ê°€ì§€í‘œ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ” í˜„ì¬ ìƒí™© ìš”ì•½

**ê²°ë¡ **: RAGAS í‰ê°€ì§€í‘œ(Faithfulness, Answer Relevance ë“±)ê°€ **ì„¤ì •ì€ ë˜ì–´ ìˆìœ¼ë‚˜ ì‹¤ì œë¡œ ê³„ì‚°/ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤**.

---

## ğŸ“‹ ì„¤ì • íŒŒì¼ ë¶„ì„

### 1. `experiments/config.yaml`

```yaml
evaluation:
  per_turn_metrics: ["faithfulness", "answer_relevance", "judge_total_score"]
  multiturn_metrics: ["context_utilization", "context_contradiction", "update_responsiveness"]
  export_summary_json: true
  export_summary_path: "runs/{run_id}/summary.json"
```

**ìƒíƒœ**: âœ… ì„¤ì •ì€ ë˜ì–´ ìˆìŒ

---

### 2. `experiments/schemas/events_record.schema.json`

```json
{
  "metrics": {
    "type": ["object", "null"],
    "description": "Evaluation metrics (if computed)",
    "properties": {
      "faithfulness": {"type": ["number", "null"]},
      "answer_relevance": {"type": ["number", "null"]},
      "context_precision": {"type": ["number", "null"]},
      "context_recall": {"type": ["number", "null"]},
      "context_relevancy": {"type": ["number", "null"]}
    }
  }
}
```

**ìƒíƒœ**: âœ… ìŠ¤í‚¤ë§ˆëŠ” ì •ì˜ë˜ì–´ ìˆìŒ

---

## âŒ ë¬¸ì œì  ë¶„ì„

### 1. ì‹¤í—˜ ëŸ¬ë„ˆì—ì„œ í‰ê°€ì§€í‘œ ê³„ì‚°/ì €ì¥ ë¡œì§ ë¶€ì¬

**íŒŒì¼**: `experiments/run_multiturn_experiment_v2.py`

**ë¬¸ì œ**:
- `_log_event()` ë©”ì„œë“œì—ì„œ `metrics` í•„ë“œë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŒ
- RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸ì¶œ ì½”ë“œê°€ ì—†ìŒ
- í‰ê°€ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ì§€ ì•ŠìŒ

**í˜„ì¬ ì½”ë“œ êµ¬ì¡°**:
```python
def _log_event(self, ...):
    event = {
        "schema_version": "events_record.v1",
        "run_id": self.run_id,
        "mode": mode,
        "patient_id": patient_id,
        "turn_id": turn_id,
        "question": {...},
        "answer": {...},
        "usage": {...},
        "timing_ms": {...},
        "metadata": {...},
        # âŒ "metrics": {...} í•„ë“œê°€ ì—†ìŒ!
    }
```

---

### 2. í‰ê°€ì§€í‘œ ê³„ì‚° ëª¨ë“ˆ ë¶€ì¬

**í™•ì¸ ê²°ê³¼**:
- `experiments/evaluation/multiturn_metrics.py`: ë©€í‹°í„´ íŠ¹í™” ì§€í‘œë§Œ ê³„ì‚° (Context Utilization, Context Contradiction ë“±)
- RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° ì½”ë“œê°€ ì—†ìŒ
- `ragas` ë¼ì´ë¸ŒëŸ¬ë¦¬ import/ì‚¬ìš© ì½”ë“œê°€ ì—†ìŒ

---

### 3. events.jsonlì— metrics í•„ë“œ ì—†ìŒ

**í™•ì¸ ê²°ê³¼**:
```bash
python -c "import json; f=open('runs/2025-12-13_primary_v1/events.jsonl','r',encoding='utf-8'); line=f.readline(); data=json.loads(line); print('metrics' in data); print(data.get('metrics', 'NOT FOUND'))"
# ì¶œë ¥: False, NOT FOUND
```

**ìƒíƒœ**: âŒ `events.jsonl`ì— `metrics` í•„ë“œê°€ ì „í˜€ ì—†ìŒ

---

### 4. summarize_run.pyëŠ” metricsë¥¼ ì½ìœ¼ë ¤ í•˜ì§€ë§Œ ë°ì´í„°ê°€ ì—†ìŒ

**íŒŒì¼**: `scripts/summarize_run.py`

**ì½”ë“œ**:
```python
DEFAULT_MAIN_METRICS = [
    "faithfulness",
    "answer_relevance",
    "judge_total",
    "grounding",
    "completeness",
    "accuracy",
]

def collect_metric_values(records: Dict[Key, Dict[str, Any]], mode: str, metric: str) -> List[float]:
    vals: List[float] = []
    for k, rec in records.items():
        if k.mode != mode:
            continue
        v = extract_metric_value(rec, metric)  # events.jsonlì—ì„œ metrics í•„ë“œ ì½ê¸° ì‹œë„
        if v is None:
            continue  # âŒ metrics í•„ë“œê°€ ì—†ì–´ì„œ í•­ìƒ None ë°˜í™˜
        vals.append(v)
    return vals  # âŒ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
```

**ìƒíƒœ**: âœ… ì½”ë“œëŠ” ì˜¬ë°”ë¥´ê²Œ ì‘ì„±ë˜ì–´ ìˆìœ¼ë‚˜, ë°ì´í„°ê°€ ì—†ì–´ì„œ ë¹ˆ ê²°ê³¼ë§Œ ë°˜í™˜

---

## ğŸ“Š í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ì§€í‘œ

### 1. íš¨ìœ¨ì„± ì§€í‘œ (âœ… ì‚¬ìš© ê°€ëŠ¥)

- **ë¹„ìš©**: `usage.estimated_cost_usd`
- **ì‘ë‹µ ì‹œê°„**: `timing_ms.total`
- **ìºì‹œ íˆíŠ¸ìœ¨**: `metadata.cache_hit` (Agent ëª¨ë“œë§Œ)

**ìœ„ì¹˜**: `runs/2025-12-13_primary_v1/paper_assets/summary.json` â†’ `efficiency` ì„¹ì…˜

---

### 2. ë©€í‹°í„´ íŠ¹í™” ì§€í‘œ (âš ï¸ ë³„ë„ ê³„ì‚° í•„ìš”)

**íŒŒì¼**: `experiments/evaluation/multiturn_metrics.py`

**ì§€í‘œ**:
- `context_utilization`: ì´ì „ í„´ ì •ë³´ í™œìš©ë„
- `context_contradiction`: ì´ì „ ì •ë³´ì™€ì˜ ëª¨ìˆœë„
- `update_responsiveness`: ìƒˆ ì •ë³´ ë°˜ì˜ë„

**ìƒíƒœ**: âš ï¸ ì½”ë“œëŠ” ìˆìœ¼ë‚˜ ì‹¤í—˜ ëŸ¬ë„ˆì—ì„œ í˜¸ì¶œë˜ì§€ ì•ŠìŒ

---

### 3. RAGAS í‰ê°€ì§€í‘œ (âŒ ê³„ì‚°ë˜ì§€ ì•ŠìŒ)

**í•„ìš”í•œ ì§€í‘œ**:
- `faithfulness`: ê·¼ê±° ë¬¸ì„œì™€ì˜ ì¼ì¹˜ë„
- `answer_relevance`: ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±
- `context_precision`: ì»¨í…ìŠ¤íŠ¸ ì •ë°€ë„
- `context_recall`: ì»¨í…ìŠ¤íŠ¸ ì¬í˜„ìœ¨
- `context_relevancy`: ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±

**ìƒíƒœ**: âŒ ê³„ì‚°/ì €ì¥ ë¡œì§ì´ ì „í˜€ ì—†ìŒ

---

## ğŸ”§ í•´ê²° ë°©ì•ˆ

### ì˜µì…˜ 1: ì‹¤í—˜ ëŸ¬ë„ˆì— RAGAS í‰ê°€ í†µí•© (ê¶Œì¥)

**í•„ìš”í•œ ì‘ì—…**:

1. **RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**
   ```bash
   pip install ragas
   ```

2. **í‰ê°€ì§€í‘œ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€**
   ```python
   # experiments/evaluation/ragas_metrics.py (ìƒˆ íŒŒì¼)
   from ragas import evaluate
   from ragas.metrics import (
       faithfulness,
       answer_relevance,
       context_precision,
       context_recall,
       context_relevancy
   )
   from datasets import Dataset
   
   def calculate_ragas_metrics(
       question: str,
       answer: str,
       contexts: List[str],  # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
       ground_truth: Optional[str] = None
   ) -> Dict[str, float]:
       """RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°"""
       dataset = Dataset.from_dict({
           "question": [question],
           "answer": [answer],
           "contexts": [contexts],
           "ground_truth": [ground_truth] if ground_truth else [None]
       })
       
       result = evaluate(
           dataset,
           metrics=[
               faithfulness,
               answer_relevance,
               context_precision,
               context_recall,
               context_relevancy
           ]
       )
       
       return {
           "faithfulness": result["faithfulness"][0],
           "answer_relevance": result["answer_relevance"][0],
           "context_precision": result["context_precision"][0],
           "context_recall": result["context_recall"][0],
           "context_relevancy": result["context_relevancy"][0]
       }
   ```

3. **ì‹¤í—˜ ëŸ¬ë„ˆì— í†µí•©**
   ```python
   # experiments/run_multiturn_experiment_v2.py
   from experiments.evaluation.ragas_metrics import calculate_ragas_metrics
   
   def _log_event(self, ..., retrieved_docs: List[Dict] = None):
       # ... ê¸°ì¡´ ì½”ë“œ ...
       
       # RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
       metrics = None
       if self.config.get('evaluation', {}).get('per_turn_metrics'):
           contexts = [doc.get('text', '') for doc in (retrieved_docs or [])]
           try:
               metrics = calculate_ragas_metrics(
                   question=question_text,
                   answer=answer_text,
                   contexts=contexts
               )
           except Exception as e:
               logger.warning(f"RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
       
       event = {
           # ... ê¸°ì¡´ í•„ë“œë“¤ ...
           "metrics": metrics  # âœ… ì¶”ê°€
       }
   ```

---

### ì˜µì…˜ 2: ì‚¬í›„ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

**í•„ìš”í•œ ì‘ì—…**:

1. **ì‚¬í›„ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
   ```python
   # scripts/evaluate_ragas_metrics.py (ìƒˆ íŒŒì¼)
   import json
   from experiments.evaluation.ragas_metrics import calculate_ragas_metrics
   
   def evaluate_existing_events(events_jsonl_path: str, output_path: str):
       """ê¸°ì¡´ events.jsonlì— RAGAS ë©”íŠ¸ë¦­ ì¶”ê°€"""
       events = []
       with open(events_jsonl_path, 'r', encoding='utf-8') as f:
           for line in f:
               events.append(json.loads(line))
       
       # ê° ì´ë²¤íŠ¸ì— ëŒ€í•´ RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
       for event in events:
           # ê²€ìƒ‰ëœ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸° (retrieval_snapshot ë˜ëŠ” node_traceì—ì„œ)
           contexts = get_retrieved_contexts(event)
           
           metrics = calculate_ragas_metrics(
               question=event['question']['text'],
               answer=event['answer']['text'],
               contexts=contexts
           )
           
           event['metrics'] = metrics
       
       # ì—…ë°ì´íŠ¸ëœ events.jsonl ì €ì¥
       with open(output_path, 'w', encoding='utf-8') as f:
           for event in events:
               f.write(json.dumps(event, ensure_ascii=False) + '\n')
   ```

2. **ì‹¤í–‰**
   ```bash
   python scripts/evaluate_ragas_metrics.py \
     --events_path runs/2025-12-13_primary_v1/events.jsonl \
     --output_path runs/2025-12-13_primary_v1/events_with_metrics.jsonl
   ```

**ë‹¨ì **: ê²€ìƒ‰ëœ ë¬¸ì„œ(`contexts`) ì •ë³´ê°€ `events.jsonl`ì— ì—†ìœ¼ë©´ ê³„ì‚° ë¶ˆê°€

---

## ğŸ“ ìš”ì•½

### í˜„ì¬ ìƒíƒœ

| í•­ëª© | ìƒíƒœ | ì„¤ëª… |
|------|------|------|
| ì„¤ì • íŒŒì¼ | âœ… | `config.yaml`ì— í‰ê°€ì§€í‘œ ì„¤ì •ë¨ |
| ìŠ¤í‚¤ë§ˆ ì •ì˜ | âœ… | `events_record.schema.json`ì— metrics í•„ë“œ ì •ì˜ë¨ |
| ê³„ì‚° ë¡œì§ | âŒ | RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° ì½”ë“œ ì—†ìŒ |
| ì €ì¥ ë¡œì§ | âŒ | `_log_event()`ì—ì„œ metrics í•„ë“œ ì €ì¥ ì•ˆ í•¨ |
| ë°ì´í„° ì¡´ì¬ | âŒ | `events.jsonl`ì— metrics í•„ë“œ ì—†ìŒ |
| ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ | âœ… | `summarize_run.py`ëŠ” metricsë¥¼ ì½ì„ ì¤€ë¹„ë¨ |

### í•µì‹¬ ë¬¸ì œ

**RAGAS í‰ê°€ì§€í‘œê°€ ì„¤ì •ì€ ë˜ì–´ ìˆìœ¼ë‚˜, ì‹¤ì œë¡œ ê³„ì‚°í•˜ê³  ì €ì¥í•˜ëŠ” ì½”ë“œê°€ ì‹¤í—˜ ëŸ¬ë„ˆì— í†µí•©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.**

### í•´ê²° í•„ìš” ì‚¬í•­

1. âœ… RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
2. âœ… RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ ì‘ì„±
3. âœ… ì‹¤í—˜ ëŸ¬ë„ˆì— í†µí•© (ê° í„´ë§ˆë‹¤ ë©”íŠ¸ë¦­ ê³„ì‚° í›„ ì €ì¥)
4. âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ(`contexts`) ì •ë³´ë¥¼ ì´ë²¤íŠ¸ì— í¬í•¨

---

ì´ ë³´ê³ ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ RAGAS í‰ê°€ì§€í‘œ ê³„ì‚° ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì‹œê² ìŠµë‹ˆê¹Œ?

