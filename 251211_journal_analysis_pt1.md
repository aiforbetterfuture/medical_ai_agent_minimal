# ë…¼ë¬¸ ë¶„ì„ ë° ìŠ¤ìºí´ë“œ ê°œì„  ì „ëµ (Part 1)
## Journal Analysis & Scaffold Improvement Strategy

ì‘ì„±ì¼: 2024-12-11
ì—°êµ¬ ì£¼ì œ: **Context Engineering ê¸°ë°˜ ì˜í•™ì§€ì‹ AI Agent**

---

## ğŸ“‹ ëª©ì°¨

1. [ë…¼ë¬¸ ê°œìš”](#ë…¼ë¬¸-ê°œìš”)
2. [í˜„ì¬ ìŠ¤ìºí´ë“œ êµ¬ì¡° ë¶„ì„](#í˜„ì¬-ìŠ¤ìºí´ë“œ-êµ¬ì¡°-ë¶„ì„)
3. [í•µì‹¬ í‚¤ì›Œë“œë³„ ê°œì„  ì „ëµ](#í•µì‹¬-í‚¤ì›Œë“œë³„-ê°œì„ -ì „ëµ)
4. [êµ¬ì²´ì  êµ¬í˜„ ë¡œë“œë§µ](#êµ¬ì²´ì -êµ¬í˜„-ë¡œë“œë§µ)
5. [ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼](#ì˜ˆìƒ-ì„±ëŠ¥-ê°œì„ -íš¨ê³¼)

---

## ë…¼ë¬¸ ê°œìš”

### ğŸ“„ ë…¼ë¬¸ 1: Multi-Turn Interaction Capabilities of LLMs
**arXiv:2501.09959v1**

#### ì—°êµ¬ ëª©ì 
LLMì˜ ë©€í‹°í„´ ëŒ€í™” ëŠ¥ë ¥ì— ëŒ€í•œ í¬ê´„ì  ì„œë² ì´. ë‹¨ì¼í„´ ì²˜ë¦¬ê°€ ì•„ë‹Œ **ë‹¤ì¤‘ í„´ì— ê±¸ì¹œ ì¼ê´€ì„±, ë§¥ë½ ìœ ì§€, ë™ì  ì ì‘**ì— ì´ˆì .

#### í•µì‹¬ ë°œê²¬

##### 1. Context Memory Mechanisms
**External Memory (ì™¸ë¶€ ë©”ëª¨ë¦¬)** vs **Internal Memory (ë‚´ë¶€ ë©”ëª¨ë¦¬)**

| ë©”ì»¤ë‹ˆì¦˜ | ë°©ì‹ | ì¥ì  | ë‹¨ì  |
|---------|------|------|------|
| **HAT** (Hierarchical Aggregate Tree) | ëŒ€í™” ì´ë ¥ì„ ê³„ì¸µì  íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ê´€ë¦¬ | â€¢ íš¨ìœ¨ì  ê²€ìƒ‰<br>â€¢ í† í° ì ˆê°<br>â€¢ ë‹¤ì¸µ ì¶”ìƒí™” | â€¢ êµ¬í˜„ ë³µì¡ë„<br>â€¢ ì´ˆê¸° êµ¬ì¶• ë¹„ìš© |
| **CCM** (Compressed Context Memory) | LoRA ê¸°ë°˜ attention key-value ì••ì¶• | â€¢ ë™ì  ì••ì¶•<br>â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨<br>â€¢ ìºì‹œ ê°€ëŠ¥ | â€¢ ëª¨ë¸ ìˆ˜ì • í•„ìš”<br>â€¢ í•™ìŠµ í•„ìš” |
| **RSum** (Sequential Summarization) | ìˆœì°¨ì  ëŒ€í™” ìš”ì•½ | â€¢ ê°„ë‹¨í•œ êµ¬í˜„<br>â€¢ í† í° ì ˆê°<br>â€¢ ì¼ê´€ì„± ìœ ì§€ | â€¢ ì •ë³´ ì†ì‹¤<br>â€¢ ìš”ì•½ í’ˆì§ˆ ì˜ì¡´ |
| **Hash-based Memory** (Think-in-Memory) | O(1) ê²€ìƒ‰ ê°€ëŠ¥í•œ í•´ì‹œ ì €ì¥ | â€¢ ë¹ ë¥¸ ê²€ìƒ‰<br>â€¢ í™•ì¥ ê°€ëŠ¥ | â€¢ ì •í™•í•œ ë§¤ì¹­ë§Œ<br>â€¢ ì˜ë¯¸ì  ê²€ìƒ‰ ë¶ˆê°€ |

##### 2. Multi-Turn Instruction Following Patterns
ì‚¬ìš©ì-LLM ìƒí˜¸ì‘ìš©ì˜ 5ê°€ì§€ íŒ¨í„´:

1. **Instruction Clarification** (ëª…í™•í™”): ëª¨í˜¸í•œ ìš”ì²­ì— ëŒ€í•œ ì§ˆë¬¸
2. **Expansion** (í™•ì¥): ì´ì „ ì‘ë‹µì˜ ìƒì„¸í™”
3. **Constraint Addition** (ì œì•½ ì¶”ê°€): ì¶”ê°€ ì¡°ê±´ ëª…ì‹œ
4. **Refinement** (ì •ì œ): ê¸°ì¡´ ë‹µë³€ ê°œì„  ìš”ì²­
5. **Global Consistency** (ì „ì—­ ì¼ê´€ì„±): ì „ì²´ ëŒ€í™” ë§¥ë½ ìœ ì§€

##### 3. Planning Mechanisms

**Dialogue Planning**:
- GDP-Zero: Monte Carlo Tree Search (MCTS) ê¸°ë°˜
- Dual-Process (DPDP): System 1/2 ì´ì¤‘ ì²˜ë¦¬
- Policy Gradient: ëŒ€í™” íë¦„ ìµœì í™”

**Agent Planning**:
- ToolPlanner: ë‹¤ë‹¨ê³„ ë„êµ¬ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- Self-MAP: ë©”ëª¨ë¦¬ ì¦ê°• ê³„íš + ìê¸° ë°˜ì„±

##### 4. Multi-Turn Reasoning
- **Self-Correction**: í”¼ë“œë°± ê¸°ë°˜ ë‹µë³€ ìˆ˜ì •
- **Reflexion**: í™˜ê²½ í”¼ë“œë°±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- **RISE**: MDP ê¸°ë°˜ ë°˜ë³µ ê°œì„ 

##### 5. í•µì‹¬ ì¸ì‚¬ì´íŠ¸

> "External memory mechanisms significantly enhance LLMs' ability to maintain continuity **while implicitly reducing token overhead** versus full-context approaches."

> "LLMs show only slight improvements over the random agent in strategic multi-turn games, revealing deficiencies in complex reasoning."

> "Long-context LLMs and RAG models still significantly lag behind human performance on long-horizon dialogues (600+ turns)."

---

### ğŸ“„ ë…¼ë¬¸ 2: Personalization of Large Language Models
**arXiv:2411.00027**

#### ì—°êµ¬ ëª©ì 
LLM ê°œì¸í™” ê¸°ë²•ì˜ ì²´ê³„ì  ë¶„ë¥˜ ë° í†µí•©. **ì‚¬ìš©ìë³„ ë§ì¶¤í˜• ì‘ë‹µ** ìƒì„±ì„ ìœ„í•œ taxonomy ì œì‹œ.

#### í•µì‹¬ ë°œê²¬

##### 1. Personalization Granularity (ê°œì¸í™” ì„¸ë¶„ì„±)

| ìˆ˜ì¤€ | ë²”ìœ„ | ì ìš© ì˜ˆì‹œ | ìš°ë¦¬ ìŠ¤ìºí´ë“œ ì ìš© |
|------|------|----------|------------------|
| **User-level** | ê°œë³„ ì‚¬ìš©ì ì „ì²´ | í™˜ì í”„ë¡œí•„, ì„ í˜¸ ìŠ¤íƒ€ì¼ | âœ… ProfileStore |
| **Session-level** | íŠ¹ì • ëŒ€í™” ì„¸ì…˜ | í˜„ì¬ ì¦ìƒ, ì„ì‹œ ì •ë³´ | âœ… Conversation history |
| **Turn-level** | ê°œë³„ ë°œí™” | ì‹¤ì‹œê°„ ê°ì •, ê¸´ê¸‰ë„ | âŒ ë¯¸êµ¬í˜„ |
| **Token-level** | í† í° ë‹¨ìœ„ | ì˜í•™ ìš©ì–´ ë‚œì´ë„ ì¡°ì ˆ | âŒ ë¯¸êµ¬í˜„ |

##### 2. Personalization Techniques

**ë©”ëª¨ë¦¬ ê¸°ë°˜ ì ‘ê·¼**:
- Explicit Memory: ëª…ì‹œì  ì‚¬ìš©ì ì •ë³´ ì €ì¥
- Implicit Memory: í–‰ë™ íŒ¨í„´ì—ì„œ ì¶”ë¡ 
- Hybrid: ëª…ì‹œ+ì•”ë¬µ ê²°í•©

**í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ì ‘ê·¼**:
- Few-shot Personalization: ì‚¬ìš©ì ì˜ˆì‹œ í¬í•¨
- Template Personalization: ë§ì¶¤í˜• í…œí”Œë¦¿
- Dynamic Prompting: ì‹¤ì‹œê°„ í”„ë¡¬í”„íŠ¸ ì¡°ì •

**ëª¨ë¸ ê¸°ë°˜ ì ‘ê·¼**:
- Fine-tuning: ì‚¬ìš©ìë³„ ëª¨ë¸ ë¯¸ì„¸ì¡°ì •
- LoRA Adapters: ê²½ëŸ‰ ê°œì¸í™”
- Mixture of Experts: ì „ë¬¸ê°€ í˜¼í•© ëª¨ë¸

##### 3. í•µì‹¬ ì¸ì‚¬ì´íŠ¸

> "Understanding personalization techniques helps identify **efficient context inclusion strategies** and informs what user-specific data warrants cached storage."

> "Personalization taxonomies enable **maintaining user-specific conversation patterns** across extended interactions."

---

## í˜„ì¬ ìŠ¤ìºí´ë“œ êµ¬ì¡° ë¶„ì„

### ì•„í‚¤í…ì²˜ ê°œìš”

```
[User Input]
     â†“
[check_similarity] â† ìºì‹œ í™•ì¸ (ì‹ ê·œ)
     â†“
  <Cache Hit?>
   Yes â†“    No â†“
[store_response] [extract_slots] â†’ [store_memory] â†’ [assemble_context]
     â†“              â†“
   [END]        [retrieve] â†’ [generate_answer] â†’ [refine] â†’ [quality_check]
                                                                  â†“
                                                            [store_response]
                                                                  â†“
                                                                [END]
```

### í˜„ì¬ êµ¬í˜„ ìˆ˜ì¤€

#### âœ… ì˜ êµ¬í˜„ëœ ë¶€ë¶„

1. **ìŠ¬ë¡¯ ì¶”ì¶œ** (extract_slots)
   - MedCAT2 ê¸°ë°˜ ì˜ë£Œ ì—”í‹°í‹° ì¶”ì¶œ
   - 6ê°œ ìŠ¬ë¡¯ êµ¬ì¡°í™” (demographics, conditions, symptoms, vitals, labs, medications)
   - ì •ê·œí‘œí˜„ì‹ ë³´ì™„

2. **ì¥ê¸° ë©”ëª¨ë¦¬** (store_memory)
   - ProfileStoreë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ì €ì¥
   - ì‹œê³„ì—´ ê°€ì¤‘ì¹˜ (temporal weights)
   - í”„ë¡œí•„ ìš”ì•½ ìƒì„±

3. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰** (retrieve)
   - BM25 + FAISS ìœµí•©
   - RRF (Reciprocal Rank Fusion)
   - ì ì‘í˜• ì¬ê²€ìƒ‰

4. **ì‘ë‹µ ìºì‹œ** (check_similarity, store_response)
   - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ (85% ì„ê³„ê°’)
   - LRU ìºì‹± (100ê°œ)
   - ë¬¸ì²´ ë³€í˜•

5. **Self-Refine** (refine, quality_check)
   - í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì¬ê²€ìƒ‰
   - ìµœëŒ€ 2íšŒ ë°˜ë³µ

#### âš ï¸ ê°œì„  í•„ìš” ë¶€ë¶„

1. **ë©€í‹°í„´ ëŒ€í™” ê´€ë¦¬**
   - âŒ ë‹¨ìˆœ ë¬¸ìì—´ ì—°ê²° (`conversation_history`)
   - âŒ ê³„ì¸µì  êµ¬ì¡° ì—†ìŒ
   - âŒ ì°¸ì¡° í•´ê²° (reference resolution) ë¯¸êµ¬í˜„
   - âŒ ëŒ€í™” íë¦„ ê³„íš ì—†ìŒ

2. **ì»¨í…ìŠ¤íŠ¸ ë©”ëª¨ë¦¬**
   - âŒ ëŒ€í™” ì´ë ¥ ì••ì¶• ë¯¸í¡
   - âŒ ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒì  í¬í•¨ ì—†ìŒ
   - âŒ í† í° ì˜ˆì‚° ë™ì  ì¡°ì • ë¶€ì¡±

3. **ê°œì¸í™” ìˆ˜ì¤€**
   - âš ï¸ User-levelë§Œ êµ¬í˜„ (ProfileStore)
   - âŒ Turn-level ê°œì¸í™” ì—†ìŒ
   - âŒ ì‹¤ì‹œê°„ ê°ì •/ê¸´ê¸‰ë„ ë¯¸ë°˜ì˜

4. **ìºì‹œ ì „ëµ**
   - âš ï¸ Response cacheë§Œ ì¡´ì¬
   - âŒ Context cache ì—†ìŒ
   - âŒ Retrieval result cache ì—†ìŒ

5. **ë©€í‹°í„´ ì¶”ë¡ **
   - âŒ ëª…í™•í™” ì§ˆë¬¸ ê¸°ëŠ¥ ì—†ìŒ
   - âŒ ëŒ€í™” ë§¥ë½ ê¸°ë°˜ ì¿¼ë¦¬ ê°œì„  ë¯¸í¡
   - âŒ í”¼ë“œë°± í†µí•© ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¡±

---

## í•µì‹¬ í‚¤ì›Œë“œë³„ ê°œì„  ì „ëµ

### ğŸ”„ 1. ë©€í‹°í„´ ëŒ€í™” (Multi-Turn Dialogue)

#### í˜„ì¬ ìƒíƒœ
```python
# app.py
conversation_history = format_conversation_history(
    st.session_state.messages[:-1]
)

def format_conversation_history(messages: list) -> str:
    history_lines = []
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        if role == "user":
            history_lines.append(f"ì‚¬ìš©ì: {content}")
        elif role == "assistant":
            history_lines.append(f"AI: {content}")
    return "\n".join(history_lines)
```

**ë¬¸ì œì **:
- ë‹¨ìˆœ ë¬¸ìì—´ ì—°ê²°ë¡œ í† í° ë‚­ë¹„
- ì¤‘ìš”ë„ êµ¬ë¶„ ì—†ìŒ
- ê²€ìƒ‰ ë¶ˆê°€ëŠ¥

#### ê°œì„  ì „ëµ: Hierarchical Aggregate Tree (HAT) êµ¬í˜„

**HAT êµ¬ì¡°**:
```python
class DialogueTurn:
    """ê°œë³„ ëŒ€í™” í„´"""
    turn_id: int
    timestamp: datetime
    user_query: str
    assistant_response: str
    extracted_slots: Dict
    importance_score: float  # 0-1
    summary: str  # ìš”ì•½

class DialogueSession:
    """ëŒ€í™” ì„¸ì…˜"""
    session_id: str
    turns: List[DialogueTurn]
    session_summary: str  # ì „ì²´ ìš”ì•½
    key_topics: List[str]  # ì£¼ìš” ì£¼ì œ

class HierarchicalDialogueTree:
    """ê³„ì¸µì  ëŒ€í™” íŠ¸ë¦¬"""

    def __init__(self):
        self.root = DialogueSession()
        self.turn_index = {}  # ë¹ ë¥¸ ê²€ìƒ‰ìš©

    def add_turn(self, turn: DialogueTurn):
        """í„´ ì¶”ê°€ + ì¤‘ìš”ë„ ê³„ì‚°"""
        turn.importance_score = self._calculate_importance(turn)
        self.root.turns.append(turn)
        self.turn_index[turn.turn_id] = turn

        # 5í„´ë§ˆë‹¤ ìš”ì•½ ì—…ë°ì´íŠ¸
        if len(self.root.turns) % 5 == 0:
            self._update_session_summary()

    def _calculate_importance(self, turn: DialogueTurn) -> float:
        """ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0

        # 1. ìŠ¬ë¡¯ ì •ë³´ í¬í•¨ ì—¬ë¶€ (0.4)
        if turn.extracted_slots:
            score += 0.4 * len(turn.extracted_slots) / 6

        # 2. ì˜ë£Œ ì—”í‹°í‹° ë°€ë„ (0.3)
        medical_entities = count_medical_entities(turn.user_query)
        score += 0.3 * min(medical_entities / 5, 1.0)

        # 3. ì‘ë‹µ ê¸¸ì´ (0.2)
        response_length = len(turn.assistant_response)
        score += 0.2 * min(response_length / 500, 1.0)

        # 4. ì‹œê°„ ê°€ì¤‘ì¹˜ (0.1) - ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ
        recency = 1.0 - (time.time() - turn.timestamp) / 3600
        score += 0.1 * max(recency, 0)

        return min(score, 1.0)

    def get_context(self, max_tokens: int = 2000) -> str:
        """í† í° ì˜ˆì‚° ë‚´ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        current_tokens = 0

        # 1. ì„¸ì…˜ ìš”ì•½ (í•­ìƒ í¬í•¨)
        if self.root.session_summary:
            summary_tokens = estimate_tokens(self.root.session_summary)
            context_parts.append(f"## ëŒ€í™” ìš”ì•½\n{self.root.session_summary}")
            current_tokens += summary_tokens

        # 2. ì¤‘ìš”ë„ ìˆœ ì •ë ¬
        sorted_turns = sorted(
            self.root.turns,
            key=lambda t: t.importance_score,
            reverse=True
        )

        # 3. ì˜ˆì‚° ë‚´ í¬í•¨
        for turn in sorted_turns:
            turn_text = f"Turn {turn.turn_id}:\nQ: {turn.user_query}\nA: {turn.summary or turn.assistant_response[:200]}"
            turn_tokens = estimate_tokens(turn_text)

            if current_tokens + turn_tokens <= max_tokens:
                context_parts.append(turn_text)
                current_tokens += turn_tokens
            else:
                break

        return "\n\n".join(context_parts)

    def _update_session_summary(self):
        """ì„¸ì…˜ ìš”ì•½ ì—…ë°ì´íŠ¸ (LLM ì‚¬ìš©)"""
        recent_turns = self.root.turns[-5:]
        turn_summaries = [t.summary or t.user_query for t in recent_turns]

        # LLMìœ¼ë¡œ ìš”ì•½ ìƒì„±
        summary_prompt = f"""ë‹¤ìŒ ëŒ€í™”ë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”:

{chr(10).join(turn_summaries)}

í•µì‹¬ ì˜ë£Œ ì •ë³´ì™€ í™˜ì ìƒíƒœë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”."""

        self.root.session_summary = llm_client.generate(summary_prompt)
```

**í† í° ì ˆê° ê³„ì‚°**:
```
ê¸°ì¡´ ë°©ì‹ (10í„´):
- í‰ê·  ì§ˆë¬¸: 50 í† í°
- í‰ê·  ë‹µë³€: 200 í† í°
- ì´: 10 Ã— (50 + 200) = 2,500 í† í°

HAT ë°©ì‹:
- ì„¸ì…˜ ìš”ì•½: 100 í† í°
- ì¤‘ìš” í„´ 5ê°œ (ìš”ì•½): 5 Ã— 50 = 250 í† í°
- ì´: 350 í† í°

ì ˆê°ë¥ : (2,500 - 350) / 2,500 = 86%
```

#### ê°œì„  ì „ëµ: Reference Resolution (ì°¸ì¡° í•´ê²°)

```python
class ReferenceResolver:
    """ëŒ€ëª…ì‚¬ ë° ì°¸ì¡° í•´ê²°"""

    def __init__(self):
        self.entity_tracker = {}  # ìµœê·¼ ì–¸ê¸‰ëœ ì—”í‹°í‹°

    def resolve(self, query: str, history: HierarchicalDialogueTree) -> str:
        """ì°¸ì¡° í•´ê²°ëœ ì¿¼ë¦¬ ë°˜í™˜"""
        resolved = query

        # 1. ëŒ€ëª…ì‚¬ íƒì§€
        pronouns = ["ê·¸ê²ƒ", "ê·¸ê±°", "ì´ê²ƒ", "ì €ê²ƒ", "ê·¸ ì¦ìƒ", "ìœ„ ì•½"]

        for pronoun in pronouns:
            if pronoun in resolved:
                # ìµœê·¼ ì—”í‹°í‹°ì—ì„œ ì°¾ê¸°
                recent_entity = self._find_recent_entity(
                    pronoun_type=self._classify_pronoun(pronoun),
                    history=history
                )
                if recent_entity:
                    resolved = resolved.replace(pronoun, recent_entity)

        # 2. "ì´ì „ì—", "ì•„ê¹Œ" ë“± ì‹œê°„ ì°¸ì¡°
        time_refs = ["ì´ì „ì—", "ì•„ê¹Œ", "ë°©ê¸ˆ"]
        for ref in time_refs:
            if ref in resolved:
                recent_context = history.root.turns[-2].user_query if len(history.root.turns) > 1 else ""
                resolved += f" (ì°¸ê³ : {recent_context})"

        return resolved

    def _classify_pronoun(self, pronoun: str) -> str:
        """ëŒ€ëª…ì‚¬ ìœ í˜• ë¶„ë¥˜"""
        if "ì¦ìƒ" in pronoun:
            return "symptom"
        elif "ì•½" in pronoun:
            return "medication"
        else:
            return "general"

    def _find_recent_entity(self, pronoun_type: str, history: HierarchicalDialogueTree) -> str:
        """ìµœê·¼ í•´ë‹¹ ìœ í˜•ì˜ ì—”í‹°í‹° ì°¾ê¸°"""
        for turn in reversed(history.root.turns):
            if pronoun_type == "symptom" and turn.extracted_slots.get("symptoms"):
                return turn.extracted_slots["symptoms"][0]["name"]
            elif pronoun_type == "medication" and turn.extracted_slots.get("medications"):
                return turn.extracted_slots["medications"][0]["name"]
        return None
```

**íš¨ê³¼**:
```
Before: "ê·¸ê²ƒ ë•Œë¬¸ì— ê±±ì •ë¼ìš”"
After: "ë‹¹ë‡¨ë³‘ ë•Œë¬¸ì— ê±±ì •ë¼ìš”"

â†’ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
â†’ LLM ì´í•´ë„ í–¥ìƒ
```

---

### ğŸ§  2. ë¡±ë©”ëª¨ë¦¬ (Long-Term Memory)

#### í˜„ì¬ ìƒíƒœ
```python
# memory/profile_store.py
class ProfileStore:
    def __init__(self):
        self.ltm = LongTermMemory()

    def update_slots(self, slot_out: Dict):
        """ìŠ¬ë¡¯ ì—…ë°ì´íŠ¸"""
        # ê°„ë‹¨í•œ append

    def apply_temporal_weights(self):
        """ì‹œê³„ì—´ ê°€ì¤‘ì¹˜"""
        for item in self.ltm.conditions:
            age_hours = (current_time - item.timestamp) / 3600
            item.weight = exp(-0.1 * age_hours)
```

**ë¬¸ì œì **:
- ë‹¨ìˆœ ì‹œê°„ ê°ì‡ ë§Œ ì ìš©
- ì¥ê¸° íŒ¨í„´ ë¯¸í•™ìŠµ
- ìš”ì•½ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¡±

#### ê°œì„  ì „ëµ: Sequential Summarization (RSum)

```python
class SequentialMemorySummarizer:
    """ìˆœì°¨ì  ë©”ëª¨ë¦¬ ìš”ì•½ê¸°"""

    def __init__(self, max_memory_size: int = 50):
        self.max_size = max_memory_size
        self.memory_chunks = []  # ë©”ëª¨ë¦¬ ì²­í¬
        self.summaries = []  # ê° ì²­í¬ì˜ ìš”ì•½

    def add_information(self, info: Dict):
        """ì •ë³´ ì¶”ê°€"""
        self.memory_chunks.append(info)

        # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ìš”ì•½
        if len(self.memory_chunks) >= 10:
            self._summarize_chunk()

    def _summarize_chunk(self):
        """ì²­í¬ ìš”ì•½"""
        chunk_to_summarize = self.memory_chunks[-10:]

        # ì˜ë£Œ ì •ë³´ ì§‘ê³„
        summary = self._aggregate_medical_info(chunk_to_summarize)

        # ìš”ì•½ ì €ì¥
        self.summaries.append({
            'timestamp': time.time(),
            'summary': summary,
            'original_count': len(chunk_to_summarize)
        })

        # ì›ë³¸ ì œê±° (ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´)
        self.memory_chunks = self.memory_chunks[:-10]

    def _aggregate_medical_info(self, chunk: List[Dict]) -> str:
        """ì˜ë£Œ ì •ë³´ ì§‘ê³„"""
        # 1. ê°€ì¥ ë¹ˆë²ˆí•œ ì¦ìƒ
        symptom_counter = Counter()
        for item in chunk:
            if 'symptoms' in item:
                for symptom in item['symptoms']:
                    symptom_counter[symptom['name']] += 1

        top_symptoms = symptom_counter.most_common(3)

        # 2. ì§„ë‹¨ ë³€í™” ì¶”ì 
        conditions = []
        for item in chunk:
            if 'conditions' in item:
                conditions.extend([c['name'] for c in item['conditions']])

        # 3. ìˆ˜ì¹˜ íŠ¸ë Œë“œ
        vitals_trend = self._calculate_trend(chunk, 'vitals')

        # 4. ìš”ì•½ ìƒì„±
        summary = f"""
### ìµœê·¼ 10íšŒ ìƒí˜¸ì‘ìš© ìš”ì•½
- ì£¼ìš” ì¦ìƒ: {', '.join([s[0] for s in top_symptoms])}
- ì§„ë‹¨: {', '.join(set(conditions))}
- í™œë ¥ì§•í›„ íŠ¸ë Œë“œ: {vitals_trend}
"""
        return summary.strip()

    def _calculate_trend(self, chunk: List[Dict], field: str) -> str:
        """ìˆ˜ì¹˜ íŠ¸ë Œë“œ ê³„ì‚°"""
        values = []
        for item in chunk:
            if field in item:
                for vital in item[field]:
                    if vital['name'] == 'SBP':  # ìˆ˜ì¶•ê¸° í˜ˆì•• ì˜ˆì‹œ
                        values.append(vital['value'])

        if len(values) < 2:
            return "ë°ì´í„° ë¶€ì¡±"

        # ì„ í˜• íšŒê·€ë¡œ íŠ¸ë Œë“œ ê³„ì‚°
        trend = (values[-1] - values[0]) / len(values)
        if trend > 0:
            return f"ìƒìŠ¹ ì¶”ì„¸ (+{trend:.1f}/íšŒ)"
        else:
            return f"í•˜ë½ ì¶”ì„¸ ({trend:.1f}/íšŒ)"

    def get_compressed_memory(self, max_tokens: int = 1000) -> str:
        """ì••ì¶•ëœ ë©”ëª¨ë¦¬ ë°˜í™˜"""
        context = []
        current_tokens = 0

        # 1. ìµœê·¼ ìš”ì•½ë“¤ í¬í•¨
        for summary in reversed(self.summaries):
            summary_tokens = estimate_tokens(summary['summary'])
            if current_tokens + summary_tokens <= max_tokens:
                context.append(summary['summary'])
                current_tokens += summary_tokens
            else:
                break

        # 2. ìµœê·¼ ì›ë³¸ ì •ë³´
        for item in reversed(self.memory_chunks):
            item_text = self._format_item(item)
            item_tokens = estimate_tokens(item_text)
            if current_tokens + item_tokens <= max_tokens:
                context.append(item_text)
                current_tokens += item_tokens
            else:
                break

        return "\n\n".join(reversed(context))

    def _format_item(self, item: Dict) -> str:
        """ì•„ì´í…œ í¬ë§·íŒ…"""
        parts = []
        if 'symptoms' in item:
            parts.append(f"ì¦ìƒ: {', '.join([s['name'] for s in item['symptoms']])}")
        if 'conditions' in item:
            parts.append(f"ì§„ë‹¨: {', '.join([c['name'] for c in item['conditions']])}")
        if 'medications' in item:
            parts.append(f"ì•½ë¬¼: {', '.join([m['name'] for m in item['medications']])}")
        return " | ".join(parts)
```

**ë©”ëª¨ë¦¬ ì ˆê° ê³„ì‚°**:
```
ê¸°ì¡´ ë°©ì‹ (50ê°œ ìƒí˜¸ì‘ìš©):
- ê° ìƒí˜¸ì‘ìš©: í‰ê·  100 í† í°
- ì´: 50 Ã— 100 = 5,000 í† í°

RSum ë°©ì‹:
- ìš”ì•½ 5ê°œ (10ê°œì”© ë¬¶ìŒ): 5 Ã— 150 = 750 í† í°
- ì´: 750 í† í°

ì ˆê°ë¥ : (5,000 - 750) / 5,000 = 85%
```

#### ê°œì„  ì „ëµ: Graph-based Memory

```python
class GraphMemoryStore:
    """Neo4j ê¸°ë°˜ ê·¸ë˜í”„ ë©”ëª¨ë¦¬"""

    def __init__(self):
        self.graph = nx.DiGraph()  # ì¼ë‹¨ NetworkXë¡œ í”„ë¡œí† íƒ€ì…

    def add_medical_fact(self, fact: Dict):
        """ì˜ë£Œ ì‚¬ì‹¤ ì¶”ê°€"""
        # ë…¸ë“œ ì¶”ê°€
        if 'condition' in fact:
            self.graph.add_node(
                fact['condition'],
                type='condition',
                timestamp=time.time()
            )

        if 'symptom' in fact:
            self.graph.add_node(
                fact['symptom'],
                type='symptom',
                timestamp=time.time()
            )

            # ê´€ê³„ ì¶”ê°€
            if 'condition' in fact:
                self.graph.add_edge(
                    fact['condition'],
                    fact['symptom'],
                    relation='HAS_SYMPTOM',
                    strength=fact.get('confidence', 1.0)
                )

    def query_related(self, entity: str, max_hops: int = 2) -> List[str]:
        """ê´€ë ¨ ì—”í‹°í‹° ì¿¼ë¦¬"""
        if entity not in self.graph:
            return []

        # BFSë¡œ ê´€ë ¨ ë…¸ë“œ ì°¾ê¸°
        related = []
        visited = set()
        queue = [(entity, 0)]

        while queue:
            node, depth = queue.pop(0)
            if depth > max_hops:
                continue

            if node in visited:
                continue
            visited.add(node)

            if node != entity:
                related.append(node)

            # ì´ì›ƒ ë…¸ë“œ ì¶”ê°€
            for neighbor in self.graph.neighbors(node):
                queue.append((neighbor, depth + 1))

        return related

    def get_context_subgraph(self, entities: List[str]) -> str:
        """ì—”í‹°í‹° ê´€ë ¨ ì„œë¸Œê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸"""
        subgraph_nodes = set()

        for entity in entities:
            related = self.query_related(entity, max_hops=1)
            subgraph_nodes.update(related)
            subgraph_nodes.add(entity)

        # ì„œë¸Œê·¸ë˜í”„ ì¶”ì¶œ
        subgraph = self.graph.subgraph(subgraph_nodes)

        # í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        context = []
        for edge in subgraph.edges(data=True):
            source, target, data = edge
            context.append(f"{source} --[{data['relation']}]--> {target}")

        return "\n".join(context)
```

**íš¨ê³¼**:
- ë³µì¡í•œ ì˜ë£Œ ê´€ê³„ ëª¨ë¸ë§
- ì¶”ë¡  ê°€ëŠ¥ (A â†’ B, B â†’ C âˆ´ A â†’ C)
- ë§ì¶¤í˜• ê²€ìƒ‰ ê°•í™”

---

### ğŸ’¾ 3. ìºì‹œ ë° í† í° ì†Œëª¨ ìµœì í™”

#### í˜„ì¬ ìƒíƒœ
```python
# memory/response_cache.py (ì‹ ê·œ)
class ResponseCache:
    """ì‘ë‹µ ìºì‹±"""
    # ì´ë¯¸ êµ¬í˜„ë¨ âœ…
```

**ë¶€ì¡±í•œ ë¶€ë¶„**:
- Context cache ì—†ìŒ
- Retrieval result cache ì—†ìŒ
- Embedding cache ê¸°ë³¸ì 

#### ê°œì„  ì „ëµ: Multi-Level Caching

```python
class MultiLevelCache:
    """ë‹¤ì¸µ ìºì‹œ ì‹œìŠ¤í…œ"""

    def __init__(self):
        # Level 1: Response Cache (ì´ë¯¸ êµ¬í˜„ë¨)
        self.response_cache = ResponseCache()

        # Level 2: Context Cache
        self.context_cache = LRUCache(max_size=200)

        # Level 3: Retrieval Cache
        self.retrieval_cache = LRUCache(max_size=500)

        # Level 4: Embedding Cache
        self.embedding_cache = LRUCache(max_size=1000)

    def get_or_compute_context(
        self,
        profile: Dict,
        history: HierarchicalDialogueTree,
        max_tokens: int
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ ë˜ëŠ” ê³„ì‚°"""
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_context_key(profile, history, max_tokens)

        # ìºì‹œ í™•ì¸
        if cache_key in self.context_cache:
            print("[Context Cache Hit]")
            return self.context_cache[cache_key]

        # ê³„ì‚°
        context = self._assemble_context(profile, history, max_tokens)

        # ìºì‹œ ì €ì¥
        self.context_cache[cache_key] = context

        return context

    def _generate_context_key(
        self,
        profile: Dict,
        history: HierarchicalDialogueTree,
        max_tokens: int
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ìºì‹œ í‚¤ ìƒì„±"""
        # í•´ì‹œ ê¸°ë°˜ í‚¤
        key_parts = [
            hashlib.md5(json.dumps(profile, sort_keys=True).encode()).hexdigest(),
            hashlib.md5(history.root.session_summary.encode()).hexdigest(),
            str(max_tokens)
        ]
        return ":".join(key_parts)

    def get_or_retrieve(
        self,
        query: str,
        k: int = 8
    ) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ë˜ëŠ” ê²€ìƒ‰"""
        # ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = self._normalize_query(query)
        cache_key = f"{normalized_query}:{k}"

        # ìºì‹œ í™•ì¸
        if cache_key in self.retrieval_cache:
            print("[Retrieval Cache Hit]")
            return self.retrieval_cache[cache_key]

        # ê²€ìƒ‰ ì‹¤í–‰
        results = self._perform_retrieval(query, k)

        # ìºì‹œ ì €ì¥
        self.retrieval_cache[cache_key] = results

        return results

    def get_or_embed(self, text: str) -> np.ndarray:
        """ì„ë² ë”© ìºì‹œ ë˜ëŠ” ê³„ì‚°"""
        cache_key = hashlib.md5(text.encode()).hexdigest()

        if cache_key in self.embedding_cache:
            print("[Embedding Cache Hit]")
            return self.embedding_cache[cache_key]

        # ì„ë² ë”© ê³„ì‚°
        embedding = llm_client.embed(text)

        # ìºì‹œ ì €ì¥
        self.embedding_cache[cache_key] = embedding

        return embedding
```

**ìºì‹œ íš¨ìœ¨ì„± ê³„ì‚°**:

| ìºì‹œ ìœ í˜• | íˆíŠ¸ìœ¨ | ì ˆê° ì‹œê°„ | ì ˆê° í† í° |
|---------|-------|----------|----------|
| Response Cache | 30% | 2.3ì´ˆ | 950 |
| Context Cache | 40% | 0.1ì´ˆ | 500 |
| Retrieval Cache | 50% | 0.3ì´ˆ | 150 |
| Embedding Cache | 70% | 0.05ì´ˆ | 10 |

**ì´ íš¨ê³¼ (100 ì¿¼ë¦¬ ê¸°ì¤€)**:
- ì‹œê°„ ì ˆê°: 30Ã—2.3 + 40Ã—0.1 + 50Ã—0.3 + 70Ã—0.05 = 88.5ì´ˆ
- í† í° ì ˆê°: 30Ã—950 + 40Ã—500 + 50Ã—150 + 70Ã—10 = 56,700 í† í°
- ë¹„ìš© ì ˆê°: 56,700 Ã— $0.00001 = $0.57

#### ê°œì„  ì „ëµ: Adaptive Token Budget

```python
class AdaptiveTokenBudget:
    """ì ì‘í˜• í† í° ì˜ˆì‚° ê´€ë¦¬ì"""

    def __init__(self):
        self.base_budget = 4000  # ê¸°ë³¸ ì˜ˆì‚°
        self.min_budget = 1000   # ìµœì†Œ ì˜ˆì‚°
        self.max_budget = 8000   # ìµœëŒ€ ì˜ˆì‚°

    def calculate_budget(
        self,
        query_complexity: float,  # 0-1
        conversation_length: int,
        user_urgency: float  # 0-1
    ) -> Dict[str, int]:
        """ë™ì  ì˜ˆì‚° í• ë‹¹"""

        # 1. ê¸°ë³¸ ì˜ˆì‚°
        total_budget = self.base_budget

        # 2. ë³µì¡ë„ì— ë”°ë¥¸ ì¡°ì •
        if query_complexity > 0.7:
            total_budget = int(total_budget * 1.5)
        elif query_complexity < 0.3:
            total_budget = int(total_budget * 0.7)

        # 3. ëŒ€í™” ê¸¸ì´ì— ë”°ë¥¸ ì¡°ì •
        if conversation_length > 10:
            total_budget = int(total_budget * 1.2)

        # 4. ê¸´ê¸‰ë„ì— ë”°ë¥¸ ì¡°ì • (ê¸´ê¸‰í•˜ë©´ í† í° ì¤„ì„)
        if user_urgency > 0.8:
            total_budget = int(total_budget * 0.8)

        # 5. ë²”ìœ„ ì œí•œ
        total_budget = max(self.min_budget, min(total_budget, self.max_budget))

        # 6. êµ¬ì„± ìš”ì†Œë³„ í• ë‹¹
        allocation = {
            'system_prompt': int(total_budget * 0.15),  # 15%
            'user_context': int(total_budget * 0.25),   # 25%
            'conversation_history': int(total_budget * 0.20),  # 20%
            'profile': int(total_budget * 0.15),        # 15%
            'evidence': int(total_budget * 0.20),       # 20%
            'buffer': int(total_budget * 0.05)          # 5% ì—¬ìœ 
        }

        return allocation

    def optimize_content(
        self,
        content: str,
        allocated_tokens: int
    ) -> str:
        """í• ë‹¹ëœ í† í°ì— ë§ê²Œ ì»¨í…ì¸  ìµœì í™”"""
        current_tokens = estimate_tokens(content)

        if current_tokens <= allocated_tokens:
            return content

        # ì´ˆê³¼ ì‹œ ì••ì¶•
        compression_ratio = allocated_tokens / current_tokens

        if compression_ratio > 0.7:
            # ê²½ë¯¸í•œ ì´ˆê³¼: ë§ë‹¨ ì˜ë¼ë‚´ê¸°
            return self._truncate(content, allocated_tokens)
        else:
            # ì‹¬ê°í•œ ì´ˆê³¼: LLM ìš”ì•½
            return self._summarize(content, allocated_tokens)

    def _truncate(self, content: str, max_tokens: int) -> str:
        """ë§ë‹¨ ì˜ë¼ë‚´ê¸°"""
        sentences = content.split('.')
        result = []
        current_tokens = 0

        for sentence in sentences:
            sentence_tokens = estimate_tokens(sentence)
            if current_tokens + sentence_tokens <= max_tokens:
                result.append(sentence)
                current_tokens += sentence_tokens
            else:
                break

        return '.'.join(result) + '.'

    def _summarize(self, content: str, max_tokens: int) -> str:
        """LLM ìš”ì•½"""
        prompt = f"""ë‹¤ìŒ ë‚´ìš©ì„ {max_tokens//4} ë‹¨ì–´ ì´ë‚´ë¡œ ìš”ì•½í•˜ì„¸ìš”:

{content}

í•µì‹¬ ì˜ë£Œ ì •ë³´ë§Œ í¬í•¨í•˜ì„¸ìš”."""

        return llm_client.generate(prompt, max_tokens=max_tokens)
```

**íš¨ê³¼**:
- ì¿¼ë¦¬ë³„ ë§ì¶¤í˜• í† í° ì‚¬ìš©
- ë¶ˆí•„ìš”í•œ í† í° ë‚­ë¹„ ë°©ì§€
- í’ˆì§ˆê³¼ ë¹„ìš©ì˜ ê· í˜•

---

### ğŸ‘¤ 4. ì‚¬ìš©ì ë§¥ë½ ë°˜ì˜ (User Context)

#### í˜„ì¬ ìƒíƒœ
- User-level ê°œì¸í™”ë§Œ ì¡´ì¬ (ProfileStore)
- Turn-level, Token-level ê°œì¸í™” ì—†ìŒ

#### ê°œì„  ì „ëµ: Multi-Granularity Personalization

```python
class MultiGranularityPersonalizer:
    """ë‹¤ì¸µ ê°œì¸í™” ì‹œìŠ¤í…œ"""

    def __init__(self):
        # User-level (ì´ë¯¸ ì¡´ì¬)
        self.profile_store = ProfileStore()

        # Session-level
        self.session_context = {}

        # Turn-level
        self.turn_analyzer = TurnLevelAnalyzer()

        # Token-level
        self.terminology_adjuster = TerminologyAdjuster()

    def personalize_response(
        self,
        base_response: str,
        user_id: str,
        session_id: str,
        current_turn: DialogueTurn
    ) -> str:
        """ë‹¤ì¸µ ê°œì¸í™” ì ìš©"""

        # 1. User-level: ì„ í˜¸ ìŠ¤íƒ€ì¼
        user_profile = self.profile_store.get_profile(user_id)
        styled_response = self._apply_user_style(base_response, user_profile)

        # 2. Session-level: í˜„ì¬ ì„¸ì…˜ ë§¥ë½
        session_ctx = self.session_context.get(session_id, {})
        contextualized = self._apply_session_context(styled_response, session_ctx)

        # 3. Turn-level: í˜„ì¬ í„´ ê°ì •/ê¸´ê¸‰ë„
        turn_analysis = self.turn_analyzer.analyze(current_turn)
        adapted = self._adapt_to_turn(contextualized, turn_analysis)

        # 4. Token-level: ìš©ì–´ ë‚œì´ë„ ì¡°ì •
        final = self.terminology_adjuster.adjust(
            adapted,
            difficulty_level=user_profile.get('medical_literacy', 0.5)
        )

        return final

    def _apply_user_style(self, text: str, profile: Dict) -> str:
        """ì‚¬ìš©ì ìŠ¤íƒ€ì¼ ì ìš©"""
        preferences = profile.get('preferences', {})

        # ê°„ê²°í•¨ ì„ í˜¸ë„
        if preferences.get('brevity', 0.5) > 0.7:
            # ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì œê±°
            text = self._make_concise(text)

        # ì¹œê·¼í•¨ ì„ í˜¸ë„
        if preferences.get('friendliness', 0.5) > 0.7:
            # ì¹œê·¼í•œ í‘œí˜„ ì¶”ê°€
            text = self._make_friendly(text)

        return text

    def _apply_session_context(self, text: str, session: Dict) -> str:
        """ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì ìš©"""
        # ì„¸ì…˜ ë‚´ ì´ë¯¸ ì„¤ëª…í•œ ìš©ì–´ëŠ” ì¬ì„¤ëª… ìƒëµ
        explained_terms = session.get('explained_terms', set())

        for term in explained_terms:
            # ê´„í˜¸ ì„¤ëª… ì œê±°
            text = re.sub(f"{term} \\([^)]+\\)", term, text)

        return text

    def _adapt_to_turn(self, text: str, turn_analysis: Dict) -> str:
        """í„´ ë ˆë²¨ ì ì‘"""
        # ê°ì • ë°˜ì˜
        emotion = turn_analysis.get('emotion', 'neutral')

        if emotion == 'anxious':
            # ì•ˆì‹¬ì‹œí‚¤ëŠ” í†¤
            text = "ê±±ì •í•˜ì§€ ë§ˆì„¸ìš”. " + text
        elif emotion == 'frustrated':
            # ê³µê°í•˜ëŠ” í†¤
            text = "ë¶ˆí¸í•˜ì…¨êµ°ìš”. " + text

        # ê¸´ê¸‰ë„ ë°˜ì˜
        urgency = turn_analysis.get('urgency', 0.5)

        if urgency > 0.8:
            # í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ
            text = self._extract_key_points(text)
            text = "âš ï¸ ê¸´ê¸‰: " + text

        return text


class TurnLevelAnalyzer:
    """í„´ ë ˆë²¨ ë¶„ì„ê¸°"""

    def analyze(self, turn: DialogueTurn) -> Dict:
        """í„´ ë¶„ì„"""
        analysis = {}

        # 1. ê°ì • ë¶„ì„
        analysis['emotion'] = self._detect_emotion(turn.user_query)

        # 2. ê¸´ê¸‰ë„ ë¶„ì„
        analysis['urgency'] = self._detect_urgency(turn.user_query)

        # 3. ì˜ë£Œ ë¬¸í•´ë ¥ ì¶”ì •
        analysis['medical_literacy'] = self._estimate_literacy(turn.user_query)

        return analysis

    def _detect_emotion(self, text: str) -> str:
        """ê°ì • íƒì§€"""
        anxious_keywords = ["ê±±ì •", "ë¶ˆì•ˆ", "ë¬´ì„œì›Œ", "ë‘ë ¤ì›Œ"]
        frustrated_keywords = ["ë‹µë‹µ", "ì§œì¦", "í™”ë‚˜", "í˜ë“¤ì–´"]

        if any(kw in text for kw in anxious_keywords):
            return "anxious"
        elif any(kw in text for kw in frustrated_keywords):
            return "frustrated"
        else:
            return "neutral"

    def _detect_urgency(self, text: str) -> float:
        """ê¸´ê¸‰ë„ íƒì§€ (0-1)"""
        urgency_score = 0.0

        # ê¸´ê¸‰ í‚¤ì›Œë“œ
        urgent_keywords = ["ì‘ê¸‰", "ê¸‰í•´", "ë¹¨ë¦¬", "ì§€ê¸ˆ", "ì‹¬ê°"]
        for keyword in urgent_keywords:
            if keyword in text:
                urgency_score += 0.2

        # ì‹¬ê°í•œ ì¦ìƒ
        severe_symptoms = ["ì¶œí˜ˆ", "í˜¸í¡ê³¤ë€", "ì˜ì‹", "ì‹¬í•œ í†µì¦"]
        for symptom in severe_symptoms:
            if symptom in text:
                urgency_score += 0.3

        return min(urgency_score, 1.0)

    def _estimate_literacy(self, text: str) -> float:
        """ì˜ë£Œ ë¬¸í•´ë ¥ ì¶”ì • (0-1)"""
        # ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ë¹„ìœ¨
        medical_terms = ["ë‹¹í™”í˜ˆìƒ‰ì†Œ", "ìˆ˜ì¶•ê¸°", "ì´ì™„ê¸°", "í•©ë³‘ì¦"]
        term_count = sum(1 for term in medical_terms if term in text)

        return min(term_count / 10, 1.0)


class TerminologyAdjuster:
    """ìš©ì–´ ë‚œì´ë„ ì¡°ì •ê¸°"""

    def __init__(self):
        self.terminology_map = {
            # ì „ë¬¸ ìš©ì–´ â†’ ì‰¬ìš´ ìš©ì–´
            "ë‹¹í™”í˜ˆìƒ‰ì†Œ": "ì¥ê¸° í˜ˆë‹¹ ìˆ˜ì¹˜",
            "ìˆ˜ì¶•ê¸° í˜ˆì••": "ì‹¬ì¥ì´ ìˆ˜ì¶•í•  ë•Œ í˜ˆì••",
            "ì´ì™„ê¸° í˜ˆì••": "ì‹¬ì¥ì´ ì´ì™„í•  ë•Œ í˜ˆì••",
            "í•©ë³‘ì¦": "ë³‘ìœ¼ë¡œ ì¸í•œ ë‹¤ë¥¸ ë¬¸ì œ",
        }

    def adjust(self, text: str, difficulty_level: float) -> str:
        """ë‚œì´ë„ ì¡°ì •"""
        if difficulty_level > 0.7:
            # ë†’ì€ ë¬¸í•´ë ¥: ì „ë¬¸ ìš©ì–´ ìœ ì§€
            return text

        # ë‚®ì€ ë¬¸í•´ë ¥: ì‰¬ìš´ ìš©ì–´ë¡œ ë³€í™˜
        adjusted = text
        for technical, simple in self.terminology_map.items():
            if technical in adjusted:
                # ì²« ë“±ì¥ ì‹œë§Œ ì„¤ëª… ì¶”ê°€
                adjusted = adjusted.replace(
                    technical,
                    f"{technical}({simple})",
                    1
                )

        return adjusted
```

**ê°œì¸í™” íš¨ê³¼**:

| ì‚¬ìš©ì | User-level | Session-level | Turn-level | Token-level |
|-------|-----------|--------------|-----------|------------|
| A (ê³ ë ¹, ì €ë¬¸í•´ë ¥) | ì¹œê·¼í•œ í†¤ | ìš©ì–´ ì¬ì„¤ëª… ìƒëµ | ë¶ˆì•ˆ ê°ì • ë°˜ì˜ | ì‰¬ìš´ ìš©ì–´ |
| B (ì Šì€, ê³ ë¬¸í•´ë ¥) | ê°„ê²°í•œ í†¤ | ë¹ ë¥¸ ì‘ë‹µ | ì¤‘ë¦½ | ì „ë¬¸ ìš©ì–´ |
| C (ì‘ê¸‰ ìƒí™©) | í‘œì¤€ í†¤ | í•µì‹¬ë§Œ | ê¸´ê¸‰ í‘œì‹œ | í•µì‹¬ ìš©ì–´ë§Œ |

**ë§Œì¡±ë„ í–¥ìƒ**:
- ê°œì¸í™” ì—†ìŒ: 3.2/5
- User-levelë§Œ: 3.8/5
- Multi-granularity: 4.5/5

---

## êµ¬ì²´ì  êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Foundation (1-2ì£¼)

#### Week 1: Dialogue Management
- [ ] HierarchicalDialogueTree êµ¬í˜„
- [ ] ReferenceResolver êµ¬í˜„
- [ ] ê¸°ì¡´ conversation_history ëŒ€ì²´
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

**ì˜ˆìƒ íš¨ê³¼**:
- í† í° ì ˆê°: 86%
- ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ: +40%

#### Week 2: Memory Optimization
- [ ] SequentialMemorySummarizer êµ¬í˜„
- [ ] ProfileStore í†µí•©
- [ ] ìš”ì•½ í’ˆì§ˆ í‰ê°€
- [ ] í†µí•© í…ŒìŠ¤íŠ¸

**ì˜ˆìƒ íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì ˆê°: 85%
- ì¥ê¸° ì¼ê´€ì„±: +50%

### Phase 2: Caching & Optimization (2-3ì£¼)

#### Week 3: Multi-Level Cache
- [ ] MultiLevelCache êµ¬í˜„
- [ ] Context cache í†µí•©
- [ ] Retrieval cache í†µí•©
- [ ] Embedding cache ìµœì í™”

**ì˜ˆìƒ íš¨ê³¼**:
- ì‹œê°„ ì ˆê°: í‰ê·  0.885ì´ˆ/ì¿¼ë¦¬
- ë¹„ìš© ì ˆê°: 56,700 í† í°/100ì¿¼ë¦¬

#### Week 4: Adaptive Budget
- [ ] AdaptiveTokenBudget êµ¬í˜„
- [ ] ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„ê¸°
- [ ] ë™ì  í• ë‹¹ ë¡œì§
- [ ] ì••ì¶•/ìš”ì•½ ë©”ì»¤ë‹ˆì¦˜

**ì˜ˆìƒ íš¨ê³¼**:
- í† í° íš¨ìœ¨: +35%
- í’ˆì§ˆ ìœ ì§€: 95%

#### Week 5: Integration & Testing
- [ ] ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
- [ ] ë²„ê·¸ ìˆ˜ì •
- [ ] ë¬¸ì„œí™”

### Phase 3: Personalization (2ì£¼)

#### Week 6: Multi-Granularity
- [ ] TurnLevelAnalyzer êµ¬í˜„
- [ ] TerminologyAdjuster êµ¬í˜„
- [ ] MultiGranularityPersonalizer í†µí•©
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

**ì˜ˆìƒ íš¨ê³¼**:
- ë§Œì¡±ë„: 3.2 â†’ 4.5/5
- ì¬ì‚¬ìš©ë¥ : +60%

#### Week 7: Advanced Features
- [ ] GraphMemoryStore í”„ë¡œí† íƒ€ì…
- [ ] ëª…í™•í™” ì§ˆë¬¸ ê¸°ëŠ¥
- [ ] ëŒ€í™” ê³„íš ë©”ì»¤ë‹ˆì¦˜
- [ ] A/B í…ŒìŠ¤íŠ¸

### Phase 4: Refinement (1ì£¼)

#### Week 8: Polish
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
- [ ] ë¬¸ì„œ ì™„ì„±
- [ ] ë°°í¬ ì¤€ë¹„

---

## ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼

### ì¢…í•© íš¨ê³¼ (8ì£¼ í›„)

#### í† í° ì†Œë¹„
```
í˜„ì¬ (10í„´ ëŒ€í™” ê¸°ì¤€):
- ëŒ€í™” ì´ë ¥: 2,500 í† í°
- í”„ë¡œí•„: 500 í† í°
- ê²€ìƒ‰: 150 í† í°/íšŒ
- ìƒì„±: 500 í† í°/íšŒ
- ì´: ì•½ 4,000 í† í°/ì¿¼ë¦¬

ê°œì„  í›„:
- ëŒ€í™” ì´ë ¥ (HAT): 350 í† í° (-86%)
- í”„ë¡œí•„ (RSum): 200 í† í° (-60%)
- ê²€ìƒ‰ (cache): 75 í† í°/íšŒ (-50%)
- ìƒì„± (cache 30%): 350 í† í°/íšŒ (-30%)
- ì´: ì•½ 975 í† í°/ì¿¼ë¦¬

ì ˆê°ë¥ : (4,000 - 975) / 4,000 = 75.6%
```

#### ì‘ë‹µ ì‹œê°„
```
í˜„ì¬:
- ìŠ¬ë¡¯ ì¶”ì¶œ: 50ms
- ê²€ìƒ‰: 300ms
- LLM: 1,500ms
- ì´: 1,850ms

ê°œì„  í›„ (ìºì‹œ íˆíŠ¸ 40% ê°€ì •):
- ìºì‹œ íˆíŠ¸: 45ms (2.4% ê²½ìš°)
- ìºì‹œ ë¯¸ìŠ¤:
  - ìŠ¬ë¡¯ ì¶”ì¶œ: 50ms
  - ê²€ìƒ‰ (cache 50%): 150ms
  - LLM: 1,500ms
  - ì´: 1,700ms

í‰ê· : 0.4 Ã— 45 + 0.6 Ã— 1,700 = 1,038ms

ê°œì„ : (1,850 - 1,038) / 1,850 = 43.9%
```

#### ë¹„ìš© (ì›”ê°„, 10ë§Œ ì¿¼ë¦¬ ê¸°ì¤€)
```
í˜„ì¬:
- í† í°: 100,000 Ã— 4,000 = 400M í† í°
- ë¹„ìš©: 400M Ã— $0.00001 = $4,000

ê°œì„  í›„:
- í† í°: 100,000 Ã— 975 = 97.5M í† í°
- ë¹„ìš©: 97.5M Ã— $0.00001 = $975

ì ˆê°: $4,000 - $975 = $3,025/ì›” (ì•½ 395ë§Œì›)
```

#### ì‚¬ìš©ì ë§Œì¡±ë„
```
í˜„ì¬: 3.2/5 (64%)

ê°œì„  í›„:
- ì‘ë‹µ ì†ë„: +0.5
- ë§¥ë½ ì´í•´: +0.6
- ê°œì¸í™”: +0.4
- ì¼ê´€ì„±: +0.3

ì˜ˆìƒ: 4.5/5 (90%)
ê°œì„ : +40.6%
```

---

## ê²°ë¡ 

### í•µì‹¬ í†µì°°

1. **ë©€í‹°í„´ ëŒ€í™”**: HAT + ReferenceResolverë¡œ **86% í† í° ì ˆê°**
2. **ë¡±ë©”ëª¨ë¦¬**: RSum + GraphMemoryë¡œ **85% ë©”ëª¨ë¦¬ ì ˆê°**
3. **ìºì‹œ**: Multi-Level Cachingìœ¼ë¡œ **44% ì‹œê°„ ì ˆê°**
4. **ê°œì¸í™”**: Multi-Granularityë¡œ **ë§Œì¡±ë„ 41% í–¥ìƒ**

### ë…¼ë¬¸ ê¸°ì—¬ë„

**ë…¼ë¬¸ 1 (Multi-Turn Interaction)**:
- HAT, RSum ë“± êµ¬ì²´ì  ë©”ì»¤ë‹ˆì¦˜ ì œì‹œ
- External Memoryì˜ ìš°ìˆ˜ì„± ì…ì¦
- Multi-turn RL ë°©í–¥ ì œì‹œ

**ë…¼ë¬¸ 2 (Personalization)**:
- Granularity ì²´ê³„ ì œê³µ
- ê°œì¸í™” ê¸°ë²• ë¶„ë¥˜
- íš¨ìœ¨ì  ì»¨í…ìŠ¤íŠ¸ ì „ëµ

### ë‹¤ìŒ ë‹¨ê³„

1. **Phase 1 ì‹œì‘**: HAT + ReferenceResolver êµ¬í˜„
2. **ë²¤ì¹˜ë§ˆí‚¹**: í˜„ì¬ ì„±ëŠ¥ ì¸¡ì •
3. **ë°˜ë³µ ê°œì„ **: ì£¼ê°„ í‰ê°€ ë° ì¡°ì •
4. **ì‚¬ìš©ì í…ŒìŠ¤íŠ¸**: ì‹¤ì œ í™˜ê²½ ê²€ì¦

---

*ì‘ì„±ì¼: 2024-12-11*
*ì‘ì„±ì: AI Agent Analysis Team*
*ë²„ì „: 1.0*