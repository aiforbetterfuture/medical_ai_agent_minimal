# RAGAS ìë™ ê³„ì‚° ì—¬ë¶€ ì„¤ëª…

## âŒ RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë§Œìœ¼ë¡œëŠ” ìë™ ê³„ì‚°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤

**ì¤‘ìš”**: RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” í‰ê°€ì§€í‘œê°€ ìë™ìœ¼ë¡œ ê³„ì‚°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

---

## ğŸ” RAGAS ì‘ë™ ë°©ì‹

### 1. RAGASëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤

RAGASëŠ” **í‰ê°€ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” ë„êµ¬**ì´ì§€, ìë™ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ì„œë¹„ìŠ¤ê°€ ì•„ë‹™ë‹ˆë‹¤.

### 2. ëª…ì‹œì  í˜¸ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤

RAGASë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ **ì½”ë“œì—ì„œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œ**í•´ì•¼ í•©ë‹ˆë‹¤:

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevance,
    context_precision,
    context_recall,
    context_relevancy
)
from datasets import Dataset

# 1. ë°ì´í„° ì¤€ë¹„
dataset = Dataset.from_dict({
    "question": ["ì§ˆë¬¸ í…ìŠ¤íŠ¸"],
    "answer": ["ë‹µë³€ í…ìŠ¤íŠ¸"],
    "contexts": [["ê²€ìƒ‰ëœ ë¬¸ì„œ 1", "ê²€ìƒ‰ëœ ë¬¸ì„œ 2", ...]],
    "ground_truth": ["ì •ë‹µ (ì„ íƒì‚¬í•­)"]
})

# 2. ëª…ì‹œì ìœ¼ë¡œ evaluate() í˜¸ì¶œ
results = evaluate(
    dataset,
    metrics=[
        faithfulness,
        answer_relevance,
        context_precision,
        context_recall,
        context_relevancy
    ]
)

# 3. ê²°ê³¼ í™•ì¸
print(results["faithfulness"])  # ì˜ˆ: [0.85]
print(results["answer_relevance"])  # ì˜ˆ: [0.78]
```

---

## ğŸ“‹ í˜„ì¬ ìƒí™©

### âœ… ì„¤ì¹˜ ì™„ë£Œ (ì´ì œ ê°€ëŠ¥)

`0_setup_env.bat`ì— ragas ì„¤ì¹˜ ì½”ë“œë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤:
- `ragas` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
- `datasets` ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (RAGAS ì˜ì¡´ì„±)

### âŒ ì•„ì§ ê³„ì‚°ë˜ì§€ ì•ŠìŒ

**ì´ìœ **: ì‹¤í—˜ ëŸ¬ë„ˆ(`experiments/run_multiturn_experiment_v2.py`)ì— RAGAS í˜¸ì¶œ ì½”ë“œê°€ ì—†ìŒ

---

## ğŸ”§ ìë™ ê³„ì‚°ì„ ìœ„í•œ í•„ìš” ì‘ì—…

### ì˜µì…˜ 1: ì‹¤í—˜ ëŸ¬ë„ˆì— í†µí•© (ê¶Œì¥)

ê° í„´ë§ˆë‹¤ ë‹µë³€ ìƒì„± í›„ ìë™ìœ¼ë¡œ RAGAS ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ë„ë¡ ì‹¤í—˜ ëŸ¬ë„ˆë¥¼ ìˆ˜ì •:

```python
# experiments/run_multiturn_experiment_v2.py

from experiments.evaluation.ragas_metrics import calculate_ragas_metrics

def _run_agent_mode(self, ...):
    # ... ê¸°ì¡´ ì½”ë“œ ...
    
    # ë‹µë³€ ìƒì„± í›„
    answer_text = final_state.get('final_answer', '')
    retrieved_docs = final_state.get('retrieved_docs', [])
    
    # RAGAS ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = None
    if self.config.get('evaluation', {}).get('per_turn_metrics'):
        contexts = [doc.get('text', '') for doc in retrieved_docs]
        try:
            metrics = calculate_ragas_metrics(
                question=question_text,
                answer=answer_text,
                contexts=contexts
            )
        except Exception as e:
            logger.warning(f"RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° ì‹¤íŒ¨: {e}")
    
    # ì´ë²¤íŠ¸ì— metrics í¬í•¨
    event = {
        # ... ê¸°ì¡´ í•„ë“œë“¤ ...
        "metrics": metrics  # âœ… ì¶”ê°€
    }
```

### ì˜µì…˜ 2: ì‚¬í›„ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì‹¤í—˜ ì™„ë£Œ í›„ ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¡œ ê³„ì‚°:

```bash
python scripts/evaluate_ragas_metrics.py \
  --events_path runs/2025-12-13_primary_v1/events.jsonl \
  --output_path runs/2025-12-13_primary_v1/events_with_metrics.jsonl
```

---

## ğŸ“ ìš”ì•½

| í•­ëª© | ìƒíƒœ | ì„¤ëª… |
|------|------|------|
| ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ | âœ… | `0_setup_env.bat`ì— ì¶”ê°€ë¨ |
| ìë™ ê³„ì‚° | âŒ | **ì„¤ì¹˜ë§Œìœ¼ë¡œëŠ” ìë™ ê³„ì‚° ì•ˆ ë¨** |
| ëª…ì‹œì  í˜¸ì¶œ í•„ìš” | âœ… | ì½”ë“œì—ì„œ `evaluate()` í˜¸ì¶œ í•„ìš” |
| ì‹¤í—˜ ëŸ¬ë„ˆ í†µí•© | âŒ | ì•„ì§ êµ¬í˜„ ì•ˆ ë¨ |

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. âœ… **ì™„ë£Œ**: `0_setup_env.bat`ì— ragas ì„¤ì¹˜ ì¶”ê°€
2. â³ **í•„ìš”**: RAGAS ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ ì‘ì„± (`experiments/evaluation/ragas_metrics.py`)
3. â³ **í•„ìš”**: ì‹¤í—˜ ëŸ¬ë„ˆì— í†µí•© (`experiments/run_multiturn_experiment_v2.py`)

**ê²°ë¡ **: RAGAS ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ìë™ ê³„ì‚°ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤í—˜ ëŸ¬ë„ˆì— RAGAS í˜¸ì¶œ ì½”ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

