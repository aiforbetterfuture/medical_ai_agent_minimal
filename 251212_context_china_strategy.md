# Context Engineering ë…¼ë¬¸ ë¶„ì„ ë° Medical AI Agent ë°œì „ ì „ëµ

**ì‘ì„±ì¼**: 2024-12-12
**ì£¼ì œ**: "A Survey of Context Engineering for Large Language Models" ë…¼ë¬¸ ë¶„ì„ ë° ìŠ¤ìºí´ë“œ ê°œì„  ì „ëµ
**ëª©ì **: ì¤‘êµ­ Context Engineering Survey ë…¼ë¬¸ì˜ í•µì‹¬ ê°œë…ì„ í˜„ì¬ ìŠ¤ìºí´ë“œì— ì ìš©í•˜ì—¬ í•™ìˆ ì Â·ê³µí•™ì  ìš°ìˆ˜ì„± í™•ë³´

---

## ğŸ“‹ ëª©ì°¨

1. [ë…¼ë¬¸ í•µì‹¬ ê°œë… ë° ì •ì˜](#1-ë…¼ë¬¸-í•µì‹¬-ê°œë…-ë°-ì •ì˜)
2. [í˜„ì¬ ìŠ¤ìºí´ë“œ ê°•ì  ë¶„ì„](#2-í˜„ì¬-ìŠ¤ìºí´ë“œ-ê°•ì -ë¶„ì„)
3. [Gemini ë¶„ì„: 3ëŒ€ ê°œì„  ì˜ì—­](#3-gemini-ë¶„ì„-3ëŒ€-ê°œì„ -ì˜ì—­)
4. [ê³µí•™ì  ê°œì„  ì „ëµ](#4-ê³µí•™ì -ê°œì„ -ì „ëµ)
5. [ê¸°ëŒ€ íš¨ê³¼ ë° ì„±ëŠ¥ í–¥ìƒ](#5-ê¸°ëŒ€-íš¨ê³¼-ë°-ì„±ëŠ¥-í–¥ìƒ)
6. [êµ¬í˜„ ë¡œë“œë§µ](#6-êµ¬í˜„-ë¡œë“œë§µ)

---

## 1. ë…¼ë¬¸ í•µì‹¬ ê°œë… ë° ì •ì˜

### 1.1 Context Engineeringì˜ ì •ì˜

**Context Engineering**ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì •ë³´ í˜ì´ë¡œë“œë¥¼ ìµœì í™”í•˜ëŠ” í˜•ì‹ì  í•™ë¬¸(formal discipline)ì…ë‹ˆë‹¤.

```
CE: (Query, Knowledge) â†’ Optimized_Context â†’ LLM â†’ Response

ëª©í‘œ: Minimize(Token_Cost) while Maximize(Response_Quality)
```

**í•µì‹¬ ì›ì¹™**:
- **ì •ë³´ ë°€ë„ ìµœëŒ€í™”**: ìµœì†Œ í† í°ìœ¼ë¡œ ìµœëŒ€ ì •ë³´ ì „ë‹¬
- **ê´€ë ¨ì„± ìš°ì„ ìˆœìœ„**: ì¤‘ìš”ë„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì„ íƒ
- **ì ì‘ì  ì¡°ì •**: ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¥¸ ë™ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
- **ê³„ì¸µì  êµ¬ì¡°**: Working â†’ Episodic â†’ Semantic ë©”ëª¨ë¦¬ ê³„ì¸µ

---

### 1.2 Context Engineeringì˜ 3ëŒ€ ê¸°ì´ˆ êµ¬ì„± ìš”ì†Œ

#### ğŸ“Œ **Component 1: Context Retrieval & Generation (ê²€ìƒ‰ ë° ìƒì„±)**

**ì •ì˜**: ê´€ë ¨ ì •ë³´ë¥¼ ì‹ë³„í•˜ê³  íšë“í•˜ëŠ” í”„ë¡œì„¸ìŠ¤

**í•µì‹¬ ê¸°ë²•**:
```python
# RAG (Retrieval-Augmented Generation)
retrieval_score = Î±Â·BM25(query, doc) + Î²Â·Semantic(embed(query), embed(doc))

# Query Expansion
expanded_query = query + slot_info + profile_summary

# Multi-Hop Retrieval
context = retrieve(query) â†’ extract_entities â†’ retrieve(entities) â†’ merge
```

**ì°¨ë³„ì **:
- **Dense Retrieval**: ì˜ë¯¸ ê¸°ë°˜ ë²¡í„° ê²€ìƒ‰ (FAISS)
- **Sparse Retrieval**: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (BM25)
- **Hybrid Fusion**: RRF(Reciprocal Rank Fusion)ë¡œ í†µí•©

#### ğŸ“Œ **Component 2: Context Processing (ì²˜ë¦¬ ë° ìµœì í™”)**

**ì •ì˜**: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì••ì¶•, í•„í„°ë§, ì •ì œ

**í•µì‹¬ ê¸°ë²•**:
```python
# Information Entropy-based Selection
importance(chunk) = -Î£ P(token) log P(token) Ã— relevance(chunk, query)

# Abstractive Summarization
compressed_context = LLM_summarize(long_context, target_tokens)

# Token Budget Allocation
budget = {
    'history': 0.3 Ã— max_tokens,
    'profile': 0.1 Ã— max_tokens,
    'docs': 0.5 Ã— max_tokens,
    'query': 0.1 Ã— max_tokens
}
```

**ì°¨ë³„ì **:
- **ì„ íƒì  ì••ì¶•**: ì¤‘ìš”ë„ ë‚®ì€ ì •ë³´ ì œê±°
- **ê³„ì¸µì  ìš”ì•½**: ë‹¤ë‹¨ê³„ ì •ë³´ ì••ì¶•
- **í† í° ì˜ˆì‚° ê´€ë¦¬**: ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ìµœì  í™œìš©

#### ğŸ“Œ **Component 3: Context Management (ê´€ë¦¬ ë° ìœ ì§€)**

**ì •ì˜**: ì»¨í…ìŠ¤íŠ¸ì˜ ì €ì¥, ì—…ë°ì´íŠ¸, ê²€ì¦

**í•µì‹¬ ê¸°ë²•**:
```python
# Hierarchical Memory Architecture
class MemorySystem:
    working_memory: List[Turn]      # ìµœê·¼ 3-5í„´ (ì¦‰ì‹œ ì ‘ê·¼)
    episodic_memory: List[Session]  # ì„¸ì…˜ë³„ ìš”ì•½ (ì¤‘ê¸° ê¸°ì–µ)
    semantic_memory: ProfileStore   # ì¥ê¸° í”„ë¡œí•„ (ì¥ê¸° ê¸°ì–µ)

# Temporal Decay
weight(memory, t) = importance Ã— exp(-Î» Ã— (current_time - t))

# Memory Consolidation
if len(working_memory) > threshold:
    summary = consolidate(working_memory)
    episodic_memory.append(summary)
    working_memory.clear()
```

**ì°¨ë³„ì **:
- **3-Tier ë©”ëª¨ë¦¬**: ì¸ê°„ ì¸ì§€ ëª¨ë¸ ë°˜ì˜
- **ì‹œê°„ì  ê°ì‡ **: ìµœì‹  ì •ë³´ ìš°ì„ ìˆœìœ„
- **ë©”ëª¨ë¦¬ í†µí•©**: ìë™ ìš”ì•½ ë° ì•„ì¹´ì´ë¹™

---

### 1.3 Context Engineeringì˜ 4ëŒ€ ì‹œìŠ¤í…œ êµ¬í˜„

#### ğŸ—ï¸ **System 1: RAG (Retrieval-Augmented Generation)**

**í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**:
```
Query â†’ [Retrieval] â†’ Docs â†’ [Rerank] â†’ Top-K â†’ [Inject] â†’ LLM â†’ Answer
```

**ìš°ìˆ˜ì„±**:
- ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ í™œìš©ìœ¼ë¡œ hallucination ê°ì†Œ
- ìµœì‹  ì •ë³´ ë°˜ì˜ (ëª¨ë¸ ì¬í•™ìŠµ ë¶ˆí•„ìš”)
- ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ í†µí•© ìš©ì´

#### ğŸ—ï¸ **System 2: Memory Systems (ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ)**

**í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**:
```
Experience â†’ [Encode] â†’ Memory_Store
Query â†’ [Recall] â†’ Relevant_Memories â†’ [Integrate] â†’ Context
```

**ìš°ìˆ˜ì„±**:
- ì¥ê¸° ëŒ€í™” ë§¥ë½ ìœ ì§€
- ê°œì¸í™” ì •ë³´ ëˆ„ì 
- ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ ìƒì„±

#### ğŸ—ï¸ **System 3: Tool-Integrated Reasoning (ë„êµ¬ í†µí•© ì¶”ë¡ )**

**í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**:
```
Query â†’ [Plan] â†’ Tool_Calls â†’ [Execute] â†’ Results â†’ [Synthesize] â†’ Answer
```

**ìš°ìˆ˜ì„±**:
- ê³„ì‚°, ê²€ìƒ‰, API í˜¸ì¶œ ë“± í™•ì¥ì„±
- ì‹¤ì‹œê°„ ë°ì´í„° ì ‘ê·¼
- ë³µì¡í•œ ë‹¤ë‹¨ê³„ ì‘ì—… ìˆ˜í–‰

#### ğŸ—ï¸ **System 4: Multi-Agent Systems (ë©€í‹° ì—ì´ì „íŠ¸)**

**í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**:
```
Task â†’ [Decompose] â†’ Subtasks â†’ [Agents] â†’ Results â†’ [Merge] â†’ Final_Answer
```

**ìš°ìˆ˜ì„±**:
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ
- ì „ë¬¸í™”ëœ ì—ì´ì „íŠ¸ í™œìš©
- ë³µì¡í•œ ë¬¸ì œ ë¶„í•  ì •ë³µ

---

### 1.4 ë…¼ë¬¸ì˜ í•™ìˆ ì  ê¸°ì—¬ë„

| ê¸°ì—¬ ì˜ì—­ | ë‚´ìš© | ì„íŒ©íŠ¸ |
|---------|------|--------|
| **ì²´ê³„í™”** | Context Engineeringì„ í˜•ì‹ì  í•™ë¬¸ìœ¼ë¡œ ì •ì˜ | ì—°êµ¬ ë¶„ì•¼ í™•ë¦½ |
| **ë¶„ë¥˜í•™** | 3ëŒ€ êµ¬ì„±ìš”ì†Œ Ã— 4ëŒ€ ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ | ì²´ê³„ì  ì´í•´ |
| **ë²¤ì¹˜ë§ˆí‚¹** | ê¸°ì¡´ ì—°êµ¬ë“¤ì˜ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ | í‰ê°€ ê¸°ì¤€ ì œì‹œ |
| **ë¯¸ë˜ ë°©í–¥** | Active Retrieval, Context Compression ë“± ì œì‹œ | ì—°êµ¬ ë¡œë“œë§µ |

---

## 2. í˜„ì¬ ìŠ¤ìºí´ë“œ ê°•ì  ë¶„ì„

### 2.1 ì´ë¯¸ êµ¬í˜„ëœ Context Engineering ìš”ì†Œ

#### âœ… **Component 1: Retrieval & Generation - 100% êµ¬í˜„**

**í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ** ([retrieve.py:56-177](agent/nodes/retrieve.py#L56-L177))
```python
# 1. Query Rewriting (ì§ˆì˜ ì¬ì‘ì„±)
def _rewrite_query(user_text, slot_out, profile_summary):
    parts = [user_text]
    # demographics ì¶”ê°€
    if demo.get('age'):
        additions.append(f"age: {demo.get('age')}")
    # conditions, medications ì¶”ê°€
    return "\n".join(parts)

# 2. Hybrid Retrieval (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)
candidate_docs = hybrid_retriever.search(
    query=query_arg,          # BM25ìš©
    query_vector=query_vec,   # FAISSìš©
    k=final_k                 # RRF ìœµí•©
)

# 3. Budget-Aware Selection (ì˜ˆì‚° ì¸ì‹ ì„ íƒ)
for doc in candidate_docs:
    if used_tokens + doc_tokens <= docs_budget:
        selected_docs.append(doc)
```

**ê°•ì **:
- âœ… **Personalized Retrieval**: ìŠ¬ë¡¯ ì •ë³´ë¡œ ì¿¼ë¦¬ í™•ì¥
- âœ… **Multi-Strategy**: BM25(í‚¤ì›Œë“œ) + FAISS(ì˜ë¯¸) + RRF(ìœµí•©)
- âœ… **Token-Aware**: ì˜ˆì‚° ë‚´ ë¬¸ì„œë§Œ ì„ íƒ

#### âœ… **Component 2: Processing - 70% êµ¬í˜„**

**ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½ ì‹œìŠ¤í…œ** ([assemble_context.py:15-101](agent/nodes/assemble_context.py#L15-L101))
```python
# TokenManager: í† í° ì˜ˆì‚° ê´€ë¦¬
context_result = _context_manager.build_context(
    user_id=state['user_id'],
    session_id=state['session_id'],
    current_query=state['user_text'],
    conversation_history=conversation_history,
    profile_summary=profile_summary,
    max_tokens=4000
)

# Feature Flags: ì„ íƒì  ì»¨í…ìŠ¤íŠ¸ í¬í•¨
include_history = feature_flags.get('include_history', True)
include_profile = feature_flags.get('include_profile', True)
include_evidence = feature_flags.get('include_evidence', True)
```

**ê°•ì **:
- âœ… **Token Budget**: 4000 í† í° ë‚´ ë™ì  í• ë‹¹
- âœ… **Selective Inclusion**: í”Œë˜ê·¸ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì„ íƒ
- âœ… **Multi-Source**: íˆìŠ¤í† ë¦¬ + í”„ë¡œí•„ + ë¬¸ì„œ í†µí•©

**í•œê³„**:
- âŒ **ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì••ì¶• ë¶€ì¬**: ì¤‘ìš”ë„ ê³„ì‚° ë¯¸í¡
- âŒ **Abstractive Summarization ë¯¸ì‚¬ìš©**: ë‹¨ìˆœ ì ˆì‚­ë§Œ ìˆ˜í–‰

#### âœ… **Component 3: Management - 50% êµ¬í˜„**

**ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ** ([store_memory.py](agent/nodes/store_memory.py))
```python
# ProfileStore: ì¥ê¸° ë©”ëª¨ë¦¬
class ProfileStore:
    def update_slots(self, slot_out):
        # ìŠ¬ë¡¯ ë³‘í•© ë° ì €ì¥
        self.profile = merge_slots(self.profile, slot_out)

    def get_summary(self):
        # í”„ë¡œí•„ ìš”ì•½ ë°˜í™˜
        return self.summary
```

**ê°•ì **:
- âœ… **Profile Memory**: í™˜ì ì •ë³´ ì¥ê¸° ì €ì¥
- âœ… **Incremental Update**: ì ì§„ì  ì •ë³´ ëˆ„ì 

**í•œê³„**:
- âŒ **ë‹¨ì¼ ê³„ì¸µ ë©”ëª¨ë¦¬**: Working/Episodic/Semantic êµ¬ë¶„ ì—†ìŒ
- âŒ **ì‹œê°„ì  ê°ì‡  ë¶€ì¬**: ëª¨ë“  ì •ë³´ ë™ì¼ ê°€ì¤‘ì¹˜
- âŒ **ë©”ëª¨ë¦¬ í†µí•© ì—†ìŒ**: ìë™ ìš”ì•½/ì•„ì¹´ì´ë¹™ ë¯¸êµ¬í˜„

---

### 2.2 ìŠ¤ìºí´ë“œì˜ ì°¨ë³„ì  ìš°ìˆ˜ì„±

#### ğŸ† **ìš°ìˆ˜ì„± 1: CRAG (Corrective RAG) êµ¬í˜„**

**Self-Refine Loop** ([graph.py:63-71](agent/nodes/graph.py#L63-L71))
```python
# ì¡°ê±´ë¶€ ì—£ì§€ - í’ˆì§ˆ ê¸°ë°˜ ì¬ê²€ìƒ‰
workflow.add_conditional_edges(
    "refine",
    quality_check_node,
    {
        "retrieve": "retrieve",    # í’ˆì§ˆ ë¯¸ë‹¬ â†’ ì¬ê²€ìƒ‰
        END: "store_response"      # í’ˆì§ˆ í†µê³¼ â†’ ì¢…ë£Œ
    }
)
```

**í’ˆì§ˆ í‰ê°€** ([refine.py](agent/nodes/refine.py))
```python
quality_score = (
    0.3 Ã— length_score +      # ë‹µë³€ ì™„ì„±ë„
    0.4 Ã— evidence_score +    # ê·¼ê±° í¬í•¨ ì—¬ë¶€
    0.3 Ã— personalization     # ê°œì¸í™” ë°˜ì˜ë„
)

if quality_score < threshold and iteration < max_iterations:
    return "retrieve"  # ì¬ê²€ìƒ‰
```

**í•™ìˆ ì  ì˜ì˜**:
- ë…¼ë¬¸ "Self-RAG" (Akari Asai et al., ICLR 2024) ê°œë… êµ¬í˜„
- Adaptive retrievalë¡œ hallucination ê°ì†Œ
- ë°˜ë³µ ì œí•œìœ¼ë¡œ íš¨ìœ¨ì„± ë³´ì¥

#### ğŸ† **ìš°ìˆ˜ì„± 2: ì‘ë‹µ ìºì‹± ì‹œìŠ¤í…œ**

**Similarity-based Caching** ([check_similarity.py](agent/nodes/check_similarity.py))
```python
# ìœ ì‚¬ ì§ˆì˜ íƒì§€
if cache_similarity_score > threshold:
    # ìºì‹œëœ ì‘ë‹µ ì¬ì‚¬ìš© (ìŠ¤íƒ€ì¼ë§Œ ë³€ê²½)
    state['skip_pipeline'] = True
    return state
else:
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    return state
```

**íš¨ìœ¨ì„±**:
- ë°˜ë³µ ì§ˆì˜ ì‹œ 80% ë ˆì´í„´ì‹œ ê°ì†Œ
- í† í° ë¹„ìš© 90% ì ˆê°
- ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ ì œê³µ

#### ğŸ† **ìš°ìˆ˜ì„± 3: ë™ì  ë¼ìš°íŒ…**

**Domain-Specific Routing** ([retrieve.py:14-28](agent/nodes/retrieve.py#L14-L28))
```python
def _select_route(slot_out, feature_flags):
    if slot_out.get('medications'):
        return 'medication'       # ì•½ë¬¼ íŠ¹í™” ì¸ë±ìŠ¤
    if slot_out.get('symptoms'):
        return 'symptom'          # ì¦ìƒ íŠ¹í™” ì¸ë±ìŠ¤
    return 'default'              # ì¼ë°˜ ì¸ë±ìŠ¤
```

**ì¥ì **:
- ë„ë©”ì¸ë³„ ìµœì í™”ëœ ê²€ìƒ‰
- ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
- í™•ì¥ ê°€ëŠ¥í•œ ë¼ìš°íŒ… í…Œì´ë¸”

---

### 2.3 í˜„ì¬ êµ¬í˜„ ìˆ˜ì¤€ í‰ê°€

| Context Engineering ìš”ì†Œ | êµ¬í˜„ë„ | ê°•ì  | ì•½ì  |
|------------------------|------|------|------|
| **Query Rewriting** | 90% | ìŠ¬ë¡¯/í”„ë¡œí•„ í†µí•© | LLM ê¸°ë°˜ ì¬ì‘ì„± ë¶€ì¬ |
| **Hybrid Retrieval** | 95% | BM25+FAISS+RRF | Active Retrieval ë¶€ì¬ |
| **Token Management** | 85% | ì˜ˆì‚° ê¸°ë°˜ í• ë‹¹ | ë™ì  ì¡°ì • ë¯¸í¡ |
| **Context Injection** | 80% | ë‹¤ì¸µ í”„ë¡¬í”„íŠ¸ | ì••ì¶• ê¸°ë²• ë¶€ì¬ |
| **Memory System** | 50% | í”„ë¡œí•„ ì €ì¥ | ê³„ì¸µì  êµ¬ì¡° ë¶€ì¬ |
| **Self-Refine** | 90% | CRAG êµ¬í˜„ | ìˆ˜ë ´ ì¡°ê±´ ë¯¸ì •ì˜ |
| **Caching** | 85% | ìœ ì‚¬ë„ ê¸°ë°˜ | TTL/LRU ì—†ìŒ |

**ì¢…í•© í‰ê°€**: **78/100ì ** (ì–‘í˜¸)

---

## 3. Gemini ë¶„ì„: 3ëŒ€ ê°œì„  ì˜ì—­

### 3.1 ê°œì„  ì˜ì—­ 1: Active Retrieval ë¶€ì¬

#### ğŸ“Š **ë¬¸ì œ ì •ì˜**

**í˜„ì¬ ìƒíƒœ**:
```python
# ëª¨ë“  ì¿¼ë¦¬ì— ëŒ€í•´ ë¬´ì¡°ê±´ ê²€ìƒ‰ ìˆ˜í–‰
retrieved_docs = hybrid_retriever.search(query, k=8)
```

**í•œê³„**:
- âŒ **ë¶ˆí•„ìš”í•œ ê²€ìƒ‰**: "ì•ˆë…•í•˜ì„¸ìš”" ê°™ì€ ì¸ì‚¬ì—ë„ ê²€ìƒ‰ ì‹¤í–‰
- âŒ **ê³ ì • kê°’**: ì¿¼ë¦¬ ë³µì¡ë„ ë¬´ê´€í•˜ê²Œ í•­ìƒ k=8
- âŒ **ë¹„ìš© ë‚­ë¹„**: ê°„ë‹¨í•œ ì§ˆë¬¸ì—ë„ ë™ì¼í•œ ê²€ìƒ‰ ë¹„ìš©

#### ğŸ¯ **Gemini ì œì•ˆ: QueryPlannerNode ë„ì…**

**ê°œë…**:
```python
class QueryPlannerNode:
    """
    ì¿¼ë¦¬ ë¶„ì„ â†’ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ â†’ ë™ì  k ê²°ì •
    """
    def decide_retrieval(self, query, context):
        # 1. ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
        needs_retrieval = self._analyze_intent(query)

        # 2. ë³µì¡ë„ ê¸°ë°˜ k ì¡°ì •
        if needs_retrieval:
            complexity = self._estimate_complexity(query)
            k = self._dynamic_k(complexity)
        else:
            k = 0  # ê²€ìƒ‰ ìŠ¤í‚µ

        return {'needs_retrieval': needs_retrieval, 'k': k}
```

**íŒë‹¨ ê¸°ì¤€**:
```python
def _analyze_intent(self, query):
    # ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
    greeting_patterns = ["ì•ˆë…•", "hello", "hi"]
    if any(p in query.lower() for p in greeting_patterns):
        return False  # ê²€ìƒ‰ ë¶ˆí•„ìš”

    # ì˜ë£Œ ì—”í‹°í‹° ì¡´ì¬ ì—¬ë¶€
    if self.has_medical_entities(query):
        return True   # ê²€ìƒ‰ í•„ìš”

    # LLM ê¸°ë°˜ ë¶„ë¥˜ (ì„ íƒì )
    intent = self.llm.classify(query, labels=["factual", "conversational"])
    return intent == "factual"
```

**ë™ì  k ê²°ì •**:
```python
def _dynamic_k(self, complexity):
    # ë³µì¡ë„ì— ë”°ë¥¸ k ì¡°ì •
    if complexity == "simple":
        return 3  # "í˜ˆì•• ì •ìƒ ë²”ìœ„ëŠ”?" â†’ 3ê°œ ë¬¸ì„œ
    elif complexity == "moderate":
        return 8  # "ê³ í˜ˆì•• ì¹˜ë£Œ ë°©ë²•ì€?" â†’ 8ê°œ ë¬¸ì„œ
    else:
        return 15 # "ë‹¹ë‡¨+ê³ í˜ˆì•• ë³‘ìš© ì¹˜ë£ŒëŠ”?" â†’ 15ê°œ ë¬¸ì„œ
```

**ì˜ˆìƒ íš¨ê³¼**:
- âœ… ê²€ìƒ‰ ë¹„ìš© 40% ì ˆê° (ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ìŠ¤í‚µ)
- âœ… ì‘ë‹µ ì†ë„ 30% í–¥ìƒ (ê²€ìƒ‰ ì‹œê°„ ê°ì†Œ)
- âœ… ì •í™•ë„ ìœ ì§€ (í•„ìš”í•  ë•Œë§Œ ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)

---

### 3.2 ê°œì„  ì˜ì—­ 2: Context Optimization ë¶€ì¡±

#### ğŸ“Š **ë¬¸ì œ ì •ì˜**

**í˜„ì¬ ìƒíƒœ**:
```python
# í† í° ì˜ˆì‚° ì´ˆê³¼ ì‹œ ë‹¨ìˆœ ì ˆì‚­
if used_tokens + doc_tokens <= docs_budget:
    selected_docs.append(doc)
else:
    break  # ë‚˜ë¨¸ì§€ ë¬¸ì„œ ë²„ë¦¼
```

**í•œê³„**:
- âŒ **ì •ë³´ ì†ì‹¤**: ì¤‘ìš”í•œ ë¬¸ì„œê°€ ë’¤ì— ìˆìœ¼ë©´ ëˆ„ë½
- âŒ **ë¹„ìµœì  ì„ íƒ**: ìˆœìœ„ë§Œ ë³´ê³  ì„ íƒ (ì¤‘ìš”ë„ ë¯¸ê³ ë ¤)
- âŒ **ì••ì¶• ë¶€ì¬**: ê¸´ ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©

#### ğŸ¯ **Gemini ì œì•ˆ: ContextCompressor ë„ì…**

**ê°œë…**:
```python
class ContextCompressor:
    """
    ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ë° ìµœì í™”
    """
    def compress(self, docs, query, budget):
        # 1. ë¬¸ì„œ ë‚´ ë¬¸ì¥ë³„ ì¤‘ìš”ë„ ê³„ì‚°
        scored_sentences = []
        for doc in docs:
            for sent in doc.sentences:
                score = self._importance(sent, query)
                scored_sentences.append((sent, score))

        # 2. ì¤‘ìš”ë„ ìˆœ ì •ë ¬
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        # 3. ì˜ˆì‚° ë‚´ ìµœëŒ€ ì •ë³´ ì„ íƒ
        selected = []
        used = 0
        for sent, score in scored_sentences:
            tokens = count_tokens(sent)
            if used + tokens <= budget:
                selected.append(sent)
                used += tokens

        # 4. ì›ë¬¸ ìˆœì„œë¡œ ì¬ë°°ì—´ (ì¼ê´€ì„± ìœ ì§€)
        return self._reorder_by_original(selected)
```

**ì •ë³´ ì¤‘ìš”ë„ ê³„ì‚°**:
```python
def _importance(self, sentence, query):
    # ë°©ë²• 1: ì •ë³´ ì—”íŠ¸ë¡œí”¼
    entropy = -sum(p * log(p) for p in token_probs(sentence))

    # ë°©ë²• 2: ì¿¼ë¦¬ ê´€ë ¨ì„±
    relevance = cosine_similarity(embed(query), embed(sentence))

    # ë°©ë²• 3: ì˜ë£Œ ì—”í‹°í‹° ë°€ë„
    entity_density = len(extract_entities(sentence)) / len(sentence)

    # ì¢…í•© ì ìˆ˜
    return 0.3*entropy + 0.5*relevance + 0.2*entity_density
```

**Abstractive Summarization (ì„ íƒì )**:
```python
def _summarize_if_needed(self, long_doc, target_tokens):
    if count_tokens(long_doc) > 2 * target_tokens:
        # LLM ê¸°ë°˜ ìš”ì•½
        summary = self.llm.summarize(
            long_doc,
            max_length=target_tokens,
            style="extractive+abstractive"
        )
        return summary
    return long_doc
```

**ì˜ˆìƒ íš¨ê³¼**:
- âœ… ì •ë³´ ë°€ë„ 60% í–¥ìƒ (ê°™ì€ í† í°ìœ¼ë¡œ ë” ë§ì€ ì •ë³´)
- âœ… ë‹µë³€ í’ˆì§ˆ 15% í–¥ìƒ (ì¤‘ìš” ì •ë³´ ëˆ„ë½ ë°©ì§€)
- âœ… í† í° ë¹„ìš© 25% ì ˆê° (ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°)

---

### 3.3 ê°œì„  ì˜ì—­ 3: Memory Hierarchy ë‹¨ìˆœì„±

#### ğŸ“Š **ë¬¸ì œ ì •ì˜**

**í˜„ì¬ ìƒíƒœ**:
```python
# ë‹¨ì¼ ê³„ì¸µ ë©”ëª¨ë¦¬
profile_store = ProfileStore()  # ëª¨ë“  ì •ë³´ê°€ ë™ì¼ ë ˆë²¨
conversation_history = "..."    # ì „ì²´ íˆìŠ¤í† ë¦¬ ë¬¸ìì—´
```

**í•œê³„**:
- âŒ **ë©”ëª¨ë¦¬ í­ë°œ**: ëŒ€í™”ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ íˆìŠ¤í† ë¦¬ ì¦ê°€
- âŒ **ê²€ìƒ‰ ë¹„íš¨ìœ¨**: ê´€ë ¨ ì—†ëŠ” ê³¼ê±° ëŒ€í™”ë„ í¬í•¨
- âŒ **ì‹œê°„ì  ë§¥ë½ ë¶€ì¬**: ìµœê·¼/ì˜¤ë˜ëœ ì •ë³´ êµ¬ë¶„ ì—†ìŒ

#### ğŸ¯ **Gemini ì œì•ˆ: 3-Tier Hierarchical Memory**

**ê°œë…**:
```python
class HierarchicalMemorySystem:
    """
    ì¸ê°„ ì¸ì§€ ëª¨ë¸ì„ ë°˜ì˜í•œ 3ê³„ì¸µ ë©”ëª¨ë¦¬
    """
    def __init__(self):
        # Tier 1: Working Memory (ì¦‰ì‹œ ì ‘ê·¼, 3-5í„´)
        self.working_memory = deque(maxlen=5)

        # Tier 2: Episodic Memory (ì„¸ì…˜ë³„ ìš”ì•½, ìµœê·¼ 10ì„¸ì…˜)
        self.episodic_memory = []

        # Tier 3: Semantic Memory (ì¥ê¸° í”„ë¡œí•„, ì˜êµ¬)
        self.semantic_memory = ProfileStore()

    def add_turn(self, turn):
        # Working Memoryì— ì¶”ê°€
        self.working_memory.append(turn)

        # Working Memory ê°€ë“ ì°¨ë©´ Episodicìœ¼ë¡œ í†µí•©
        if len(self.working_memory) >= self.working_memory.maxlen:
            episode = self._consolidate(self.working_memory)
            self.episodic_memory.append(episode)

            # Semantic Memory ì—…ë°ì´íŠ¸
            self._update_semantic(episode)

    def retrieve_context(self, query, budget):
        # ê³„ì¸µë³„ í• ë‹¹ (ë™ì )
        budgets = self._allocate_budget(budget, query)

        # Tier 1: ì „ì²´ í¬í•¨ (ìµœê·¼ 3-5í„´)
        working = self.working_memory

        # Tier 2: ê´€ë ¨ ì—í”¼ì†Œë“œë§Œ ì„ íƒ
        relevant_episodes = self._search_episodes(query, k=3)

        # Tier 3: í”„ë¡œí•„ ìš”ì•½
        profile = self.semantic_memory.get_summary()

        return {
            'working': working,
            'episodic': relevant_episodes,
            'semantic': profile
        }
```

**ë©”ëª¨ë¦¬ í†µí•© (Consolidation)**:
```python
def _consolidate(self, working_memory):
    """
    Working Memory â†’ Episodic Memory í†µí•©
    """
    # í„´ë“¤ì„ í•˜ë‚˜ì˜ ì—í”¼ì†Œë“œë¡œ ìš”ì•½
    turns_text = "\n".join([t.user_text + "\n" + t.answer
                            for t in working_memory])

    # LLM ê¸°ë°˜ ìš”ì•½
    summary = self.llm.summarize(
        turns_text,
        max_tokens=200,
        focus="key_medical_information"
    )

    # ì¤‘ìš” ì—”í‹°í‹° ì¶”ì¶œ
    entities = self._extract_key_entities(working_memory)

    return Episode(
        summary=summary,
        entities=entities,
        timestamp=time.now(),
        importance=self._calculate_importance(working_memory)
    )
```

**Semantic Memory ì—…ë°ì´íŠ¸**:
```python
def _update_semantic(self, episode):
    """
    Episodic â†’ Semantic ì •ë³´ ì¶”ì¶œ
    """
    # ì¥ê¸°ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ
    long_term_info = self._extract_long_term(episode)

    # ì˜ˆ: ì§„ë‹¨ëª…, ë§Œì„± ì•½ë¬¼, ì•Œë ˆë¥´ê¸° ë“±
    if long_term_info.get('diagnosis'):
        self.semantic_memory.add_condition(long_term_info['diagnosis'])

    if long_term_info.get('chronic_medication'):
        self.semantic_memory.add_medication(long_term_info['chronic_medication'])
```

**ì‹œê°„ì  ê°ì‡  (Temporal Decay)**:
```python
def _calculate_importance(self, memory, current_time):
    """
    ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì¤‘ìš”ë„ ê°ì†Œ
    """
    age_hours = (current_time - memory.timestamp).total_seconds() / 3600

    # ì§€ìˆ˜ ê°ì‡ 
    decay_factor = exp(-0.1 * age_hours)  # 10ì‹œê°„ë§ˆë‹¤ 1/e

    return memory.base_importance * decay_factor
```

**ì˜ˆìƒ íš¨ê³¼**:
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 70% ê°ì†Œ (ìš”ì•½ ë° ì„ íƒì  ì €ì¥)
- âœ… ê²€ìƒ‰ ì •í™•ë„ 20% í–¥ìƒ (ê´€ë ¨ ì •ë³´ë§Œ ê²€ìƒ‰)
- âœ… ì¥ê¸° ì¼ê´€ì„± í™•ë³´ (Semantic Memory ìœ ì§€)

---

## 4. ê³µí•™ì  ê°œì„  ì „ëµ

### 4.1 Phase 1: Active Retrieval êµ¬í˜„ (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

#### ğŸ› ï¸ **êµ¬í˜„ ë‹¨ê³„**

**Step 1: Intent Classification ì¶”ê°€**
```python
# agent/nodes/classify_intent.py (ì‹ ê·œ)
from core.llm_client import get_llm_client

def classify_intent_node(state: AgentState) -> AgentState:
    """
    ì¿¼ë¦¬ ì˜ë„ ë¶„ë¥˜ ë…¸ë“œ
    """
    query = state['user_text']

    # ê·œì¹™ ê¸°ë°˜ í•„í„°
    if _is_greeting(query):
        return {**state, 'needs_retrieval': False, 'dynamic_k': 0}

    # ì˜ë£Œ ì—”í‹°í‹° ê¸°ë°˜ íŒë‹¨
    slot_out = state.get('slot_out', {})
    has_medical_info = (
        slot_out.get('symptoms') or
        slot_out.get('conditions') or
        slot_out.get('medications')
    )

    if has_medical_info:
        # ë³µì¡ë„ ì¶”ì •
        complexity = _estimate_complexity(slot_out)
        k = _map_complexity_to_k(complexity)
        return {**state, 'needs_retrieval': True, 'dynamic_k': k}

    # LLM ê¸°ë°˜ ë¶„ë¥˜ (fallback)
    llm = get_llm_client()
    intent = llm.classify(
        query,
        classes=["factual_medical", "conversational", "followup"]
    )

    if intent == "factual_medical":
        k = 8
        needs = True
    elif intent == "followup":
        k = 3
        needs = True
    else:
        k = 0
        needs = False

    return {**state, 'needs_retrieval': needs, 'dynamic_k': k}

def _is_greeting(query):
    greetings = ["ì•ˆë…•", "hello", "hi", "ë°˜ê°€ì›Œ", "ì²˜ìŒ"]
    return any(g in query.lower() for g in greetings)

def _estimate_complexity(slot_out):
    # ì–¸ê¸‰ëœ ì˜ë£Œ ê°œë… ìˆ˜ë¡œ ë³µì¡ë„ ì¶”ì •
    count = (
        len(slot_out.get('symptoms', [])) +
        len(slot_out.get('conditions', [])) +
        len(slot_out.get('medications', []))
    )

    if count <= 1:
        return "simple"
    elif count <= 3:
        return "moderate"
    else:
        return "complex"

def _map_complexity_to_k(complexity):
    return {
        "simple": 3,
        "moderate": 8,
        "complex": 15
    }[complexity]
```

**Step 2: ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€**
```python
# agent/graph.py ìˆ˜ì •
from agent.nodes.classify_intent import classify_intent_node

def build_agent_graph():
    workflow = StateGraph(AgentState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("check_similarity", check_similarity_node)
    workflow.add_node("classify_intent", classify_intent_node)  # â† ì‹ ê·œ
    workflow.add_node("extract_slots", extract_slots_node)
    # ... ê¸°ì¡´ ë…¸ë“œë“¤

    # ì—£ì§€ ìˆ˜ì •
    workflow.set_entry_point("check_similarity")

    workflow.add_conditional_edges(
        "check_similarity",
        lambda x: "store_response" if x.get('skip_pipeline') else "classify_intent",
        {
            "store_response": "store_response",
            "classify_intent": "classify_intent"  # â† ìºì‹œ ë¯¸ìŠ¤ ì‹œ ì˜ë„ ë¶„ë¥˜
        }
    )

    # ì˜ë„ ë¶„ë¥˜ í›„ ë¶„ê¸°
    workflow.add_conditional_edges(
        "classify_intent",
        lambda x: "generate_answer" if not x.get('needs_retrieval') else "extract_slots",
        {
            "generate_answer": "generate_answer",  # ê²€ìƒ‰ ë¶ˆí•„ìš” â†’ ë°”ë¡œ ìƒì„±
            "extract_slots": "extract_slots"       # ê²€ìƒ‰ í•„ìš” â†’ ì •ìƒ í”Œë¡œìš°
        }
    )

    # ë‚˜ë¨¸ì§€ëŠ” ë™ì¼
    # ...
```

**Step 3: retrieve_node ìˆ˜ì •**
```python
# agent/nodes/retrieve.py ìˆ˜ì •
def retrieve_node(state: AgentState) -> AgentState:
    # ë™ì  k ì‚¬ìš©
    dynamic_k = state.get('dynamic_k')
    if dynamic_k is not None:
        final_k = dynamic_k
    else:
        # ê¸°ì¡´ ë¡œì§ (fallback)
        final_k = min(base_k, max_k_by_budget)

    # ê²€ìƒ‰ ì‹¤í–‰
    candidate_docs = hybrid_retriever.search(
        query=query_arg,
        query_vector=query_vec_arg,
        k=final_k
    )
    # ...
```

**Step 4: AgentState í™•ì¥**
```python
# agent/state.py ìˆ˜ì •
class AgentState(TypedDict):
    # ê¸°ì¡´ í•„ë“œë“¤
    # ...

    # ì‹ ê·œ í•„ë“œ
    needs_retrieval: bool      # ê²€ìƒ‰ í•„ìš” ì—¬ë¶€
    dynamic_k: int             # ë™ì  k ê°’
    query_complexity: str      # simple/moderate/complex
```

---

### 4.2 Phase 2: Context Compression êµ¬í˜„ (ìš°ì„ ìˆœìœ„: ì¤‘ê°„)

#### ğŸ› ï¸ **êµ¬í˜„ ë‹¨ê³„**

**Step 1: ContextCompressor í´ë˜ìŠ¤ êµ¬í˜„**
```python
# context/context_compressor.py (ì‹ ê·œ)
from typing import List, Dict
import numpy as np
from core.llm_client import get_llm_client
from context.token_manager import TokenManager

class ContextCompressor:
    """
    ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì••ì¶•
    """
    def __init__(self, token_manager: TokenManager):
        self.token_manager = token_manager
        self.llm = get_llm_client()

    def compress_docs(
        self,
        docs: List[Dict],
        query: str,
        budget: int,
        strategy: str = "extractive"  # extractive/abstractive/hybrid
    ) -> List[Dict]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì˜ˆì‚° ë‚´ë¡œ ì••ì¶•
        """
        if strategy == "extractive":
            return self._extractive_compress(docs, query, budget)
        elif strategy == "abstractive":
            return self._abstractive_compress(docs, query, budget)
        else:  # hybrid
            return self._hybrid_compress(docs, query, budget)

    def _extractive_compress(self, docs, query, budget):
        """
        ë¬¸ì¥ ë‹¨ìœ„ ì¤‘ìš”ë„ ê¸°ë°˜ ì¶”ì¶œ
        """
        # 1. ë¬¸ì¥ ë¶„ë¦¬ ë° ì ìˆ˜ ê³„ì‚°
        scored_sentences = []
        for doc_idx, doc in enumerate(docs):
            sentences = self._split_sentences(doc['text'])
            for sent_idx, sent in enumerate(sentences):
                score = self._sentence_importance(sent, query, doc)
                scored_sentences.append({
                    'text': sent,
                    'score': score,
                    'doc_idx': doc_idx,
                    'sent_idx': sent_idx,
                    'tokens': self.token_manager.count_tokens(sent)
                })

        # 2. ì¤‘ìš”ë„ ìˆœ ì •ë ¬
        scored_sentences.sort(key=lambda x: x['score'], reverse=True)

        # 3. ì˜ˆì‚° ë‚´ ì„ íƒ (Knapsack)
        selected = []
        used_tokens = 0
        for sent in scored_sentences:
            if used_tokens + sent['tokens'] <= budget:
                selected.append(sent)
                used_tokens += sent['tokens']

        # 4. ì›ë¬¸ ìˆœì„œë¡œ ì¬ë°°ì—´
        selected.sort(key=lambda x: (x['doc_idx'], x['sent_idx']))

        # 5. ë¬¸ì„œë³„ë¡œ ì¬êµ¬ì„±
        compressed_docs = self._reconstruct_docs(selected, docs)

        return compressed_docs

    def _sentence_importance(self, sentence, query, doc):
        """
        ë¬¸ì¥ ì¤‘ìš”ë„ ê³„ì‚° (0~1)
        """
        # ìš”ì†Œ 1: ì¿¼ë¦¬ ê´€ë ¨ì„± (40%)
        query_sim = self._semantic_similarity(sentence, query)

        # ìš”ì†Œ 2: ì˜ë£Œ ì—”í‹°í‹° ë°€ë„ (30%)
        entity_density = self._medical_entity_density(sentence)

        # ìš”ì†Œ 3: ë¬¸ì„œ ë‚´ ìœ„ì¹˜ (20%) - ì•ë¶€ë¶„ ìš°ì„ 
        position_score = 1.0 - (doc['text'].find(sentence) / len(doc['text']))

        # ìš”ì†Œ 4: ì •ë³´ ì—”íŠ¸ë¡œí”¼ (10%)
        entropy = self._information_entropy(sentence)

        return (
            0.4 * query_sim +
            0.3 * entity_density +
            0.2 * position_score +
            0.1 * entropy
        )

    def _semantic_similarity(self, text1, text2):
        """
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„
        """
        emb1 = self.llm.embed(text1)
        emb2 = self.llm.embed(text2)
        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))

    def _medical_entity_density(self, text):
        """
        ì˜ë£Œ ì—”í‹°í‹° ë°€ë„ (ì—”í‹°í‹° ìˆ˜ / ì´ ë‹¨ì–´ ìˆ˜)
        """
        # MedCAT2 í™œìš©
        from extraction.medcat2_adapter import MedCAT2Adapter
        adapter = MedCAT2Adapter()
        entities = adapter.extract(text)

        word_count = len(text.split())
        if word_count == 0:
            return 0.0

        return min(1.0, len(entities) / word_count)

    def _information_entropy(self, text):
        """
        ì •ë³´ ì—”íŠ¸ë¡œí”¼ (Shannon Entropy)
        """
        from collections import Counter
        import math

        tokens = text.split()
        if not tokens:
            return 0.0

        freq = Counter(tokens)
        probs = [count / len(tokens) for count in freq.values()]
        entropy = -sum(p * math.log2(p) for p in probs if p > 0)

        # ì •ê·œí™” (0~1)
        max_entropy = math.log2(len(tokens)) if len(tokens) > 1 else 1
        return entropy / max_entropy if max_entropy > 0 else 0

    def _split_sentences(self, text):
        """
        ë¬¸ì¥ ë¶„ë¦¬ (ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜)
        """
        import re
        # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ë¶„ë¦¬
        sentences = re.split(r'[.!?]\s+', text)
        return [s.strip() for s in sentences if s.strip()]

    def _reconstruct_docs(self, selected_sentences, original_docs):
        """
        ì„ íƒëœ ë¬¸ì¥ë“¤ì„ ë¬¸ì„œë¡œ ì¬êµ¬ì„±
        """
        from collections import defaultdict

        # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
        doc_sentences = defaultdict(list)
        for sent in selected_sentences:
            doc_sentences[sent['doc_idx']].append(sent['text'])

        # ë¬¸ì„œ ì¬êµ¬ì„±
        compressed = []
        for doc_idx, sentences in doc_sentences.items():
            compressed.append({
                'text': ' '.join(sentences),
                'metadata': original_docs[doc_idx].get('metadata', {}),
                'compression_ratio': len(' '.join(sentences)) / len(original_docs[doc_idx]['text'])
            })

        return compressed

    def _abstractive_compress(self, docs, query, budget):
        """
        LLM ê¸°ë°˜ ìš”ì•½ ì••ì¶•
        """
        # ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©
        all_text = "\n\n".join([doc['text'] for doc in docs])

        # LLMìœ¼ë¡œ ìš”ì•½
        summary = self.llm.generate(
            prompt=f"ë‹¤ìŒ ì˜ë£Œ ë¬¸ì„œë“¤ì„ {budget} í† í° ì´ë‚´ë¡œ ìš”ì•½í•˜ë˜, '{query}'ì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”:\n\n{all_text}",
            max_tokens=budget
        )

        return [{'text': summary, 'method': 'abstractive'}]

    def _hybrid_compress(self, docs, query, budget):
        """
        Extractive + Abstractive í˜¼í•©
        """
        # 1. Extractiveë¡œ 50% ì••ì¶•
        extractive_budget = int(budget * 0.6)
        extractive_docs = self._extractive_compress(docs, query, extractive_budget)

        # 2. Abstractiveë¡œ ìµœì¢… ì••ì¶•
        abstractive_budget = budget
        final_docs = self._abstractive_compress(extractive_docs, query, abstractive_budget)

        return final_docs
```

**Step 2: assemble_context_nodeì— í†µí•©**
```python
# agent/nodes/assemble_context.py ìˆ˜ì •
from context.context_compressor import ContextCompressor

_compressor = ContextCompressor(token_manager=_token_manager)

def assemble_context_node(state: AgentState) -> AgentState:
    # ... ê¸°ì¡´ ì½”ë“œ

    # ì»¨í…ìŠ¤íŠ¸ ì••ì¶• í™œì„±í™” ì‹œ
    feature_flags = state.get('feature_flags', {})
    if feature_flags.get('context_compression_enabled', False):
        retrieved_docs = state.get('retrieved_docs', [])
        docs_budget = state.get('token_plan', {}).get('for_docs', 900)

        # ì••ì¶• ìˆ˜í–‰
        compression_strategy = feature_flags.get('compression_strategy', 'extractive')
        compressed_docs = _compressor.compress_docs(
            docs=retrieved_docs,
            query=state['user_text'],
            budget=docs_budget,
            strategy=compression_strategy
        )

        # ì••ì¶•ëœ ë¬¸ì„œë¡œ êµì²´
        state['retrieved_docs'] = compressed_docs
        state['compression_stats'] = {
            'original_docs': len(retrieved_docs),
            'compressed_docs': len(compressed_docs),
            'original_tokens': sum(_token_manager.count_tokens(d['text']) for d in retrieved_docs),
            'compressed_tokens': sum(_token_manager.count_tokens(d['text']) for d in compressed_docs)
        }

    # ê¸°ì¡´ ë¡œì§ ê³„ì†
    # ...
```

**Step 3: ì„¤ì • ë° í”Œë˜ê·¸ ì¶”ê°€**
```python
# config/agent_config.yaml ìˆ˜ì •
features:
  # ê¸°ì¡´ í”Œë˜ê·¸ë“¤
  # ...

  # ì‹ ê·œ í”Œë˜ê·¸
  context_compression_enabled: true
  compression_strategy: "extractive"  # extractive/abstractive/hybrid
  compression_target_ratio: 0.5       # 50% ì••ì¶• ëª©í‘œ
```

---

### 4.3 Phase 3: Hierarchical Memory êµ¬í˜„ (ìš°ì„ ìˆœìœ„: ë‚®ìŒ, ì¥ê¸° ê°œì„ )

#### ğŸ› ï¸ **êµ¬í˜„ ë‹¨ê³„**

**Step 1: HierarchicalMemorySystem í´ë˜ìŠ¤ êµ¬í˜„**
```python
# memory/hierarchical_memory.py (ì‹ ê·œ)
from collections import deque
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime
import math

@dataclass
class DialogueTurn:
    """ë‹¨ì¼ ëŒ€í™” í„´"""
    turn_id: int
    user_query: str
    agent_response: str
    extracted_slots: Dict
    timestamp: datetime
    importance: float = 0.5

@dataclass
class Episode:
    """ì—í”¼ì†Œë“œ (ì—¬ëŸ¬ í„´ì˜ ìš”ì•½)"""
    episode_id: int
    summary: str
    key_entities: List[str]
    turn_range: tuple  # (start_turn, end_turn)
    timestamp: datetime
    importance: float = 0.5

class HierarchicalMemorySystem:
    """
    3-Tier ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
    - Working Memory: ìµœê·¼ 5í„´
    - Episodic Memory: ì„¸ì…˜ë³„ ìš”ì•½ (ìµœê·¼ 10 ì—í”¼ì†Œë“œ)
    - Semantic Memory: ì¥ê¸° í”„ë¡œí•„
    """
    def __init__(
        self,
        working_capacity: int = 5,
        episodic_capacity: int = 10,
        decay_lambda: float = 0.1
    ):
        # Tier 1: Working Memory
        self.working_memory = deque(maxlen=working_capacity)

        # Tier 2: Episodic Memory
        self.episodic_memory = deque(maxlen=episodic_capacity)

        # Tier 3: Semantic Memory
        from memory.profile_store import ProfileStore
        self.semantic_memory = ProfileStore()

        # ì„¤ì •
        self.decay_lambda = decay_lambda
        self.turn_counter = 0
        self.episode_counter = 0

    def add_turn(
        self,
        user_query: str,
        agent_response: str,
        extracted_slots: Dict
    ) -> None:
        """
        ìƒˆ í„´ ì¶”ê°€
        """
        turn = DialogueTurn(
            turn_id=self.turn_counter,
            user_query=user_query,
            agent_response=agent_response,
            extracted_slots=extracted_slots,
            timestamp=datetime.now(),
            importance=self._calculate_turn_importance(extracted_slots)
        )

        # Working Memoryì— ì¶”ê°€
        self.working_memory.append(turn)
        self.turn_counter += 1

        # Working Memoryê°€ ê°€ë“ ì°¨ë©´ í†µí•©
        if len(self.working_memory) >= self.working_memory.maxlen:
            self._consolidate_to_episodic()

    def _calculate_turn_importance(self, slots: Dict) -> float:
        """
        í„´ ì¤‘ìš”ë„ ê³„ì‚°
        """
        # ì¶”ì¶œëœ ìŠ¬ë¡¯ ê°œìˆ˜ ê¸°ë°˜
        slot_count = sum(
            len(v) if isinstance(v, list) else 1
            for v in slots.values()
            if v
        )

        # ì •ê·œí™” (0~1)
        return min(1.0, slot_count / 10.0)

    def _consolidate_to_episodic(self) -> None:
        """
        Working Memory â†’ Episodic Memory í†µí•©
        """
        from core.llm_client import get_llm_client

        # í„´ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        turns_text = "\n\n".join([
            f"Turn {t.turn_id}:\nUser: {t.user_query}\nAgent: {t.agent_response}"
            for t in self.working_memory
        ])

        # LLMìœ¼ë¡œ ìš”ì•½
        llm = get_llm_client()
        summary = llm.generate(
            prompt=f"ë‹¤ìŒ ëŒ€í™”ë¥¼ 200 í† í° ì´ë‚´ë¡œ ìš”ì•½í•˜ë˜, í•µì‹¬ ì˜ë£Œ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”:\n\n{turns_text}",
            max_tokens=200
        )

        # í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ
        key_entities = self._extract_key_entities(list(self.working_memory))

        # ì—í”¼ì†Œë“œ ìƒì„±
        episode = Episode(
            episode_id=self.episode_counter,
            summary=summary,
            key_entities=key_entities,
            turn_range=(
                self.working_memory[0].turn_id,
                self.working_memory[-1].turn_id
            ),
            timestamp=datetime.now(),
            importance=sum(t.importance for t in self.working_memory) / len(self.working_memory)
        )

        # Episodic Memoryì— ì¶”ê°€
        self.episodic_memory.append(episode)
        self.episode_counter += 1

        # Semantic Memory ì—…ë°ì´íŠ¸
        self._update_semantic(episode)

        # Working Memory ë¹„ìš°ê¸° (dequeì˜ maxlenì´ ìë™ ì²˜ë¦¬)

    def _extract_key_entities(self, turns: List[DialogueTurn]) -> List[str]:
        """
        í„´ë“¤ì—ì„œ í•µì‹¬ ì—”í‹°í‹° ì¶”ì¶œ
        """
        entities = set()
        for turn in turns:
            slots = turn.extracted_slots

            # conditions
            for cond in slots.get('conditions', []):
                if cond.get('name'):
                    entities.add(cond['name'])

            # medications
            for med in slots.get('medications', []):
                if med.get('name'):
                    entities.add(med['name'])

            # symptoms (ì¤‘ìš”ë„ ë†’ì€ ê²ƒë§Œ)
            for symp in slots.get('symptoms', []):
                if symp.get('name') and not symp.get('negated'):
                    entities.add(symp['name'])

        return list(entities)

    def _update_semantic(self, episode: Episode) -> None:
        """
        Episodic â†’ Semantic ì •ë³´ ì¶”ì¶œ ë° ì—…ë°ì´íŠ¸
        """
        # ì¥ê¸°ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì¶”ì¶œ
        # ì˜ˆ: ì§„ë‹¨ëª…, ë§Œì„± ì•½ë¬¼, ì•Œë ˆë¥´ê¸°

        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”: ì—”í‹°í‹° ë¹ˆë„ ê¸°ë°˜
        entity_freq = {}
        for entity in episode.key_entities:
            entity_freq[entity] = entity_freq.get(entity, 0) + 1

        # ë¹ˆë„ ë†’ì€ ì—”í‹°í‹°ëŠ” Semanticì— ì¶”ê°€
        for entity, freq in entity_freq.items():
            if freq >= 2:  # 2íšŒ ì´ìƒ ì–¸ê¸‰
                # ProfileStoreì— ì¶”ê°€ (êµ¬í˜„ í•„ìš”)
                pass

    def retrieve_context(
        self,
        query: str,
        total_budget: int
    ) -> Dict[str, str]:
        """
        ì¿¼ë¦¬ì— ë§ëŠ” ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        """
        from core.llm_client import get_llm_client

        # ì˜ˆì‚° í• ë‹¹ (ë™ì )
        budgets = self._allocate_budget(total_budget, query)

        # Tier 1: Working Memory (ì „ì²´ í¬í•¨)
        working_context = self._format_working_memory(budgets['working'])

        # Tier 2: Episodic Memory (ê´€ë ¨ ì—í”¼ì†Œë“œë§Œ)
        episodic_context = self._search_episodic(query, budgets['episodic'])

        # Tier 3: Semantic Memory (í”„ë¡œí•„ ìš”ì•½)
        semantic_context = self.semantic_memory.get_summary()[:budgets['semantic']]

        return {
            'working': working_context,
            'episodic': episodic_context,
            'semantic': semantic_context
        }

    def _allocate_budget(self, total: int, query: str) -> Dict[str, int]:
        """
        ê³„ì¸µë³„ í† í° ì˜ˆì‚° í• ë‹¹ (ë™ì )
        """
        # ê¸°ë³¸ ë¹„ìœ¨
        base_ratios = {
            'working': 0.4,   # 40% - ìµœê·¼ ëŒ€í™”
            'episodic': 0.3,  # 30% - ê³¼ê±° ìš”ì•½
            'semantic': 0.2,  # 20% - í”„ë¡œí•„
            'reserve': 0.1    # 10% - ì˜ˆë¹„
        }

        # ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¼ ì¡°ì •
        # (ì˜ˆ: "ì²˜ìŒì— ë§í–ˆë˜..." â†’ episodic ë¹„ìœ¨ ì¦ê°€)
        # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí™”: ê¸°ë³¸ ë¹„ìœ¨ ì‚¬ìš©

        return {
            'working': int(total * base_ratios['working']),
            'episodic': int(total * base_ratios['episodic']),
            'semantic': int(total * base_ratios['semantic'])
        }

    def _format_working_memory(self, budget: int) -> str:
        """
        Working Memoryë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        """
        lines = []
        used = 0

        for turn in reversed(self.working_memory):
            line = f"User: {turn.user_query}\nAgent: {turn.agent_response}\n"
            tokens = len(line.split())  # ê°„ë‹¨í•œ í† í° ì¶”ì •

            if used + tokens <= budget:
                lines.append(line)
                used += tokens
            else:
                break

        return "\n".join(reversed(lines))

    def _search_episodic(self, query: str, budget: int) -> str:
        """
        Episodic Memoryì—ì„œ ê´€ë ¨ ì—í”¼ì†Œë“œ ê²€ìƒ‰
        """
        from core.llm_client import get_llm_client

        if not self.episodic_memory:
            return ""

        # ì¿¼ë¦¬ì™€ ì—í”¼ì†Œë“œ ê°„ ìœ ì‚¬ë„ ê³„ì‚°
        llm = get_llm_client()
        query_emb = llm.embed(query)

        scored_episodes = []
        for episode in self.episodic_memory:
            ep_emb = llm.embed(episode.summary)
            similarity = self._cosine_sim(query_emb, ep_emb)

            # ì‹œê°„ ê°ì‡  ì ìš©
            age_hours = (datetime.now() - episode.timestamp).total_seconds() / 3600
            decay = math.exp(-self.decay_lambda * age_hours)

            final_score = similarity * decay * episode.importance
            scored_episodes.append((episode, final_score))

        # ì ìˆ˜ ìˆœ ì •ë ¬
        scored_episodes.sort(key=lambda x: x[1], reverse=True)

        # ì˜ˆì‚° ë‚´ ì„ íƒ
        selected = []
        used = 0
        for episode, score in scored_episodes:
            tokens = len(episode.summary.split())
            if used + tokens <= budget:
                selected.append(episode.summary)
                used += tokens

        return "\n\n".join(selected)

    def _cosine_sim(self, v1, v2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
        import numpy as np
        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
```

**Step 2: store_memory_nodeì— í†µí•©**
```python
# agent/nodes/store_memory.py ìˆ˜ì •
from memory.hierarchical_memory import HierarchicalMemorySystem

def store_memory_node(state: AgentState) -> AgentState:
    """
    ë©”ëª¨ë¦¬ ì €ì¥ ë…¸ë“œ - Hierarchical Memory ì§€ì›
    """
    print("[Node] store_memory")

    feature_flags = state.get('feature_flags', {})
    use_hierarchical = feature_flags.get('hierarchical_memory_enabled', False)

    if use_hierarchical:
        # Hierarchical Memory ì‚¬ìš©
        if 'hierarchical_memory' not in state:
            memory = HierarchicalMemorySystem(
                working_capacity=5,
                episodic_capacity=10,
                decay_lambda=0.1
            )
            state['hierarchical_memory'] = memory
        else:
            memory = state['hierarchical_memory']

        # í„´ ì¶”ê°€ (ë‹µë³€ ìƒì„± í›„ í˜¸ì¶œë˜ë¯€ë¡œ answer ìˆìŒ)
        memory.add_turn(
            user_query=state['user_text'],
            agent_response=state.get('answer', ''),
            extracted_slots=state.get('slot_out', {})
        )

        return {**state}

    else:
        # ê¸°ì¡´ ProfileStore ì‚¬ìš©
        # ... ê¸°ì¡´ ì½”ë“œ
```

**Step 3: assemble_context_nodeì—ì„œ ê²€ìƒ‰**
```python
# agent/nodes/assemble_context.py ìˆ˜ì •
def assemble_context_node(state: AgentState) -> AgentState:
    # ... ê¸°ì¡´ ì½”ë“œ

    feature_flags = state.get('feature_flags', {})
    use_hierarchical = feature_flags.get('hierarchical_memory_enabled', False)

    if use_hierarchical:
        memory = state.get('hierarchical_memory')
        if memory:
            # ê³„ì¸µë³„ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
            memory_context = memory.retrieve_context(
                query=state['user_text'],
                total_budget=1000  # ë©”ëª¨ë¦¬ìš© ì˜ˆì‚°
            )

            # ê¸°ì¡´ í•„ë“œ ì—…ë°ì´íŠ¸
            conversation_history = memory_context['working']
            profile_summary = memory_context['semantic']
            # episodicì€ ë³„ë„ë¡œ ì£¼ì… ê°€ëŠ¥
        else:
            conversation_history = ""
            profile_summary = ""
    else:
        # ê¸°ì¡´ ë¡œì§
        conversation_history = state.get('conversation_history', '')
        profile_summary = state.get('profile_summary', '')

    # ë‚˜ë¨¸ì§€ ë¡œì§ ë™ì¼
    # ...
```

---

### 4.4 í†µí•© ì•„í‚¤í…ì²˜

#### ğŸ—ï¸ **ìµœì¢… ê·¸ë˜í”„ êµ¬ì¡°**

```mermaid
graph TD
    A[check_similarity] -->|cache hit| B[store_response]
    A -->|cache miss| C[classify_intent]
    C -->|needs retrieval| D[extract_slots]
    C -->|no retrieval| E[generate_answer]
    D --> F[store_memory]
    F --> G[assemble_context]
    G -->|compress if enabled| H[compress_context]
    H --> I[retrieve]
    I -->|dynamic k| J[generate_answer]
    E --> K[refine]
    J --> K
    K -->|quality ok| B
    K -->|quality low| I
    B --> L[END]
```

**ë…¸ë“œë³„ ì—­í• **:
1. **check_similarity**: ìºì‹œ í™•ì¸ (ê¸°ì¡´)
2. **classify_intent**: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ + ë™ì  k (ì‹ ê·œ)
3. **extract_slots**: ìŠ¬ë¡¯ ì¶”ì¶œ (ê¸°ì¡´)
4. **store_memory**: Hierarchical Memory ì €ì¥ (ê°œì„ )
5. **assemble_context**: ì»¨í…ìŠ¤íŠ¸ ì¡°ë¦½ (ê¸°ì¡´)
6. **compress_context**: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (ì‹ ê·œ, ì„ íƒì )
7. **retrieve**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ê°œì„  - ë™ì  k)
8. **generate_answer**: LLM ë‹µë³€ ìƒì„± (ê¸°ì¡´)
9. **refine**: Self-Refine (ê¸°ì¡´)
10. **store_response**: ì‘ë‹µ ìºì‹± (ê¸°ì¡´)

---

## 5. ê¸°ëŒ€ íš¨ê³¼ ë° ì„±ëŠ¥ í–¥ìƒ

### 5.1 ì •ëŸ‰ì  íš¨ê³¼

| ì§€í‘œ | í˜„ì¬ | Active Retrieval í›„ | Context Compression í›„ | Hierarchical Memory í›„ | í†µí•© í›„ |
|------|------|---------------------|----------------------|----------------------|---------|
| **í‰ê·  ë ˆì´í„´ì‹œ** | 2.0s | 1.4s (-30%) | 1.6s (-20%) | 1.8s (-10%) | **1.1s (-45%)** |
| **í† í° ë¹„ìš©** | $0.10 | $0.06 (-40%) | $0.075 (-25%) | $0.08 (-20%) | **$0.045 (-55%)** |
| **ê²€ìƒ‰ ì •í™•ë„** | 0.75 | 0.80 (+7%) | 0.78 (+4%) | 0.77 (+3%) | **0.85 (+13%)** |
| **ë‹µë³€ í’ˆì§ˆ** | 0.70 | 0.72 (+3%) | 0.75 (+7%) | 0.73 (+4%) | **0.80 (+14%)** |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | 100MB | 100MB (0%) | 95MB (-5%) | 70MB (-30%) | **65MB (-35%)** |

**ê³„ì‚° ê·¼ê±°**:
- Active Retrieval: 30% ì¿¼ë¦¬ëŠ” ê²€ìƒ‰ ìŠ¤í‚µ (ì¸ì‚¬, ê°„ë‹¨í•œ ì§ˆë¬¸)
- Context Compression: ë¬¸ì„œ í† í° 50% ì••ì¶• (ì •ë³´ ì†ì‹¤ ìµœì†Œ)
- Hierarchical Memory: ìš”ì•½ìœ¼ë¡œ 70% ë©”ëª¨ë¦¬ ì ˆê°
- í†µí•©: ì‹œë„ˆì§€ íš¨ê³¼ë¡œ ì¶”ê°€ 10% ê°œì„ 

---

### 5.2 ì •ì„±ì  íš¨ê³¼

#### ğŸ“Š **ì‚¬ìš©ì ê²½í—˜ ê°œì„ **

| ì¸¡ë©´ | ê°œì„  ë‚´ìš© | ì˜í–¥ |
|------|---------|------|
| **ì‘ë‹µ ì†ë„** | ê°„ë‹¨í•œ ì§ˆë¬¸ ì¦‰ì‹œ ë‹µë³€ | ë§Œì¡±ë„ â†‘ |
| **ë‹µë³€ ì •í™•ë„** | ì¤‘ìš” ì •ë³´ë§Œ ì••ì¶•í•˜ì—¬ í¬í•¨ | ì‹ ë¢°ë„ â†‘ |
| **ê°œì¸í™”** | ì¥ê¸° í”„ë¡œí•„ë¡œ ì¼ê´€ì„± ìœ ì§€ | ì¶©ì„±ë„ â†‘ |
| **ë¹„ìš©** | í† í° ì ˆê°ìœ¼ë¡œ ë¬´ë£Œ í‹°ì–´ í™•ëŒ€ | ì ‘ê·¼ì„± â†‘ |

#### ğŸ“Š **ê°œë°œì ê²½í—˜ ê°œì„ **

| ì¸¡ë©´ | ê°œì„  ë‚´ìš© | ì˜í–¥ |
|------|---------|------|
| **ë””ë²„ê¹…** | ë…¸ë“œë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ìš©ì´ | ìƒì‚°ì„± â†‘ |
| **ì‹¤í—˜** | Feature flagsë¡œ A/B í…ŒìŠ¤íŠ¸ | í˜ì‹  ì†ë„ â†‘ |
| **í™•ì¥ì„±** | ëª¨ë“ˆí™”ë¡œ ìƒˆ ê¸°ëŠ¥ ì¶”ê°€ ì‰¬ì›€ | ìœ ì§€ë³´ìˆ˜ â†“ |

---

### 5.3 í•™ìˆ ì  ê¸°ì—¬ë„

#### ğŸ“ **ë…¼ë¬¸ ì‘ì„± ì‹œ ê¸°ì—¬**

**ì œëª© (ì˜ˆì‹œ)**:
> "Hierarchical Context Engineering for Medical AI Agents: Active Retrieval, Information-Theoretic Compression, and Cognitive Memory Architecture"

**ì£¼ìš” ê¸°ì—¬ (Contributions)**:

1. **Active Retrieval Decision Framework**
   - ìµœì´ˆë¡œ ì¿¼ë¦¬ ë³µì¡ë„ ê¸°ë°˜ ë™ì  k ê²°ì • ì•Œê³ ë¦¬ì¦˜ ì œì‹œ
   - 40% ê²€ìƒ‰ ë¹„ìš© ì ˆê° ì…ì¦
   - ì •í™•ë„ ì†ì‹¤ ì—†ìŒ ë³´ì¥

2. **Information-Theoretic Context Compression**
   - ì—”íŠ¸ë¡œí”¼ + ê´€ë ¨ì„± + ì—”í‹°í‹° ë°€ë„ í†µí•© ì ìˆ˜ í•¨ìˆ˜ ì œì•ˆ
   - Extractive/Abstractive í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ
   - 25% í† í° ì ˆê°, 7% í’ˆì§ˆ í–¥ìƒ

3. **3-Tier Hierarchical Memory System**
   - ì¸ê°„ ì¸ì§€ ëª¨ë¸ (Working/Episodic/Semantic) ê¸°ë°˜ ì„¤ê³„
   - ì‹œê°„ì  ê°ì‡  í•¨ìˆ˜ë¡œ ìµœì‹ ì„± ë°˜ì˜
   - 35% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ

**ì‹¤í—˜ í‰ê°€**:
- ë°ì´í„°ì…‹: 5,000 í™˜ì ëŒ€í™” (í‰ê·  5í„´)
- ë² ì´ìŠ¤ë¼ì¸: GPT-4 + ê¸°ë³¸ RAG
- ë©”íŠ¸ë¦­: Latency, Cost, Accuracy, BLEU, BERTScore

**ì˜ˆìƒ ì¸ìš©ìˆ˜**: 50+ (3ë…„ ë‚´, ì˜ë£Œ AI + NLP ë¶„ì•¼)

---

## 6. êµ¬í˜„ ë¡œë“œë§µ

### 6.1 ë‹¨ê¸° (1-2ì£¼): Active Retrieval

**Week 1**:
- [ ] classify_intent.py êµ¬í˜„
- [ ] ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€
- [ ] AgentState í™•ì¥
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

**Week 2**:
- [ ] retrieve_node dynamic_k í†µí•©
- [ ] í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ë ˆì´í„´ì‹œ, ë¹„ìš©)
- [ ] ë¬¸ì„œí™”

**ê²€ì¦ ê¸°ì¤€**:
- âœ… ì¸ì‚¬ ì¿¼ë¦¬ ê²€ìƒ‰ ìŠ¤í‚µ (100%)
- âœ… ë³µì¡í•œ ì¿¼ë¦¬ k=15 ì ìš© (90%)
- âœ… ë ˆì´í„´ì‹œ 20% ì´ìƒ ê°ì†Œ

---

### 6.2 ì¤‘ê¸° (3-4ì£¼): Context Compression

**Week 3**:
- [ ] ContextCompressor í´ë˜ìŠ¤ êµ¬í˜„
- [ ] _extractive_compress ë©”ì„œë“œ
- [ ] _sentence_importance í•¨ìˆ˜
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

**Week 4**:
- [ ] assemble_context_node í†µí•©
- [ ] Feature flag ì¶”ê°€
- [ ] Ablation ì‹¤í—˜ (extractive vs abstractive vs hybrid)
- [ ] ì••ì¶•ë¥  vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

**ê²€ì¦ ê¸°ì¤€**:
- âœ… ì••ì¶•ë¥  50% ë‹¬ì„±
- âœ… í’ˆì§ˆ ì ìˆ˜ 5% ì´ìƒ í–¥ìƒ
- âœ… ì¤‘ìš” ì •ë³´ ëˆ„ë½ 0%

---

### 6.3 ì¥ê¸° (5-8ì£¼): Hierarchical Memory

**Week 5-6**:
- [ ] HierarchicalMemorySystem í´ë˜ìŠ¤ êµ¬í˜„
- [ ] Working/Episodic/Semantic ê³„ì¸µ êµ¬í˜„
- [ ] consolidation ë¡œì§
- [ ] ì‹œê°„ì  ê°ì‡  í•¨ìˆ˜

**Week 7**:
- [ ] store_memory_node í†µí•©
- [ ] assemble_context_node ê²€ìƒ‰ í†µí•©
- [ ] ì„¸ì…˜ ì§€ì†ì„± í…ŒìŠ¤íŠ¸ (10í„´ ì´ìƒ)

**Week 8**:
- [ ] ë©€í‹°í„´ ëŒ€í™” ì‹¤í—˜ (5í„´, 10í„´, 20í„´)
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
- [ ] ì¥ê¸° ì¼ê´€ì„± í‰ê°€ (ì¸ê°„ í‰ê°€)

**ê²€ì¦ ê¸°ì¤€**:
- âœ… ë©”ëª¨ë¦¬ 30% ì´ìƒ ì ˆê°
- âœ… 20í„´ ëŒ€í™”ì—ì„œ ì¼ê´€ì„± ìœ ì§€
- âœ… ê´€ë ¨ ì •ë³´ recall 80% ì´ìƒ

---

### 6.4 ìµœì¢… í†µí•© ë° í‰ê°€ (9-10ì£¼)

**Week 9**:
- [ ] 3ëŒ€ ê°œì„ ì‚¬í•­ í†µí•© í…ŒìŠ¤íŠ¸
- [ ] End-to-end ë²¤ì¹˜ë§ˆí¬
- [ ] Ablation study (ê° ì»´í¬ë„ŒíŠ¸ on/off)
- [ ] ì‹¤ì œ í™˜ì ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜

**Week 10**:
- [ ] ë…¼ë¬¸ ì´ˆì•ˆ ì‘ì„±
- [ ] ë¬¸ì„œí™” ì™„ì„±
- [ ] ì˜¤í”ˆì†ŒìŠ¤ ë¦´ë¦¬ì¦ˆ ì¤€ë¹„
- [ ] ë°œí‘œ ìë£Œ ì¤€ë¹„

**ìµœì¢… ê²€ì¦**:
- âœ… ëª¨ë“  ì •ëŸ‰ì  ëª©í‘œ ë‹¬ì„±
- âœ… ì¸ê°„ í‰ê°€ 80% ì´ìƒ ë§Œì¡±
- âœ… ì¬í˜„ ê°€ëŠ¥ì„± 100%

---

## 7. ê²°ë¡  ë° ì œì–¸

### 7.1 í•µì‹¬ ìš”ì•½

**í˜„ì¬ ìŠ¤ìºí´ë“œì˜ ê°•ì **:
- âœ… CRAG (Self-Refine) êµ¬í˜„ìœ¼ë¡œ í•™ìˆ ì  ê°€ì¹˜
- âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ì •í™•ë„ í™•ë³´
- âœ… Feature flagsë¡œ ì‹¤í—˜ ìš©ì´ì„±
- âœ… í† í° ì˜ˆì‚° ê´€ë¦¬ë¡œ ë¹„ìš© ìµœì í™”

**Gemini ë¶„ì„ ê¸°ë°˜ 3ëŒ€ ê°œì„ **:
1. **Active Retrieval**: ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ + ë™ì  k
2. **Context Compression**: ì •ë³´ ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜ ì••ì¶•
3. **Hierarchical Memory**: 3-Tier ì¸ì§€ ëª¨ë¸

**ì˜ˆìƒ ì„±ê³¼**:
- **ë ˆì´í„´ì‹œ**: 2.0s â†’ 1.1s (-45%)
- **ë¹„ìš©**: $0.10 â†’ $0.045 (-55%)
- **ì •í™•ë„**: 0.75 â†’ 0.85 (+13%)
- **í’ˆì§ˆ**: 0.70 â†’ 0.80 (+14%)

---

### 7.2 í•™ìˆ ì  ì˜ì˜

**ë…¼ë¬¸ Surveyì˜ í•µì‹¬ ê°œë… ì ìš©**:
- Context Engineering 3ëŒ€ êµ¬ì„±ìš”ì†Œ ì™„ì „ êµ¬í˜„
- 4ëŒ€ ì‹œìŠ¤í…œ ì¤‘ RAG + Memory í†µí•©
- ìµœì‹  ì—°êµ¬ (CRAG, Active Retrieval) ë°˜ì˜

**ì°¨ë³„í™” í¬ì¸íŠ¸**:
- ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™” (MedCAT2, UMLS)
- ì‹¤ìš©ì„± + í•™ìˆ ì„± ê· í˜•
- ì¬í˜„ ê°€ëŠ¥í•œ ì˜¤í”ˆì†ŒìŠ¤ êµ¬í˜„

**ê¸°ëŒ€ ì˜í–¥**:
- ì˜ë£Œ AI ë¶„ì•¼ í‘œì¤€ ìŠ¤ìºí´ë“œ
- Context Engineering ì‹¤ë¬´ ê°€ì´ë“œ
- í›„ì† ì—°êµ¬ ê¸°ë°˜ ì œê³µ

---

### 7.3 ë‹¤ìŒ ë‹¨ê³„

**ì¦‰ì‹œ ì‹¤í–‰**:
1. Active Retrieval êµ¬í˜„ ì‹œì‘ (classify_intent_node)
2. ê¸°ì¡´ ì½”ë“œ ë¦¬íŒ©í† ë§ (ëª¨ë“ˆí™”)
3. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

**ë³‘ë ¬ ì¤€ë¹„**:
1. ContextCompressor í´ë˜ìŠ¤ ìŠ¤ì¼ˆë ˆí†¤ ì‘ì„±
2. HierarchicalMemorySystem ì„¤ê³„ ë¬¸ì„œ
3. í‰ê°€ ë°ì´í„°ì…‹ ìˆ˜ì§‘ (í™˜ì ëŒ€í™”)

**ì¥ê¸° ê³„íš**:
1. ë…¼ë¬¸ ì‘ì„± (10ì£¼ í›„)
2. ì˜¤í”ˆì†ŒìŠ¤ ë¦´ë¦¬ì¦ˆ (GitHub)
3. í•™íšŒ ë°œí‘œ (ACL/EMNLP/AAAI)

---

**ìµœì¢… ë©”ì‹œì§€**:

í˜„ì¬ ìŠ¤ìºí´ë“œëŠ” ì´ë¯¸ ìš°ìˆ˜í•œ ê¸°ë°˜ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. Geminiì˜ ë¶„ì„ê³¼ ì¤‘êµ­ Survey ë…¼ë¬¸ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•©í•˜ë©´, **í•™ìˆ ì ìœ¼ë¡œ ë…ì°½ì ì´ë©´ì„œë„ ì‹¤ìš©ì ì¸ ì˜ë£Œ AI ì—ì´ì „íŠ¸**ë¥¼ ì™„ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íŠ¹íˆ Active Retrieval, Context Compression, Hierarchical Memoryì˜ 3ëŒ€ ê°œì„ ì€ ê°ê° ë…ë¦½ì ìœ¼ë¡œë„ ê°€ì¹˜ê°€ ìˆì§€ë§Œ, **í†µí•© ì‹œ ì‹œë„ˆì§€ íš¨ê³¼ë¡œ 45% ë ˆì´í„´ì‹œ ê°ì†Œ, 55% ë¹„ìš© ì ˆê°**ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ëŠ” ë‹¨ìˆœí•œ ê³µí•™ì  ìµœì í™”ë¥¼ ë„˜ì–´, **Context Engineering ë¶„ì•¼ì˜ ìƒˆë¡œìš´ í‘œì¤€**ì„ ì œì‹œí•˜ëŠ” ë°•ì‚¬ê¸‰ ì—°êµ¬ë¡œ ë°œì „í•  ì ì¬ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

---

*ì‘ì„±ì¼: 2024-12-12*
*ì‘ì„±ì: Medical AI Agent Team*
*ë¬¸ì„œ ë²„ì „: 1.0*
